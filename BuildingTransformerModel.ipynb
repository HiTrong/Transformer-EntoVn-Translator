{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce87a780-5d95-4d1f-b044-e916bee3cd14",
   "metadata": {},
   "source": [
    "# Building Transformer Model - Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f715195-33e0-4cd2-bea6-3877af2256dd",
   "metadata": {},
   "source": [
    "![Transformer Architecture](./Transformer_Understanding/img/transformerblock.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60ca720-9523-4748-a135-504f14310a63",
   "metadata": {},
   "source": [
    "Base on the upper picture, we will make our model functions with Transformer Architecture following the things we have been researching in the \"[Transformer Understanding](./Transformer_Understanding/TransformerNeuralNetworks.ipynb)\" part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8398fa25-9656-40a2-8a69-eb01e46cb1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import copy\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99343a2f-e0c9-4aa9-a6b1-2d101f8132cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d0c0e63-2dca-4b55-bbc3-3f51283811c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. PyTorch is using GPU.\n",
      "Number of GPUs available:  1\n",
      "GPU name:  NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. PyTorch is using GPU.\")\n",
    "    print(\"Number of GPUs available: \", torch.cuda.device_count())\n",
    "    print(\"GPU name: \", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch is using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44143e5c-22f3-4b4b-ba98-850d5512a972",
   "metadata": {},
   "source": [
    "## Start with these small blocks, functions, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de739e-d0e0-4776-ac43-aa0110974965",
   "metadata": {},
   "source": [
    "- **Get Using Device** (Torch requires all tensor  must be in the same device.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb05b66a-9351-4aa7-9276-f6bb32178767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cb8fed-58b8-4375-bc10-6134d4894127",
   "metadata": {},
   "source": [
    "- **Token Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ab64882-f3ce-4a20-8cb9-5f62b81df436",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        \"\"\"\n",
    "        Token Embedding is used for converting a word / token into a embedding numeric vector space.\n",
    "        \n",
    "        :param vocab_size: Number of words / token in vocabulary\n",
    "        :param d_model: The embedding dimension\n",
    "        \n",
    "        Example: With 1000 words in vocabulary and our embedding dimension is 512, the Token Embedding layer will be 1000x512\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: the word or sequence of words\n",
    "        :return: the numerical representation of the input\n",
    "        \n",
    "        Example:\n",
    "        Input: (Batch_size, Sequence of words) - (30x100)\n",
    "        Output: (Batch_size, Sequence of words, d_model) - (30x100x512)\n",
    "        \"\"\"\n",
    "        x = self.embedding_layer(x)\n",
    "        return x.to(get_device())\n",
    "\n",
    "# Or just Simple\n",
    "# token_embedding = nn.Embedding(vocab_size, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7837bba9-74c4-417f-8555-370e6923fa27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 100, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Example\n",
    "vocab_size = 1000\n",
    "d_model = 512\n",
    "\n",
    "embedding_layer = TokenEmbedding(vocab_size, d_model)\n",
    "input_data = torch.randint(0, vocab_size, (30, 100))\n",
    "embedding_layer(input_data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e47d72-f888-491c-82ab-e3ea6a67f6c1",
   "metadata": {},
   "source": [
    "- **Positional Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b935a5a-5a1a-47fe-a6d9-b234e7f96055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, max_sequence_length, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Positional Encoding layer for adding positional information to token embeddings.\n",
    "        \n",
    "        :param d_model: The embedding dimension.\n",
    "        :param max_sequence_length: The maximum length of the input sequences.\n",
    "        :param dropout: Dropout rate.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding,self).__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self):\n",
    "        even_i = torch.arange(0, self.d_model, 2).float()\n",
    "        denominator = torch.pow(10000, even_i/self.d_model)\n",
    "        position = (torch.arange(self.max_sequence_length)\n",
    "                          .reshape(self.max_sequence_length, 1))\n",
    "        even_PE = torch.sin(position / denominator)\n",
    "        odd_PE = torch.cos(position / denominator)\n",
    "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
    "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "        PE = PE.unsqueeze(0)\n",
    "        return self.dropout(PE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0565979b-3e58-49a6-8fd4-4a375bd24df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PE = PositionalEncoding(512,100,0.1)\n",
    "PE().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00ab53f-2eb1-4237-931f-4bc4f65fac9f",
   "metadata": {},
   "source": [
    "- **Multi-Head Attention**\n",
    "\n",
    "2 options for: 'encoder' and 'decoder' (**Multi-Head Cross Attention**)\n",
    "\n",
    "options for mask: **None**, **Self-Attention Mask** (**Causal Mask** hoáº·c **Look-Ahead Mask**), **Padding Mask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e69746d2-092e-4b4b-a451-4f838cb03d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads=8, cross=False):\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        :param d_model: the embedding dimension\n",
    "        :param num_heads: the number of heads, default equals 8\n",
    "        :param cross: True for Multi-Head Cross Attention, False for Multi-Head Attention only\n",
    "        \n",
    "        # note: The embedding dimension must be divided by the number of heads\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.cross = cross\n",
    "\n",
    "        # query, key value layer\n",
    "        if self.cross: # Multi-Head Cross Attention\n",
    "            self.kv_layer = nn.Linear(d_model , 2 * d_model)\n",
    "            self.q_layer = nn.Linear(d_model , d_model)\n",
    "        else:\n",
    "            self.qkv_layer = nn.Linear(d_model , 3 * d_model)\n",
    "        \n",
    "        \n",
    "        # method 1: old, cost alot\n",
    "        # self.query = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        # self.key = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        # self.value = nn.Linear(self.head_dim, self.head_dim, bias=False) \n",
    "\n",
    "        # method 2: the fewer linear layers the better the cost\n",
    "        \n",
    "        \n",
    "        # Linear Layer in Multi-Head Attention\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product(self, q, k, v, mask=None):\n",
    "        d_k = q.size()[-1]\n",
    "        scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scaled = scaled.permute(1, 0, 2, 3) + mask\n",
    "            scaled = scaled.permute(1, 0, 2, 3)\n",
    "        attention = F.softmax(scaled, dim=-1)\n",
    "        values = torch.matmul(attention, v)\n",
    "        return values, attention\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Perform forward pass of the multi-head attention mechanism.\n",
    "\n",
    "        :param x: if cross is True then x is a dictionary including  'encoder_output' and 'w'.\n",
    "        :param mask: Optional mask tensor\n",
    "        \n",
    "        :return: Output tensor of shape (batch_size, length_seq, d_model)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # For MultiHead Cross Attention\n",
    "        if self.cross:\n",
    "            encoder_output = x['encoder_output']\n",
    "            w = x['w']\n",
    "            batch_size, length_seq, d_model = w.size()\n",
    "            kv = self.kv_layer(w)\n",
    "            q = self.q_layer(encoder_output)\n",
    "            kv = kv.reshape(batch_size, length_seq, self.num_heads, 2 * self.head_dim)\n",
    "            q = q.reshape(batch_size, length_seq, self.num_heads, self.head_dim)\n",
    "            kv = kv.permute(0, 2, 1, 3)\n",
    "            q = q.permute(0, 2, 1, 3)\n",
    "            k, v = kv.chunk(2, dim=-1)\n",
    "            values, attention = self.scaled_dot_product(q, k, v, mask) # mask is not required in Cross Attention\n",
    "            values = values.permute(0, 2, 1, 3).reshape(batch_size, length_seq, self.num_heads * self.head_dim)\n",
    "            out = self.linear_layer(values)\n",
    "            return out\n",
    "\n",
    "        # For MultiHead Attention\n",
    "        batch_size, length_seq, d_model = x.size()\n",
    "        qkv = self.qkv_layer(x)\n",
    "        qkv = qkv.reshape(batch_size, length_seq, self.num_heads, 3 * self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        values, attention = self.scaled_dot_product(q, k, v, mask)\n",
    "        values = values.permute(0, 2, 1, 3).reshape(batch_size, length_seq, self.num_heads * self.head_dim)\n",
    "        out = self.linear_layer(values)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8207b97a-4398-4057-91a1-0c02dd9e315d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0562, -0.0427,  0.2180,  ...,  0.0322, -0.1573, -0.0523],\n",
       "         [ 0.0623, -0.0722,  0.2759,  ...,  0.0920, -0.0904, -0.0578],\n",
       "         [-0.0176, -0.0694,  0.2166,  ...,  0.1226, -0.0949,  0.0099],\n",
       "         ...,\n",
       "         [-0.0676, -0.0433,  0.2561,  ...,  0.0992, -0.0785, -0.0028],\n",
       "         [ 0.0559, -0.0197,  0.2547,  ...,  0.1085, -0.1260, -0.0105],\n",
       "         [-0.0468, -0.0607,  0.2399,  ...,  0.0299, -0.1175,  0.0150]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Example\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "\n",
    "\n",
    "# Attention\n",
    "mha_layer = MultiHeadAttention(d_model, num_heads)\n",
    "mha_layer(torch.randn(1,10,d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2f3315a-bff2-435c-9f9a-54b37630fc38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0175,  0.1712,  0.0623,  ...,  0.0460,  0.2901, -0.1208],\n",
       "         [ 0.0578,  0.0376, -0.0211,  ...,  0.0196,  0.3044, -0.0735],\n",
       "         [-0.0547,  0.2268,  0.0110,  ...,  0.0335,  0.3101, -0.1269],\n",
       "         ...,\n",
       "         [-0.0098,  0.1542,  0.0377,  ..., -0.0568,  0.3967, -0.0505],\n",
       "         [-0.0268,  0.1181,  0.0296,  ...,  0.0190,  0.3105, -0.0837],\n",
       "         [-0.0478,  0.1196,  0.0164,  ...,  0.0180,  0.2942, -0.1132]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross Attention\n",
    "mha_layer = MultiHeadAttention(d_model, num_heads,cross=True)\n",
    "mha_layer({'encoder_output':torch.randn(1,10,d_model),'w':torch.randn(1,10,d_model)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52d1ed4-ff0d-4988-b5b0-82e964758ff5",
   "metadata": {},
   "source": [
    "- **Layer Normalization Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d640c067-39fa-4181-ad87-6a6bb6cadca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape=parameters_shape\n",
    "        self.eps=eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
    "        mean = inputs.mean(dim=dims, keepdim=True)\n",
    "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        y = (inputs - mean) / std\n",
    "        out = self.gamma * y  + self.beta\n",
    "        return out\n",
    "\n",
    "# Or using nn.LayerNorm(d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "276d988f-e30e-4436-8a06-3ef7c11f13f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4980, -0.3382, -1.0320],\n",
       "         [ 1.5102, -0.8764,  1.2344]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Example\n",
    "ln = LayerNormalization((1,2,3))\n",
    "ln(torch.randn(1,2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bb68ec-eaf5-40e6-8ac2-21ab67b3e1a0",
   "metadata": {},
   "source": [
    "- **Positionwise Feed Forward Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79967c5c-330f-4ba9-ac6d-1cd1512843ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "# feed_forward = nn.Sequential(\n",
    "#     nn.Linear(d_model, expansion_factor * d_model),  # e.g: 512x(4*512) -> (512, 2048)\n",
    "#     nn.ReLU(),  # ReLU activation function\n",
    "#     nn.Linear(d_model * expansion_factor, d_model),  # e.g: 4*512)x512 -> (2048, 512)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9e15943-748f-45fa-a6de-41e33d52e6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Example\n",
    "ff = PositionwiseFeedForward(512, 300)\n",
    "ff(torch.randn(1,5,512)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2b3196-79bd-4808-bb96-ad2cab19acae",
   "metadata": {},
   "source": [
    "- **Copy Block Function**: we can use nn.Sequential but i think we don't need to do that because we don't have any changes in Module Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51af5dd9-2ae2-4f58-969a-5cd9e3e45c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replicate(block, N=6) -> nn.ModuleList:\n",
    "    \"\"\"\n",
    "    Method to replicate the existing block to N set of blocks\n",
    "    :param block: class inherited from nn.Module, mainly it is the encoder or decoder part of the architecture\n",
    "    :param N: the number of stack, in the original paper they used 6\n",
    "    :return: a set of N blocks\n",
    "    \"\"\"\n",
    "    block_stack = nn.ModuleList([copy.deepcopy(block) for _ in range(N)])\n",
    "    return block_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cd5d10-d943-4116-8d07-f32885077b2a",
   "metadata": {},
   "source": [
    "## With those small blocks and functions, let's build these important blocks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f430efe3-685e-485d-b44d-48af49b286d7",
   "metadata": {},
   "source": [
    "- **Preprocessing** for Input Pre-processing and Output Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f5b0acc-5acf-4a60-b2a7-76592cddd9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing(nn.Module):\n",
    "\n",
    "    def __init__(self, max_length_seq, d_model, language_to_index, start_token, end_token, pad_token, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = len(language_to_index)\n",
    "        self.language_to_index = language_to_index\n",
    "        self.max_length_seq = max_length_seq\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "        # Layer\n",
    "        self.token_embedding = TokenEmbedding(self.vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_length_seq, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def tokenize(self, sentence, start_token:bool, end_token:bool):\n",
    "        encode_char = [self.language_to_index[token] for token in list(sentence)]\n",
    "        if start_token:\n",
    "            encode_char.insert(0, self.language_to_index[self.start_token])\n",
    "        if end_token:\n",
    "            encode_char.append(self.language_to_index[self.end_token])\n",
    "        for _ in range(len(encode_char), self.max_length_seq):\n",
    "            encode_char.append(self.language_to_index[self.pad_token])\n",
    "        return torch.tensor(encode_char)\n",
    "    \n",
    "    def batch_tokens(self, batch, start_token:bool, end_token:bool):\n",
    "        tokens = []\n",
    "        for i in range(len(batch)):\n",
    "            tokens.append(self.tokenize(batch[i], start_token, end_token))\n",
    "        tokens = torch.stack(tokens)\n",
    "        return tokens.to(get_device())\n",
    "\n",
    "    def forward(self, x, start_token:bool, end_token:bool): \n",
    "        x = self.batch_tokens(x, start_token, end_token)\n",
    "        x = self.token_embedding(x)\n",
    "        pos = self.positional_encoding().to(get_device())\n",
    "        x = self.dropout(x + pos)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf2e50e-5dce-46d5-b9c5-f5145367baf5",
   "metadata": {},
   "source": [
    "- **Transformer Block** includes: **Multi-Head Attention**, **Add & Norm**, **Feed & Forward** and **Dropout**\n",
    "\n",
    "2 options: 'Encoder' and 'Decoder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "852e35db-7a0f-4804-b164-fb36427c7677",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model=512,\n",
    "                 num_heads=8,\n",
    "                 ff_hidden=300,\n",
    "                 dropout=0.1,\n",
    "                 options='encoder'\n",
    "                ):\n",
    "        \"\"\"\n",
    "        The Transformer Block used in the encoder and decoder as well\n",
    "\n",
    "        :param d_model: the embedding dimension\n",
    "        :param num_heads: the number of heads\n",
    "        :param ff_hidden: The output dimension of the feed forward layer\n",
    "        :param dropout: probability dropout (between 0 and 1)\n",
    "        :param options: The choice between 'encoder' and 'decoder'\n",
    "        \"\"\"\n",
    "        super(TransformerBlock, self).__init__()\n",
    "    \n",
    "        self.options = options\n",
    "        \n",
    "        # For both 2 options: encoder and decoder\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm_for_attention = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout_attention = nn.Dropout(dropout)\n",
    "\n",
    "        \n",
    "        \n",
    "        # For decoder\n",
    "        if self.options=='decoder':\n",
    "            self.cross_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads, cross=True)\n",
    "            self.norm_for_cross_attention = LayerNormalization(parameters_shape=[d_model])\n",
    "            self.dropout2 = nn.Dropout(dropout)\n",
    "        elif self.options!='encoder':\n",
    "            raise Exception(f\"Unknown option {options}\")\n",
    "\n",
    "        # For both 2 options: encoder and decoder\n",
    "        self.ff = PositionwiseFeedForward(d_model=d_model, hidden=ff_hidden, drop_prob=dropout)\n",
    "        self.norm_for_ff = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout_for_ff = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # For decoder\n",
    "        if self.options == 'decoder':\n",
    "            encoder_output = x['encoder_output']\n",
    "            w = x['w']\n",
    "            w_residual = w.clone()\n",
    "            w = self.attention(w,mask['self_attention_mask'])\n",
    "            w = self.dropout_attention(w)\n",
    "            w = self.norm_for_attention(w + w_residual)\n",
    "\n",
    "            w_residual = w.clone()\n",
    "            w = self.cross_attention({'encoder_output':encoder_output,'w':w},mask['cross_attention_mask'])\n",
    "            w = self.dropout2(w)\n",
    "            w = self.norm_for_cross_attention(w + w_residual)\n",
    "\n",
    "            w_residual = w.clone()\n",
    "            w = self.ff(w)\n",
    "            w = self.dropout_for_ff(w)\n",
    "            w = self.norm_for_ff(w + w_residual)\n",
    "            return w\n",
    "        else:\n",
    "        # For encoder\n",
    "            x_residual = x.clone()\n",
    "            x = self.attention(x, mask)\n",
    "            x = self.dropout_attention(x)\n",
    "            x = self.norm_for_attention(x + x_residual)\n",
    "\n",
    "            x_residual = x.clone()\n",
    "            x = self.ff(x)\n",
    "            x = self.dropout_for_ff(x)\n",
    "            x = self.norm_for_ff(x + x_residual)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db60b8d3-ed32-44fa-83dd-637d7a202e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7325, -0.7324, -0.8844,  ..., -0.1414, -1.8609,  1.2694],\n",
       "         [-0.0189,  1.1276,  1.0198,  ...,  0.4492, -0.8654,  0.8697],\n",
       "         [-0.9913,  2.0883,  0.5105,  ...,  0.0388,  1.1496,  0.2762],\n",
       "         ...,\n",
       "         [-0.2725, -0.1853, -0.0847,  ...,  0.4693, -1.0658,  1.9107],\n",
       "         [ 0.2683,  1.4403,  1.5199,  ...,  1.1040,  1.2863,  1.2746],\n",
       "         [-1.2582,  2.7587,  2.7908,  ...,  0.2152, -0.0603, -0.3052]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "trans_block = TransformerBlock()\n",
    "trans_block(torch.randn(1,10,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8bc025b5-d22e-4a7a-91db-1cbdcb91895e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0348,  0.1849,  0.0468,  ...,  1.0645, -0.3162, -0.3297],\n",
       "         [-0.7798,  0.5172, -0.7827,  ...,  0.2869,  0.8152,  1.2620],\n",
       "         [-1.7776,  0.9825, -0.1279,  ...,  0.9662,  0.9253,  0.5077],\n",
       "         ...,\n",
       "         [-1.1933,  0.9722, -0.7518,  ..., -0.3489, -0.6589, -0.6248],\n",
       "         [-0.2129,  0.3359,  0.3757,  ...,  0.1093, -0.9151,  0.4977],\n",
       "         [ 0.4353,  0.2861, -0.8510,  ..., -0.5683,  0.0124,  1.2205]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_block = TransformerBlock(options='decoder')\n",
    "trans_block({'encoder_output':torch.randn(1,10,d_model),'w':torch.randn(1,10,d_model)},\n",
    "    {'self_attention_mask': None, 'cross_attention_mask': None}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0145ab0a-1473-4b97-8ce0-4f716f731674",
   "metadata": {},
   "source": [
    "- **Encoder** includes Input Pre-processing (**Token Embedding** & **Positional Encoding**) and N **Transformer Block** (Encode block in the picture on the top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d59c851-23c2-412a-b9b9-e9024bd74a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 ff_hidden,\n",
    "                 num_heads,\n",
    "                 dropout,\n",
    "                 num_blocks,\n",
    "                 max_length_seq,\n",
    "                 language_to_index,\n",
    "                 start_token, \n",
    "                 end_token, \n",
    "                 pad_token\n",
    "                ):\n",
    "        \"\"\"\n",
    "        The Encoder part of the Transformer architecture\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Layer\n",
    "        self.input_preprocessing = Preprocessing(max_length_seq, d_model, language_to_index, start_token, end_token, pad_token, dropout)\n",
    "        \n",
    "        # Transformer Blocks\n",
    "        self.transformer_blocks = replicate(TransformerBlock(d_model, num_heads, ff_hidden, dropout, options=\"encoder\"),num_blocks)\n",
    "\n",
    "    def forward(self, x, self_attention_mask, start_token:bool, end_token:bool):\n",
    "        # Input Pre-processing: Token Embedding + Positional Encoding\n",
    "        out = self.input_preprocessing(x, start_token, end_token)\n",
    "\n",
    "        # Go to Transformer Blocks (Encode)\n",
    "        for block in self.transformer_blocks:\n",
    "            out = block(out, self_attention_mask)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf4215e2-b0c5-486d-a4b1-1dc8e3555209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1834,  0.4173, -1.4288,  ..., -0.8192,  0.0781,  1.8494],\n",
       "         [ 0.0085, -0.0522,  0.3505,  ..., -0.8484, -0.7208, -1.0828],\n",
       "         [ 0.2975, -2.2400, -0.9833,  ...,  0.8163, -0.2242,  0.1052],\n",
       "         ...,\n",
       "         [-0.1892, -0.5154, -0.9726,  ...,  1.6462, -1.0744,  1.0129],\n",
       "         [-0.3732, -0.3893,  0.0549,  ...,  1.5311, -0.8773,  0.8742],\n",
       "         [-1.2761, -0.1116,  0.7824,  ...,  1.4351, -0.8063,  0.7314]],\n",
       "\n",
       "        [[ 0.8681,  1.5104, -0.2532,  ...,  0.2230, -1.6173,  0.2736],\n",
       "         [ 1.9741,  1.4769,  0.1803,  ...,  0.6149, -2.3257,  0.1587],\n",
       "         [ 1.8738,  0.7153,  0.3597,  ...,  0.5758, -2.6589,  0.0357],\n",
       "         ...,\n",
       "         [ 0.1281, -0.6617, -0.9936,  ...,  1.5914, -0.9194,  0.9107],\n",
       "         [-0.2175, -0.4650,  0.0308,  ...,  1.3927, -0.7941,  0.9496],\n",
       "         [-1.3464,  0.2845,  0.4976,  ...,  1.4611, -0.7491,  0.7999]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "special_tokens = [\"<start>\", \"<end>\", \"<pad>\"]\n",
    "alphabet = list(\"abcdefghijklmnopqrstuvwxyz \") + special_tokens\n",
    "language_to_index = {word: idx for idx, word in enumerate(alphabet)}\n",
    "encoder = Encoder(d_model=100, ff_hidden=50, num_heads=2, dropout=0.1 ,num_blocks=1, max_length_seq=100, language_to_index=language_to_index, start_token=\"<start>\", end_token=\"<end>\", pad_token=\"<pad>\")\n",
    "encoder.to(get_device())\n",
    "batch = ['hello','goodbye']\n",
    "encoder(batch, None, False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58513193-9156-41c9-8861-6464a3ae3341",
   "metadata": {},
   "source": [
    "- **Decoder** includes **Output Pre-processing** (**Token Embedding** & **Positional Encoding**), N **Transformer Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "64c335b7-26ce-447f-96d3-449efe7bf12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 ff_hidden,\n",
    "                 num_heads,\n",
    "                 dropout,\n",
    "                 num_blocks,\n",
    "                 max_length_seq,\n",
    "                 language_to_index,\n",
    "                 start_token, \n",
    "                 end_token, \n",
    "                 pad_token\n",
    "                ):\n",
    "        \"\"\"\n",
    "        The Decoder part of the Transformer architecture\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "         # Layer\n",
    "        self.output_preprocessing = Preprocessing(max_length_seq, d_model, language_to_index, start_token, end_token, pad_token, dropout)\n",
    "        \n",
    "        # Transformer Blocks\n",
    "        self.transformer_blocks = replicate(TransformerBlock(d_model, num_heads, ff_hidden, dropout, options=\"decoder\"),num_blocks)\n",
    "\n",
    "    def forward(self, x, y, self_attention_mask, cross_attention_mask, start_token:bool, end_token:bool): \n",
    "        # x is output, y is output from encoder\n",
    "        # Output Pre-processing: Token Embedding + Positional Encoding\n",
    "        x = self.output_preprocessing(x, start_token, end_token)\n",
    "\n",
    "        # Go to Transformer Blocks (Decode)\n",
    "        encode_decode = {'encoder_output': y,'w':x}\n",
    "        mask = {'self_attention_mask': self_attention_mask,'cross_attention_mask': cross_attention_mask}\n",
    "        for block in self.transformer_blocks:\n",
    "            encode_decode['w'] = x\n",
    "            x = block(encode_decode, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadb9ff7-e4f5-4a9c-b1c3-2c434216fe36",
   "metadata": {},
   "source": [
    "## Finally, The Transformer Architecture is complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "724efbec-53d8-4efd-9469-58df2f117555",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 ff_hidden,\n",
    "                 num_heads,\n",
    "                 dropout,\n",
    "                 num_blocks,\n",
    "                 max_length_seq,\n",
    "                 language_to_index,\n",
    "                 target_language_to_index,\n",
    "                 start_token, \n",
    "                 end_token, \n",
    "                 pad_token\n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Device\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            d_model=d_model,\n",
    "            ff_hidden=ff_hidden,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            num_blocks=num_blocks,\n",
    "            max_length_seq=max_length_seq,\n",
    "            language_to_index=language_to_index,\n",
    "            start_token=start_token,\n",
    "            end_token=end_token,\n",
    "            pad_token=pad_token\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(\n",
    "            d_model=d_model,\n",
    "            ff_hidden=ff_hidden,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            num_blocks=num_blocks,\n",
    "            max_length_seq=max_length_seq,\n",
    "            language_to_index=target_language_to_index,\n",
    "            start_token=start_token,\n",
    "            end_token=end_token,\n",
    "            pad_token=pad_token\n",
    "        )\n",
    "\n",
    "        # Linear Layer\n",
    "        self.linear = nn.Linear(d_model, len(target_language_to_index))\n",
    "\n",
    "        # Softmax\n",
    "        \n",
    "\n",
    "    def forward(self,\n",
    "                x,\n",
    "                y,\n",
    "                encoder_self_attention_mask=None,\n",
    "                decoder_self_attention_mask=None,\n",
    "                decoder_cross_attention_mask=None,\n",
    "                encoder_start_token=False,\n",
    "                encoder_end_token=False,\n",
    "                decoder_start_token=False,\n",
    "                decoder_end_token=False):\n",
    "        encoder_output = self.encoder(x, encoder_self_attention_mask, encoder_start_token, encoder_end_token)\n",
    "        out = self.decoder(y, encoder_output, decoder_self_attention_mask, decoder_cross_attention_mask, decoder_start_token, decoder_end_token)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "afa02306-57b4-481e-84de-0fe43bb2de9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (input_preprocessing): Preprocessing(\n",
       "      (token_embedding): TokenEmbedding(\n",
       "        (embedding_layer): Embedding(30, 512)\n",
       "      )\n",
       "      (positional_encoding): PositionalEncoding(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm_for_attention): LayerNormalization()\n",
       "        (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "        (ff): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=3000, bias=True)\n",
       "          (linear2): Linear(in_features=3000, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm_for_ff): LayerNormalization()\n",
       "        (dropout_for_ff): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (output_preprocessing): Preprocessing(\n",
       "      (token_embedding): TokenEmbedding(\n",
       "        (embedding_layer): Embedding(30, 512)\n",
       "      )\n",
       "      (positional_encoding): PositionalEncoding(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm_for_attention): LayerNormalization()\n",
       "        (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "        (cross_attention): MultiHeadAttention(\n",
       "          (kv_layer): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (q_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm_for_cross_attention): LayerNormalization()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ff): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=3000, bias=True)\n",
       "          (linear2): Linear(in_features=3000, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm_for_ff): LayerNormalization()\n",
       "        (dropout_for_ff): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=30, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just an example to work with (Don't care about right or wrong)\n",
    "special_tokens = [\"<start>\", \"<end>\", \"<pad>\"]\n",
    "alphabet = list(\"abcdefghijklmnopqrstuvwxyz \") + special_tokens\n",
    "language_to_index = {word: idx for idx, word in enumerate(alphabet)}\n",
    "\n",
    "model = Transformer(512, 3000, 8, 0.1, 1, 100, language_to_index, language_to_index, \"<start>\", \"<end>\", \"<pad>\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5d0154f2-2def-44ae-8aef-b36759b77127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2282,  1.1035,  0.2848,  ..., -0.1102, -0.4442, -0.3117],\n",
       "         [-0.8254, -0.1476,  0.6141,  ..., -0.5090,  0.1984,  0.2809],\n",
       "         [-1.0713,  0.7109,  0.3258,  ..., -0.9360,  0.2301, -0.0523],\n",
       "         ...,\n",
       "         [ 0.4032, -0.3972, -0.4304,  ..., -0.3845,  0.3783,  0.7064],\n",
       "         [ 0.7196, -0.2915, -0.3925,  ..., -0.2429,  0.0862,  0.3884],\n",
       "         [ 0.1167, -0.0694, -0.0647,  ..., -0.4290,  0.0512,  0.2166]],\n",
       "\n",
       "        [[-0.1074, -0.9415,  0.0861,  ...,  0.2044,  0.6242, -0.1079],\n",
       "         [-0.0246,  0.2943, -0.1001,  ...,  0.0617,  0.8480, -0.8726],\n",
       "         [-1.1477, -0.4672,  1.1439,  ...,  0.5774,  1.7076,  0.1687],\n",
       "         ...,\n",
       "         [ 0.1420, -0.1864, -0.7107,  ..., -0.3897, -0.1633,  0.4926],\n",
       "         [ 0.0744, -0.0528, -0.4178,  ..., -0.2942, -0.0717,  0.2624],\n",
       "         [ 0.2281, -0.0058, -0.2660,  ..., -0.3544, -0.3754,  0.2085]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(get_device())\n",
    "model(['hello','goodbye'],['xin chao','tam biet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab8045-7a8b-4c2b-97c5-2fc5748556ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TorchCuda",
   "language": "python",
   "name": "torchcuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
