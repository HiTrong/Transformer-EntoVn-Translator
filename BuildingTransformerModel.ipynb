{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce87a780-5d95-4d1f-b044-e916bee3cd14",
   "metadata": {},
   "source": [
    "# Building Transformer Model - Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f715195-33e0-4cd2-bea6-3877af2256dd",
   "metadata": {},
   "source": [
    "![Transformer Architecture](./Transformer_Understanding/img/transformerblock.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60ca720-9523-4748-a135-504f14310a63",
   "metadata": {},
   "source": [
    "Base on the upper picture, we will make our model functions with Transformer Architecture following the things we have been researching in the \"[Transformer Understanding](./Transformer_Understanding/TransformerNeuralNetworks.ipynb)\" part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8398fa25-9656-40a2-8a69-eb01e46cb1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import copy\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99343a2f-e0c9-4aa9-a6b1-2d101f8132cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d0c0e63-2dca-4b55-bbc3-3f51283811c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. PyTorch is using GPU.\n",
      "Number of GPUs available:  1\n",
      "GPU name:  NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. PyTorch is using GPU.\")\n",
    "    print(\"Number of GPUs available: \", torch.cuda.device_count())\n",
    "    print(\"GPU name: \", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch is using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44143e5c-22f3-4b4b-ba98-850d5512a972",
   "metadata": {},
   "source": [
    "## Start with these small blocks, functions, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de739e-d0e0-4776-ac43-aa0110974965",
   "metadata": {},
   "source": [
    "- **Get Using Device** (Torch requires all tensor  must be in the same device.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb05b66a-9351-4aa7-9276-f6bb32178767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cb8fed-58b8-4375-bc10-6134d4894127",
   "metadata": {},
   "source": [
    "- **Token Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ab64882-f3ce-4a20-8cb9-5f62b81df436",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        \"\"\"\n",
    "        Token Embedding is used for converting a word / token into a embedding numeric vector space.\n",
    "        \n",
    "        :param vocab_size: Number of words / token in vocabulary\n",
    "        :param d_model: The embedding dimension\n",
    "        \n",
    "        Example: With 1000 words in vocabulary and our embedding dimension is 512, the Token Embedding layer will be 1000x512\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: the word or sequence of words\n",
    "        :return: the numerical representation of the input\n",
    "        \n",
    "        Example:\n",
    "        Input: (Batch_size, Sequence of words) - (30x100)\n",
    "        Output: (Batch_size, Sequence of words, d_model) - (30x100x512)\n",
    "        \"\"\"\n",
    "        x = self.embedding_layer(x)\n",
    "        return x.to(get_device())\n",
    "\n",
    "# Or just Simple\n",
    "# token_embedding = nn.Embedding(vocab_size, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7837bba9-74c4-417f-8555-370e6923fa27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 100, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Example\n",
    "vocab_size = 1000\n",
    "d_model = 512\n",
    "\n",
    "embedding_layer = TokenEmbedding(vocab_size, d_model)\n",
    "input_data = torch.randint(0, vocab_size, (30, 100))\n",
    "embedding_layer(input_data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e47d72-f888-491c-82ab-e3ea6a67f6c1",
   "metadata": {},
   "source": [
    "- **Positional Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b935a5a-5a1a-47fe-a6d9-b234e7f96055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, max_sequence_length, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Positional Encoding layer for adding positional information to token embeddings.\n",
    "        \n",
    "        :param d_model: The embedding dimension.\n",
    "        :param max_sequence_length: The maximum length of the input sequences.\n",
    "        :param dropout: Dropout rate.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding,self).__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self):\n",
    "        even_i = torch.arange(0, self.d_model, 2).float()\n",
    "        denominator = torch.pow(10000, even_i/self.d_model)\n",
    "        position = (torch.arange(self.max_sequence_length)\n",
    "                          .reshape(self.max_sequence_length, 1))\n",
    "        even_PE = torch.sin(position / denominator)\n",
    "        odd_PE = torch.cos(position / denominator)\n",
    "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
    "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "        PE = PE.unsqueeze(0)\n",
    "        return self.dropout(PE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0565979b-3e58-49a6-8fd4-4a375bd24df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PE = PositionalEncoding(512,100,0.1)\n",
    "PE().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00ab53f-2eb1-4237-931f-4bc4f65fac9f",
   "metadata": {},
   "source": [
    "- **Multi-Head Attention**\n",
    "\n",
    "2 options for: 'encoder' and 'decoder' (**Multi-Head Cross Attention**)\n",
    "\n",
    "options for mask: **None**, **Self-Attention Mask** (**Causal Mask** hoáº·c **Look-Ahead Mask**), **Padding Mask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e69746d2-092e-4b4b-a451-4f838cb03d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads=8, cross=False):\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        :param d_model: the embedding dimension\n",
    "        :param num_heads: the number of heads, default equals 8\n",
    "        :param cross: True for Multi-Head Cross Attention, False for Multi-Head Attention only\n",
    "        \n",
    "        # note: The embedding dimension must be divided by the number of heads\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.cross = cross\n",
    "\n",
    "        # query, key value layer\n",
    "        if self.cross: # Multi-Head Cross Attention\n",
    "            self.kv_layer = nn.Linear(d_model , 2 * d_model)\n",
    "            self.q_layer = nn.Linear(d_model , d_model)\n",
    "        else:\n",
    "            self.qkv_layer = nn.Linear(d_model , 3 * d_model)\n",
    "        \n",
    "        \n",
    "        # method 1: old, cost alot\n",
    "        # self.query = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        # self.key = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        # self.value = nn.Linear(self.head_dim, self.head_dim, bias=False) \n",
    "\n",
    "        # method 2: the fewer linear layers the better the cost\n",
    "        \n",
    "        \n",
    "        # Linear Layer in Multi-Head Attention\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product(self, q, k, v, mask=None):\n",
    "        d_k = q.size()[-1]\n",
    "        scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scaled = scaled.permute(1, 0, 2, 3) + mask\n",
    "            scaled = scaled.permute(1, 0, 2, 3)\n",
    "        attention = F.softmax(scaled, dim=-1)\n",
    "        values = torch.matmul(attention, v)\n",
    "        return values, attention\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Perform forward pass of the multi-head attention mechanism.\n",
    "\n",
    "        :param x: if cross is True then x is a dictionary including  'encoder_output' and 'w'.\n",
    "        :param mask: Optional mask tensor\n",
    "        \n",
    "        :return: Output tensor of shape (batch_size, length_seq, d_model)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # For MultiHead Cross Attention\n",
    "        if self.cross:\n",
    "            encoder_output = x['encoder_output']\n",
    "            w = x['w']\n",
    "            batch_size, length_seq, d_model = w.size()\n",
    "            kv = self.kv_layer(w)\n",
    "            q = self.q_layer(encoder_output)\n",
    "            kv = kv.reshape(batch_size, length_seq, self.num_heads, 2 * self.head_dim)\n",
    "            q = q.reshape(batch_size, length_seq, self.num_heads, self.head_dim)\n",
    "            kv = kv.permute(0, 2, 1, 3)\n",
    "            q = q.permute(0, 2, 1, 3)\n",
    "            k, v = kv.chunk(2, dim=-1)\n",
    "            values, attention = self.scaled_dot_product(q, k, v, mask) # mask is not required in Cross Attention\n",
    "            values = values.permute(0, 2, 1, 3).reshape(batch_size, length_seq, self.num_heads * self.head_dim)\n",
    "            out = self.linear_layer(values)\n",
    "            return out\n",
    "\n",
    "        # For MultiHead Attention\n",
    "        batch_size, length_seq, d_model = x.size()\n",
    "        qkv = self.qkv_layer(x)\n",
    "        qkv = qkv.reshape(batch_size, length_seq, self.num_heads, 3 * self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        values, attention = self.scaled_dot_product(q, k, v, mask)\n",
    "        values = values.permute(0, 2, 1, 3).reshape(batch_size, length_seq, self.num_heads * self.head_dim)\n",
    "        out = self.linear_layer(values)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8207b97a-4398-4057-91a1-0c02dd9e315d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1934, -0.1114, -0.1102,  ...,  0.2552, -0.0385,  0.0420],\n",
       "         [ 0.1294, -0.0963, -0.0465,  ...,  0.2400, -0.0796,  0.0796],\n",
       "         [ 0.1541, -0.1141, -0.0579,  ...,  0.2480,  0.0236, -0.0210],\n",
       "         ...,\n",
       "         [ 0.0971, -0.1248, -0.0860,  ...,  0.2161, -0.1267,  0.0463],\n",
       "         [ 0.1686, -0.1130, -0.0474,  ...,  0.2752, -0.0954, -0.0198],\n",
       "         [ 0.1757, -0.1306, -0.0977,  ...,  0.2291, -0.0526,  0.0270]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Example\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "\n",
    "\n",
    "# Attention\n",
    "mha_layer = MultiHeadAttention(d_model, num_heads)\n",
    "mha_layer(torch.randn(1,10,d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2f3315a-bff2-435c-9f9a-54b37630fc38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1164,  0.1507, -0.1053,  ...,  0.1042, -0.2139,  0.2552],\n",
       "         [-0.1215,  0.0759, -0.1088,  ...,  0.0505, -0.1319,  0.1522],\n",
       "         [-0.1225,  0.1095, -0.1695,  ...,  0.0866, -0.1963,  0.2787],\n",
       "         ...,\n",
       "         [-0.0864,  0.1292, -0.1470,  ...,  0.1072, -0.1512,  0.2815],\n",
       "         [-0.0796,  0.0907, -0.1386,  ...,  0.1461, -0.1886,  0.2560],\n",
       "         [-0.0626,  0.0965, -0.1146,  ...,  0.1255, -0.1642,  0.3000]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross Attention\n",
    "mha_layer = MultiHeadAttention(d_model, num_heads,cross=True)\n",
    "mha_layer({'encoder_output':torch.randn(1,10,d_model),'w':torch.randn(1,10,d_model)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52d1ed4-ff0d-4988-b5b0-82e964758ff5",
   "metadata": {},
   "source": [
    "- **Layer Normalization Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d640c067-39fa-4181-ad87-6a6bb6cadca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape=parameters_shape\n",
    "        self.eps=eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
    "        mean = inputs.mean(dim=dims, keepdim=True)\n",
    "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        y = (inputs - mean) / std\n",
    "        out = self.gamma * y  + self.beta\n",
    "        return out\n",
    "\n",
    "# Or using nn.LayerNorm(d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "276d988f-e30e-4436-8a06-3ef7c11f13f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5837,  1.2111, -0.9032],\n",
       "         [-0.5793, -1.3794,  1.0670]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Example\n",
    "ln = LayerNormalization((1,2,3))\n",
    "ln(torch.randn(1,2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bb68ec-eaf5-40e6-8ac2-21ab67b3e1a0",
   "metadata": {},
   "source": [
    "- **Positionwise Feed Forward Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79967c5c-330f-4ba9-ac6d-1cd1512843ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "# feed_forward = nn.Sequential(\n",
    "#     nn.Linear(d_model, expansion_factor * d_model),  # e.g: 512x(4*512) -> (512, 2048)\n",
    "#     nn.ReLU(),  # ReLU activation function\n",
    "#     nn.Linear(d_model * expansion_factor, d_model),  # e.g: 4*512)x512 -> (2048, 512)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9e15943-748f-45fa-a6de-41e33d52e6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Example\n",
    "ff = PositionwiseFeedForward(512, 300)\n",
    "ff(torch.randn(1,5,512)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2b3196-79bd-4808-bb96-ad2cab19acae",
   "metadata": {},
   "source": [
    "- **Copy Block Function**: we can use nn.Sequential but i think we don't need to do that because we don't have any changes in Module Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51af5dd9-2ae2-4f58-969a-5cd9e3e45c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replicate(block, N=6) -> nn.ModuleList:\n",
    "    \"\"\"\n",
    "    Method to replicate the existing block to N set of blocks\n",
    "    :param block: class inherited from nn.Module, mainly it is the encoder or decoder part of the architecture\n",
    "    :param N: the number of stack, in the original paper they used 6\n",
    "    :return: a set of N blocks\n",
    "    \"\"\"\n",
    "    block_stack = nn.ModuleList([copy.deepcopy(block) for _ in range(N)])\n",
    "    return block_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cd5d10-d943-4116-8d07-f32885077b2a",
   "metadata": {},
   "source": [
    "## With those small blocks and functions, let's build these important blocks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f430efe3-685e-485d-b44d-48af49b286d7",
   "metadata": {},
   "source": [
    "- **Preprocessing** for Input Pre-processing and Output Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f5b0acc-5acf-4a60-b2a7-76592cddd9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing(nn.Module):\n",
    "\n",
    "    def __init__(self, max_length_seq, d_model, language_to_index, start_token, end_token, pad_token, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = len(language_to_index)\n",
    "        self.language_to_index = language_to_index\n",
    "        self.max_length_seq = max_length_seq\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "        # Layer\n",
    "        self.token_embedding = TokenEmbedding(self.vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_length_seq, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def tokenize(self, sentence, start_token:bool, end_token:bool):\n",
    "        encode_char = [self.language_to_index[token] for token in list(sentence)]\n",
    "        if start_token:\n",
    "            encode_char.insert(0, self.language_to_index[self.start_token])\n",
    "        if end_token:\n",
    "            encode_char.append(self.language_to_index[self.end_token])\n",
    "        for _ in range(len(encode_char), self.max_length_seq):\n",
    "            encode_char.append(self.language_to_index[self.pad_token])\n",
    "        return torch.tensor(encode_char)\n",
    "    \n",
    "    def batch_tokens(self, batch, start_token:bool, end_token:bool):\n",
    "        tokens = []\n",
    "        for i in range(len(batch)):\n",
    "            tokens.append(self.tokenize(batch[i], start_token, end_token))\n",
    "        tokens = torch.stack(tokens)\n",
    "        return tokens.to(get_device())\n",
    "\n",
    "    def forward(self, x, start_token:bool, end_token:bool): \n",
    "        x = self.batch_tokens(x, start_token, end_token)\n",
    "        x = self.token_embedding(x)\n",
    "        pos = self.positional_encoding().to(get_device())\n",
    "        x = self.dropout(x + pos)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf2e50e-5dce-46d5-b9c5-f5145367baf5",
   "metadata": {},
   "source": [
    "- **Transformer Block** includes: **Multi-Head Attention**, **Add & Norm**, **Feed & Forward** and **Dropout**\n",
    "\n",
    "2 options: 'Encoder' and 'Decoder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "852e35db-7a0f-4804-b164-fb36427c7677",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model=512,\n",
    "                 num_heads=8,\n",
    "                 ff_hidden=300,\n",
    "                 dropout=0.1,\n",
    "                 options='encoder'\n",
    "                ):\n",
    "        \"\"\"\n",
    "        The Transformer Block used in the encoder and decoder as well\n",
    "\n",
    "        :param d_model: the embedding dimension\n",
    "        :param num_heads: the number of heads\n",
    "        :param ff_hidden: The output dimension of the feed forward layer\n",
    "        :param dropout: probability dropout (between 0 and 1)\n",
    "        :param options: The choice between 'encoder' and 'decoder'\n",
    "        \"\"\"\n",
    "        super(TransformerBlock, self).__init__()\n",
    "    \n",
    "        self.options = options\n",
    "        \n",
    "        # For both 2 options: encoder and decoder\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm_for_attention = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout_attention = nn.Dropout(dropout)\n",
    "\n",
    "        \n",
    "        \n",
    "        # For decoder\n",
    "        if self.options=='decoder':\n",
    "            self.cross_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads, cross=True)\n",
    "            self.norm_for_cross_attention = LayerNormalization(parameters_shape=[d_model])\n",
    "            self.dropout2 = nn.Dropout(dropout)\n",
    "        elif self.options!='encoder':\n",
    "            raise Exception(f\"Unknown option {options}\")\n",
    "\n",
    "        # For both 2 options: encoder and decoder\n",
    "        self.ff = PositionwiseFeedForward(d_model=d_model, hidden=ff_hidden, drop_prob=dropout)\n",
    "        self.norm_for_ff = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout_for_ff = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # For decoder\n",
    "        if self.options == 'decoder':\n",
    "            encoder_output = x['encoder_output']\n",
    "            w = x['w']\n",
    "            w_residual = w.clone()\n",
    "            w = self.attention(w,mask['self_attention_mask'])\n",
    "            w = self.dropout_attention(w)\n",
    "            w = self.norm_for_attention(w + w_residual)\n",
    "\n",
    "            w_residual = w.clone()\n",
    "            w = self.cross_attention({'encoder_output':encoder_output,'w':w},mask['cross_attention_mask'])\n",
    "            w = self.dropout2(w)\n",
    "            w = self.norm_for_cross_attention(w + w_residual)\n",
    "\n",
    "            w_residual = w.clone()\n",
    "            w = self.ff(w)\n",
    "            w = self.dropout_for_ff(w)\n",
    "            w = self.norm_for_ff(w + w_residual)\n",
    "            return w\n",
    "        else:\n",
    "        # For encoder\n",
    "            x_residual = x.clone()\n",
    "            x = self.attention(x, mask)\n",
    "            x = self.dropout_attention(x)\n",
    "            x = self.norm_for_attention(x + x_residual)\n",
    "\n",
    "            x_residual = x.clone()\n",
    "            x = self.ff(x)\n",
    "            x = self.dropout_for_ff(x)\n",
    "            x = self.norm_for_ff(x + x_residual)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db60b8d3-ed32-44fa-83dd-637d7a202e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.6347e+00,  2.9519e-01, -5.0073e-01,  ..., -9.8680e-01,\n",
       "           5.5744e-02, -1.0531e+00],\n",
       "         [ 3.3081e-01,  1.2031e+00,  7.6933e-01,  ...,  3.4544e-01,\n",
       "          -6.9123e-01,  1.3671e-01],\n",
       "         [ 1.3679e+00, -6.7214e-01, -6.1000e-01,  ..., -1.1033e-03,\n",
       "           1.6234e+00,  7.3969e-01],\n",
       "         ...,\n",
       "         [-1.2435e-01, -2.5058e-01, -1.4812e-01,  ...,  1.1968e+00,\n",
       "           4.7031e-01, -2.1733e-01],\n",
       "         [-1.4620e-01, -1.7710e+00,  1.8269e-01,  ...,  5.5685e-01,\n",
       "           9.5737e-01,  3.6664e-01],\n",
       "         [ 1.5424e+00,  2.7748e+00, -7.2107e-01,  ..., -1.7869e+00,\n",
       "          -9.5714e-01, -6.5211e-01]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "trans_block = TransformerBlock()\n",
    "trans_block(torch.randn(1,10,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bc025b5-d22e-4a7a-91db-1cbdcb91895e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1136,  0.3166, -0.3398,  ...,  0.6556, -0.3054,  1.0212],\n",
       "         [-1.0302, -0.2919, -0.5072,  ...,  0.4768, -1.1288, -0.4150],\n",
       "         [ 0.4223, -0.3623,  1.4023,  ...,  0.0115,  0.3815,  0.3529],\n",
       "         ...,\n",
       "         [-0.0731, -0.3068, -0.6297,  ..., -0.5859,  2.2606, -2.1841],\n",
       "         [ 0.2536, -2.5272,  0.3633,  ..., -1.6203, -0.5734,  1.2083],\n",
       "         [ 0.4487,  0.3852, -1.1425,  ..., -0.1755, -0.5927, -0.9773]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_block = TransformerBlock(options='decoder')\n",
    "trans_block({'encoder_output':torch.randn(1,10,d_model),'w':torch.randn(1,10,d_model)},\n",
    "    {'self_attention_mask': None, 'cross_attention_mask': None}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0145ab0a-1473-4b97-8ce0-4f716f731674",
   "metadata": {},
   "source": [
    "- **Encoder** includes Input Pre-processing (**Token Embedding** & **Positional Encoding**) and N **Transformer Block** (Encode block in the picture on the top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d59c851-23c2-412a-b9b9-e9024bd74a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 ff_hidden,\n",
    "                 num_heads,\n",
    "                 dropout,\n",
    "                 num_blocks,\n",
    "                 max_length_seq,\n",
    "                 language_to_index,\n",
    "                 start_token, \n",
    "                 end_token, \n",
    "                 pad_token\n",
    "                ):\n",
    "        \"\"\"\n",
    "        The Encoder part of the Transformer architecture\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Layer\n",
    "        self.input_preprocessing = Preprocessing(max_length_seq, d_model, language_to_index, start_token, end_token, pad_token, dropout)\n",
    "        \n",
    "        # Transformer Blocks\n",
    "        self.transformer_blocks = replicate(TransformerBlock(d_model, num_heads, ff_hidden, dropout, options=\"encoder\"),num_blocks)\n",
    "\n",
    "    def forward(self, x, self_attention_mask, start_token:bool, end_token:bool):\n",
    "        # Input Pre-processing: Token Embedding + Positional Encoding\n",
    "        out = self.input_preprocessing(x, start_token, end_token)\n",
    "\n",
    "        # Go to Transformer Blocks (Encode)\n",
    "        for block in self.transformer_blocks:\n",
    "            out = block(out, self_attention_mask)\n",
    "\n",
    "        return out\n",
    "\n",
    "# Or just using\n",
    "# nn.TransformerEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf4215e2-b0c5-486d-a4b1-1dc8e3555209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2348,  0.9120,  1.7391,  ..., -0.2278, -0.8417,  1.1081],\n",
       "         [ 1.6559,  0.3212, -0.8062,  ...,  1.7960, -0.7977,  1.6432],\n",
       "         [-0.0429, -0.7864,  0.7223,  ...,  0.8125, -0.2887, -0.0834],\n",
       "         ...,\n",
       "         [ 0.4337, -1.5136, -1.0085,  ..., -0.6353, -0.1083,  1.5586],\n",
       "         [-0.2551, -0.4255, -0.3870,  ...,  0.2286, -0.1370,  1.7222],\n",
       "         [-0.5061, -0.3137,  0.3685,  ..., -0.6921, -0.2590,  1.3873]],\n",
       "\n",
       "        [[-1.2848, -0.0301,  0.7372,  ...,  1.3673, -1.1085,  0.1970],\n",
       "         [ 0.6411, -1.6698,  1.1865,  ...,  0.1364,  0.3833, -0.1536],\n",
       "         [ 0.7109, -1.0754,  1.3723,  ..., -0.0966,  0.5129,  0.1411],\n",
       "         ...,\n",
       "         [ 0.2068, -1.4160, -1.1323,  ..., -0.1001, -0.1457,  1.7570],\n",
       "         [-0.2347, -0.4453, -0.4428,  ...,  0.0818, -0.0247,  1.4377],\n",
       "         [-0.5970, -0.5825,  0.2446,  ..., -0.3669, -0.2918,  1.8792]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "special_tokens = [\"<start>\", \"<end>\", \"<pad>\"]\n",
    "alphabet = list(\"abcdefghijklmnopqrstuvwxyz \") + special_tokens\n",
    "language_to_index = {word: idx for idx, word in enumerate(alphabet)}\n",
    "encoder = Encoder(d_model=100, ff_hidden=50, num_heads=2, dropout=0.1 ,num_blocks=1, max_length_seq=100, language_to_index=language_to_index, start_token=\"<start>\", end_token=\"<end>\", pad_token=\"<pad>\")\n",
    "encoder.to(get_device())\n",
    "batch = ['hello','goodbye']\n",
    "encoder(batch, None, False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58513193-9156-41c9-8861-6464a3ae3341",
   "metadata": {},
   "source": [
    "- **Decoder** includes **Output Pre-processing** (**Token Embedding** & **Positional Encoding**), N **Transformer Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64c335b7-26ce-447f-96d3-449efe7bf12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 ff_hidden,\n",
    "                 num_heads,\n",
    "                 dropout,\n",
    "                 num_blocks,\n",
    "                 max_length_seq,\n",
    "                 language_to_index,\n",
    "                 start_token, \n",
    "                 end_token, \n",
    "                 pad_token\n",
    "                ):\n",
    "        \"\"\"\n",
    "        The Decoder part of the Transformer architecture\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "         # Layer\n",
    "        self.output_preprocessing = Preprocessing(max_length_seq, d_model, language_to_index, start_token, end_token, pad_token, dropout)\n",
    "        \n",
    "        # Transformer Blocks\n",
    "        self.transformer_blocks = replicate(TransformerBlock(d_model, num_heads, ff_hidden, dropout, options=\"decoder\"),num_blocks)\n",
    "\n",
    "    def forward(self, x, y, self_attention_mask, cross_attention_mask, start_token:bool, end_token:bool): \n",
    "        # x is output, y is output from encoder\n",
    "        # Output Pre-processing: Token Embedding + Positional Encoding\n",
    "        x = self.output_preprocessing(x, start_token, end_token)\n",
    "\n",
    "        # Go to Transformer Blocks (Decode)\n",
    "        encode_decode = {'encoder_output': y,'w':x}\n",
    "        mask = {'self_attention_mask': self_attention_mask,'cross_attention_mask': cross_attention_mask}\n",
    "        for block in self.transformer_blocks:\n",
    "            encode_decode['w'] = x\n",
    "            x = block(encode_decode, mask)\n",
    "        return x\n",
    "\n",
    "# Or just using\n",
    "# nn.TransformerDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadb9ff7-e4f5-4a9c-b1c3-2c434216fe36",
   "metadata": {},
   "source": [
    "## Finally, The Transformer Architecture is complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "724efbec-53d8-4efd-9469-58df2f117555",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 ff_hidden,\n",
    "                 num_heads,\n",
    "                 dropout,\n",
    "                 num_blocks,\n",
    "                 max_length_seq,\n",
    "                 language_to_index,\n",
    "                 target_language_to_index,\n",
    "                 start_token, \n",
    "                 end_token, \n",
    "                 pad_token\n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Device\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            d_model=d_model,\n",
    "            ff_hidden=ff_hidden,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            num_blocks=num_blocks,\n",
    "            max_length_seq=max_length_seq,\n",
    "            language_to_index=language_to_index,\n",
    "            start_token=start_token,\n",
    "            end_token=end_token,\n",
    "            pad_token=pad_token\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(\n",
    "            d_model=d_model,\n",
    "            ff_hidden=ff_hidden,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            num_blocks=num_blocks,\n",
    "            max_length_seq=max_length_seq,\n",
    "            language_to_index=target_language_to_index,\n",
    "            start_token=start_token,\n",
    "            end_token=end_token,\n",
    "            pad_token=pad_token\n",
    "        )\n",
    "\n",
    "        # Linear Layer\n",
    "        self.linear = nn.Linear(d_model, len(target_language_to_index))\n",
    "\n",
    "        # Softmax\n",
    "        \n",
    "\n",
    "    def forward(self,\n",
    "                x,\n",
    "                y,\n",
    "                encoder_self_attention_mask=None,\n",
    "                decoder_self_attention_mask=None,\n",
    "                decoder_cross_attention_mask=None,\n",
    "                encoder_start_token=False,\n",
    "                encoder_end_token=False,\n",
    "                decoder_start_token=False,\n",
    "                decoder_end_token=False):\n",
    "        encoder_output = self.encoder(x, encoder_self_attention_mask, encoder_start_token, encoder_end_token)\n",
    "        out = self.decoder(y, encoder_output, decoder_self_attention_mask, decoder_cross_attention_mask, decoder_start_token, decoder_end_token)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "# or just using\n",
    "# nn.Transformer, TokenEmbedding, PositionalEncoding to make the complete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afa02306-57b4-481e-84de-0fe43bb2de9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (input_preprocessing): Preprocessing(\n",
       "      (token_embedding): TokenEmbedding(\n",
       "        (embedding_layer): Embedding(30, 512)\n",
       "      )\n",
       "      (positional_encoding): PositionalEncoding(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm_for_attention): LayerNormalization()\n",
       "        (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "        (ff): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=3000, bias=True)\n",
       "          (linear2): Linear(in_features=3000, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm_for_ff): LayerNormalization()\n",
       "        (dropout_for_ff): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (output_preprocessing): Preprocessing(\n",
       "      (token_embedding): TokenEmbedding(\n",
       "        (embedding_layer): Embedding(30, 512)\n",
       "      )\n",
       "      (positional_encoding): PositionalEncoding(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm_for_attention): LayerNormalization()\n",
       "        (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "        (cross_attention): MultiHeadAttention(\n",
       "          (kv_layer): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (q_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm_for_cross_attention): LayerNormalization()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ff): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=3000, bias=True)\n",
       "          (linear2): Linear(in_features=3000, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm_for_ff): LayerNormalization()\n",
       "        (dropout_for_ff): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=30, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just an example to work with (Don't care about right or wrong)\n",
    "special_tokens = [\"<start>\", \"<end>\", \"<pad>\"]\n",
    "alphabet = list(\"abcdefghijklmnopqrstuvwxyz \") + special_tokens\n",
    "language_to_index = {word: idx for idx, word in enumerate(alphabet)}\n",
    "\n",
    "model = Transformer(512, 3000, 8, 0.1, 1, 100, language_to_index, language_to_index, \"<start>\", \"<end>\", \"<pad>\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d0154f2-2def-44ae-8aef-b36759b77127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0911, -0.0599,  0.3554,  ..., -0.5377,  0.7098, -0.2050],\n",
       "         [ 0.9216, -0.1291,  0.5588,  ..., -0.1937, -0.6276,  0.8985],\n",
       "         [ 0.3588,  0.4092,  0.7912,  ..., -0.2776, -0.5603, -1.1455],\n",
       "         ...,\n",
       "         [-0.6896,  0.1985,  0.6271,  ...,  0.1436, -0.1053,  0.8588],\n",
       "         [-1.1051,  0.2928,  0.5118,  ...,  0.7733, -0.0668,  0.5815],\n",
       "         [-1.2186,  0.2824, -0.0428,  ...,  0.5773, -0.5747,  0.1023]],\n",
       "\n",
       "        [[ 0.0676, -0.0335, -0.1566,  ..., -0.1537,  0.2937, -0.1206],\n",
       "         [ 0.0718,  0.0806, -0.3775,  ..., -0.1113, -0.2573, -0.7228],\n",
       "         [-0.2775, -0.4968,  0.6976,  ...,  0.1742, -0.0686, -0.7298],\n",
       "         ...,\n",
       "         [-0.7425,  0.2690,  0.3774,  ...,  0.1982,  0.1753,  0.3259],\n",
       "         [-0.5395, -0.0758,  0.1406,  ...,  0.4591, -0.1938,  0.4627],\n",
       "         [-0.8725,  0.4493,  0.5609,  ...,  0.4942, -0.2736,  0.2028]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(get_device())\n",
    "model(['hello','goodbye'],['xin chao','tam biet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab8045-7a8b-4c2b-97c5-2fc5748556ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TorchCuda",
   "language": "python",
   "name": "torchcuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
