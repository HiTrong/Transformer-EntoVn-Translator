{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"# We should use GPU to train this model! Thanks\n# I recommend Kaggle or Google Colab and of course both are free but have limitations\n# MyTransformer.py\n# import library\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport copy\n\n# ======================================== Module ========================================\ndef get_device():\n    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\ndef Masking(encoder_batch, decoder_batch, max_length_seq):\n    NEG_INFTY = -1e9\n    num_sentences = len(encoder_batch)\n    look_ahead_mask = torch.full([max_length_seq, max_length_seq] , True)\n    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n    encoder_padding_mask = torch.full([num_sentences, max_length_seq, max_length_seq] , False)\n    decoder_padding_mask_self_attention = torch.full([num_sentences, max_length_seq, max_length_seq] , False)\n    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_length_seq, max_length_seq] , False)\n\n    for idx in range(num_sentences):\n      encoder_sentence_length, decoder_sentence_length = len(encoder_batch[idx]), len(decoder_batch[idx])\n      encoder_chars_to_padding_mask = np.arange(encoder_sentence_length + 1, max_length_seq)\n      decoder_chars_to_padding_mask = np.arange(decoder_sentence_length + 1, max_length_seq)\n      encoder_padding_mask[idx, :, encoder_chars_to_padding_mask] = True\n      encoder_padding_mask[idx, encoder_chars_to_padding_mask, :] = True\n      decoder_padding_mask_self_attention[idx, :, decoder_chars_to_padding_mask] = True\n      decoder_padding_mask_self_attention[idx, decoder_chars_to_padding_mask, :] = True\n      decoder_padding_mask_cross_attention[idx, :, encoder_chars_to_padding_mask] = True\n      decoder_padding_mask_cross_attention[idx, decoder_chars_to_padding_mask, :] = True\n      \n\n    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0).to(get_device())\n    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0).to(get_device())\n    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0).to(get_device())\n    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask\n     \n\nclass TokenEmbedding(nn.Module):\n    \n    def __init__(self, vocab_size, d_model):\n        \"\"\"\n        Token Embedding is used for converting a word / token into a embedding numeric vector space.\n        \n        :param vocab_size: Number of words / token in vocabulary\n        :param d_model: The embedding dimension\n        \n        Example: With 1000 words in vocabulary and our embedding dimension is 512, the Token Embedding layer will be 1000x512\n        \"\"\"\n        super().__init__()\n        self.embedding_layer = nn.Embedding(vocab_size, d_model)\n\n    def forward(self, x):\n        \"\"\"\n        :param x: the word or sequence of words\n        :return: the numerical representation of the input\n        \n        Example:\n        Input: (Batch_size, Sequence of words) - (30x100)\n        Output: (Batch_size, Sequence of words, d_model) - (30x100x512)\n        \"\"\"\n        x = self.embedding_layer(x)\n        return x.to(get_device())\n\n# Or just Simple\n# token_embedding = nn.Embedding(vocab_size, d_model)\n\nclass PositionalEncoding(nn.Module):\n    \n    def __init__(self, d_model, max_sequence_length, dropout=0.1):\n        \"\"\"\n        Positional Encoding layer for adding positional information to token embeddings.\n        \n        :param d_model: The embedding dimension.\n        :param max_sequence_length: The maximum length of the input sequences.\n        :param dropout: Dropout rate.\n        \"\"\"\n        super(PositionalEncoding,self).__init__()\n        self.max_sequence_length = max_sequence_length\n        self.d_model = d_model\n        self.dropout = nn.Dropout(dropout)\n        PE = self.get()\n        self.register_buffer('PE', PE)\n        \n    def get(self):\n        even_i = torch.arange(0, self.d_model, 2).float()\n        denominator = torch.pow(10000, even_i/self.d_model)\n        position = (torch.arange(self.max_sequence_length)\n                          .reshape(self.max_sequence_length, 1))\n        even_PE = torch.sin(position / denominator)\n        odd_PE = torch.cos(position / denominator)\n        stacked = torch.stack([even_PE, odd_PE], dim=2)\n        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n        PE = PE.unsqueeze(0)\n        return PE\n\n\n    def forward(self):\n        return self.dropout(self.PE)\n    \nclass MultiHeadAttention(nn.Module):\n\n    def __init__(self, d_model, num_heads=8, cross=False):\n        \"\"\"\n        Multi-Head Attention\n        :param d_model: the embedding dimension\n        :param num_heads: the number of heads, default equals 8\n        :param cross: True for Multi-Head Cross Attention, False for Multi-Head Attention only\n        \n        # note: The embedding dimension must be divided by the number of heads\n        \"\"\"\n        super(MultiHeadAttention,self).__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n        self.cross = cross\n\n        # query, key value layer\n        if self.cross: # Multi-Head Cross Attention\n            self.kv_layer = nn.Linear(d_model , 2 * d_model)\n            self.q_layer = nn.Linear(d_model , d_model)\n        else:\n            self.qkv_layer = nn.Linear(d_model , 3 * d_model)\n        \n        \n        # method 1: old, cost alot\n        # self.query = nn.Linear(self.head_dim, self.head_dim, bias=False)\n        # self.key = nn.Linear(self.head_dim, self.head_dim, bias=False)\n        # self.value = nn.Linear(self.head_dim, self.head_dim, bias=False) \n\n        # method 2: the fewer linear layers the better the cost\n        \n        \n        # Linear Layer in Multi-Head Attention\n        self.linear_layer = nn.Linear(d_model, d_model)\n\n    def scaled_dot_product(self, q, k, v, mask=None):\n        d_k = q.size()[-1]\n        scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n        if mask is not None:\n            scaled = scaled.permute(1, 0, 2, 3) + mask\n            scaled = scaled.permute(1, 0, 2, 3)\n        attention = F.softmax(scaled, dim=-1)\n        values = torch.matmul(attention, v)\n        return values, attention\n    \n    def forward(self, x, mask=None):\n        \"\"\"\n        Perform forward pass of the multi-head attention mechanism.\n\n        :param x: if cross is True then x is a dictionary including  'encoder_output' and 'w'.\n        :param mask: Optional mask tensor\n        \n        :return: Output tensor of shape (batch_size, length_seq, d_model)\n\n        \"\"\"\n\n        # For MultiHead Cross Attention\n        if self.cross:\n            encoder_output = x['encoder_output']\n            w = x['w']\n            batch_size, length_seq, d_model = w.size()\n            kv = self.kv_layer(w)\n            q = self.q_layer(encoder_output)\n            kv = kv.reshape(batch_size, length_seq, self.num_heads, 2 * self.head_dim)\n            q = q.reshape(batch_size, length_seq, self.num_heads, self.head_dim)\n            kv = kv.permute(0, 2, 1, 3)\n            q = q.permute(0, 2, 1, 3)\n            k, v = kv.chunk(2, dim=-1)\n            values, attention = self.scaled_dot_product(q, k, v, mask) # mask is not required in Cross Attention\n            values = values.permute(0, 2, 1, 3).reshape(batch_size, length_seq, self.num_heads * self.head_dim)\n            out = self.linear_layer(values)\n            return out\n\n        # For MultiHead Attention\n        batch_size, length_seq, d_model = x.size()\n        qkv = self.qkv_layer(x)\n        qkv = qkv.reshape(batch_size, length_seq, self.num_heads, 3 * self.head_dim)\n        qkv = qkv.permute(0, 2, 1, 3)\n        q, k, v = qkv.chunk(3, dim=-1)\n        values, attention = self.scaled_dot_product(q, k, v, mask)\n        values = values.permute(0, 2, 1, 3).reshape(batch_size, length_seq, self.num_heads * self.head_dim)\n        out = self.linear_layer(values)\n        return out\n    \nclass LayerNormalization(nn.Module):\n    def __init__(self, parameters_shape, eps=1e-5):\n        super().__init__()\n        self.parameters_shape=parameters_shape\n        self.eps=eps\n        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n\n    def forward(self, inputs):\n        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n        mean = inputs.mean(dim=dims, keepdim=True)\n        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n        std = (var + self.eps).sqrt()\n        y = (inputs - mean) / std\n        out = self.gamma * y  + self.beta\n        return out\n\n# Or using nn.LayerNorm(d_model)\n\nclass PositionwiseFeedForward(nn.Module):\n\n    def __init__(self, d_model, hidden, drop_prob=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.linear1 = nn.Linear(d_model, hidden)\n        self.linear2 = nn.Linear(hidden, d_model)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=drop_prob)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.linear2(x)\n        return x\n\n# feed_forward = nn.Sequential(\n#     nn.Linear(d_model, expansion_factor * d_model),  # e.g: 512x(4*512) -> (512, 2048)\n#     nn.ReLU(),  # ReLU activation function\n#     nn.Linear(d_model * expansion_factor, d_model),  # e.g: 4*512)x512 -> (2048, 512)\n# )\n\ndef replicate(block, N=6) -> nn.ModuleList:\n    \"\"\"\n    Method to replicate the existing block to N set of blocks\n    :param block: class inherited from nn.Module, mainly it is the encoder or decoder part of the architecture\n    :param N: the number of stack, in the original paper they used 6\n    :return: a set of N blocks\n    \"\"\"\n    block_stack = nn.ModuleList([copy.deepcopy(block) for _ in range(N)])\n    return block_stack\n\nclass Preprocessing(nn.Module):\n\n    def __init__(self, max_length_seq, d_model, language_to_index, start_token, end_token, pad_token, dropout=0.1):\n        super().__init__()\n        self.vocab_size = len(language_to_index)\n        self.language_to_index = language_to_index\n        self.max_length_seq = max_length_seq\n        self.start_token = start_token\n        self.end_token = end_token\n        self.pad_token = pad_token\n\n        # Layer\n        self.token_embedding = TokenEmbedding(self.vocab_size, d_model)\n        self.positional_encoding = PositionalEncoding(d_model, max_length_seq, dropout)\n        self.dropout = nn.Dropout(dropout)\n\n    \n    \n    def batch_tokens(self, batch, start_token:bool, end_token:bool):\n        def tokenize(sentence, start_token:bool, end_token:bool):\n            encode_char = [self.language_to_index[token] for token in list(sentence)]\n            if start_token:\n                encode_char.insert(0, self.language_to_index[self.start_token])\n            if end_token:\n                encode_char.append(self.language_to_index[self.end_token])\n            for _ in range(len(encode_char), self.max_length_seq):\n                encode_char.append(self.language_to_index[self.pad_token])\n            return torch.tensor(encode_char)\n        \n        tokens = []\n        for i in range(len(batch)):\n            tokens.append(tokenize(batch[i], start_token, end_token))\n        tokens = torch.stack(tokens)\n        return tokens\n\n    def forward(self, x, start_token:bool, end_token:bool): \n        x = self.batch_tokens(x, start_token, end_token)\n        x = self.token_embedding(x.to(get_device()))\n        pos = self.positional_encoding().to(get_device())\n        x = self.dropout(x + pos)\n        return x\n    \nclass TransformerBlock(nn.Module):\n\n    def __init__(self,\n                 d_model=512,\n                 num_heads=8,\n                 ff_hidden=300,\n                 dropout=0.1,\n                 options='encoder'\n                ):\n        \"\"\"\n        The Transformer Block used in the encoder and decoder as well\n\n        :param d_model: the embedding dimension\n        :param num_heads: the number of heads\n        :param ff_hidden: The output dimension of the feed forward layer\n        :param dropout: probability dropout (between 0 and 1)\n        :param options: The choice between 'encoder' and 'decoder'\n        \"\"\"\n        super(TransformerBlock, self).__init__()\n    \n        self.options = options\n        \n        # For both 2 options: encoder and decoder\n        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n        self.norm_for_attention = LayerNormalization(parameters_shape=[d_model])\n        self.dropout_attention = nn.Dropout(dropout)\n\n        \n        \n        # For decoder\n        if self.options=='decoder':\n            self.cross_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads, cross=True)\n            self.norm_for_cross_attention = LayerNormalization(parameters_shape=[d_model])\n            self.dropout2 = nn.Dropout(dropout)\n        elif self.options!='encoder':\n            raise Exception(f\"Unknown option {options}\")\n\n        # For both 2 options: encoder and decoder\n        self.ff = PositionwiseFeedForward(d_model=d_model, hidden=ff_hidden, drop_prob=dropout)\n        self.norm_for_ff = LayerNormalization(parameters_shape=[d_model])\n        self.dropout_for_ff = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        # For decoder\n        if self.options == 'decoder':\n            encoder_output = x['encoder_output']\n            w = x['w']\n            w_residual = w.clone()\n            w = self.attention(w,mask['self_attention_mask'])\n            w = self.dropout_attention(w)\n            w = self.norm_for_attention(w + w_residual)\n\n            w_residual = w.clone()\n            w = self.cross_attention({'encoder_output':encoder_output,'w':w},mask['cross_attention_mask'])\n            w = self.dropout2(w)\n            w = self.norm_for_cross_attention(w + w_residual)\n\n            w_residual = w.clone()\n            w = self.ff(w)\n            w = self.dropout_for_ff(w)\n            w = self.norm_for_ff(w + w_residual)\n            return w\n        else:\n        # For encoder\n            x_residual = x.clone()\n            x = self.attention(x, mask)\n            x = self.dropout_attention(x)\n            x = self.norm_for_attention(x + x_residual)\n\n            x_residual = x.clone()\n            x = self.ff(x)\n            x = self.dropout_for_ff(x)\n            x = self.norm_for_ff(x + x_residual)\n            return x\n        \nclass Encoder(nn.Module):\n\n    def __init__(self,\n                 d_model,\n                 ff_hidden,\n                 num_heads,\n                 dropout,\n                 num_blocks,\n                 max_length_seq,\n                 language_to_index,\n                 start_token, \n                 end_token, \n                 pad_token\n                ):\n        \"\"\"\n        The Encoder part of the Transformer architecture\n        \"\"\"\n        super().__init__()\n\n        # Layer\n        self.input_preprocessing = Preprocessing(max_length_seq, d_model, language_to_index, start_token, end_token, pad_token, dropout)\n        \n        # Transformer Blocks\n        self.transformer_blocks = replicate(TransformerBlock(d_model, num_heads, ff_hidden, dropout, options=\"encoder\"),num_blocks)\n\n    def forward(self, x, self_attention_mask, start_token:bool, end_token:bool):\n        # Input Pre-processing: Token Embedding + Positional Encoding\n        out = self.input_preprocessing(x, start_token, end_token)\n\n        # Go to Transformer Blocks (Encode)\n        for block in self.transformer_blocks:\n            out = block(out, self_attention_mask)\n\n        return out\n    \nclass Decoder(nn.Module):\n\n    def __init__(self,\n                 d_model,\n                 ff_hidden,\n                 num_heads,\n                 dropout,\n                 num_blocks,\n                 max_length_seq,\n                 language_to_index,\n                 start_token, \n                 end_token, \n                 pad_token\n                ):\n        \"\"\"\n        The Decoder part of the Transformer architecture\n\n        \"\"\"\n        super().__init__()\n        \n         # Layer\n        self.output_preprocessing = Preprocessing(max_length_seq, d_model, language_to_index, start_token, end_token, pad_token, dropout)\n        \n        # Transformer Blocks\n        self.transformer_blocks = replicate(TransformerBlock(d_model, num_heads, ff_hidden, dropout, options=\"decoder\"),num_blocks)\n\n    def forward(self, x, y, self_attention_mask, cross_attention_mask, start_token:bool, end_token:bool): \n        # x is output, y is output from encoder\n        # Output Pre-processing: Token Embedding + Positional Encoding\n        x = self.output_preprocessing(x, start_token, end_token)\n\n        # Go to Transformer Blocks (Decode)\n        encode_decode = {'encoder_output': y,'w':x}\n        mask = {'self_attention_mask': self_attention_mask,'cross_attention_mask': cross_attention_mask}\n        for block in self.transformer_blocks:\n            encode_decode['w'] = x\n            x = block(encode_decode, mask)\n        return x\n    \nclass Transformer(nn.Module):\n\n    def __init__(self,\n                 d_model,\n                 ff_hidden,\n                 num_heads,\n                 dropout,\n                 num_blocks,\n                 max_length_seq,\n                 language_to_index,\n                 target_language_to_index,\n                 start_token, \n                 end_token, \n                 pad_token\n                ):\n        super().__init__()\n\n        # Device\n        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n        \n        # Encoder\n        self.encoder = Encoder(\n            d_model=d_model,\n            ff_hidden=ff_hidden,\n            num_heads=num_heads,\n            dropout=dropout,\n            num_blocks=num_blocks,\n            max_length_seq=max_length_seq,\n            language_to_index=language_to_index,\n            start_token=start_token,\n            end_token=end_token,\n            pad_token=pad_token\n        )\n\n        # Decoder\n        self.decoder = Decoder(\n            d_model=d_model,\n            ff_hidden=ff_hidden,\n            num_heads=num_heads,\n            dropout=dropout,\n            num_blocks=num_blocks,\n            max_length_seq=max_length_seq,\n            language_to_index=target_language_to_index,\n            start_token=start_token,\n            end_token=end_token,\n            pad_token=pad_token\n        )\n\n        # Linear Layer\n        self.linear = nn.Linear(d_model, len(target_language_to_index))\n\n        # Softmax\n        \n\n    def forward(self,\n                x,\n                y,\n                encoder_self_attention_mask=None,\n                decoder_self_attention_mask=None,\n                decoder_cross_attention_mask=None,\n                encoder_start_token=False,\n                encoder_end_token=False,\n                decoder_start_token=False,\n                decoder_end_token=False):\n        encoder_output = self.encoder(x, encoder_self_attention_mask, encoder_start_token, encoder_end_token)\n        out = self.decoder(y, encoder_output, decoder_self_attention_mask, decoder_cross_attention_mask, decoder_start_token, decoder_end_token)\n        out = self.linear(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:42:22.971964Z","iopub.execute_input":"2024-06-12T19:42:22.972311Z","iopub.status.idle":"2024-06-12T19:42:26.282996Z","shell.execute_reply.started":"2024-06-12T19:42:22.972279Z","shell.execute_reply":"2024-06-12T19:42:26.281986Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Transformer Model - Translator English to Vietnamese - Training","metadata":{}},{"cell_type":"markdown","source":"- Import library","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport warnings\nimport pandas as pd\nimport time\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn\nfrom datasets import load_dataset\n# from MyTransformer import Transformer, Masking\n\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:42:28.675819Z","iopub.execute_input":"2024-06-12T19:42:28.676310Z","iopub.status.idle":"2024-06-12T19:42:30.313919Z","shell.execute_reply.started":"2024-06-12T19:42:28.676280Z","shell.execute_reply":"2024-06-12T19:42:30.312960Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"- Load Dataset from Hugging Face","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"kaitchup/opus-Vietnamese-to-English\")\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:42:33.999837Z","iopub.execute_input":"2024-06-12T19:42:34.000572Z","iopub.status.idle":"2024-06-12T19:42:38.407796Z","shell.execute_reply.started":"2024-06-12T19:42:34.000542Z","shell.execute_reply":"2024-06-12T19:42:38.406923Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/559 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f32ff67d44d47fda964150614e96652"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/138k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bef597ceb333490a9d4afed04d210494"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/56.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4edbce915822475b92012bdd83dbbdad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e5375e44eb948f49c6fbe7ef2dfcfe0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/992248 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc297c4b294d420e8031bc2fd18e2965"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 2000\n    })\n    train: Dataset({\n        features: ['text'],\n        num_rows: 992248\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset['train']['text'][0:5]","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:42:39.421011Z","iopub.execute_input":"2024-06-12T19:42:39.421346Z","iopub.status.idle":"2024-06-12T19:42:41.359700Z","shell.execute_reply.started":"2024-06-12T19:42:39.421321Z","shell.execute_reply":"2024-06-12T19:42:41.358730Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"['Cái gì đó? ###>What is it?',\n \"Con nghĩ chúng ta nên đến mái ấm. ###>I thought we would go to the children's home.\",\n 'Có điều gì cô muốn nói với chồng mình không? ###>Is there something you want to tell your husband?',\n 'Thầy của ngươi muốn săn chúng ta, thiêu chúng ta, ăn tim chúng ta. ###>Your master wants to hunt us, burn us, eat our hearts.',\n 'Haylàkẻ yếuđuối? ###>Or too weak to see this through?']"},"metadata":{}}]},{"cell_type":"code","source":"sentences_train = list(map(lambda x: x.split('###>'), dataset['train']['text']))\nvietnam_sentences_train = list(map(lambda x : x[0], sentences_train))\nenglish_sentences_train = list(map(lambda x : x[1], sentences_train))\nlen(vietnam_sentences_train), len(english_sentences_train)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:42:42.604469Z","iopub.execute_input":"2024-06-12T19:42:42.604820Z","iopub.status.idle":"2024-06-12T19:42:46.823618Z","shell.execute_reply.started":"2024-06-12T19:42:42.604791Z","shell.execute_reply":"2024-06-12T19:42:46.822512Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(992248, 992248)"},"metadata":{}}]},{"cell_type":"code","source":"sentences_valid = list(map(lambda x: x.split('###>'), dataset['validation']['text']))\nvietnam_sentences_valid = list(map(lambda x : x[0], sentences_valid))\nenglish_sentences_valid = list(map(lambda x : x[1], sentences_valid))\nlen(vietnam_sentences_valid), len(english_sentences_valid)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:42:46.825654Z","iopub.execute_input":"2024-06-12T19:42:46.826064Z","iopub.status.idle":"2024-06-12T19:42:46.840625Z","shell.execute_reply.started":"2024-06-12T19:42:46.826031Z","shell.execute_reply":"2024-06-12T19:42:46.839644Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(2000, 2000)"},"metadata":{}}]},{"cell_type":"code","source":"vietnam_sentences_valid[0:4]","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:42:46.841756Z","iopub.execute_input":"2024-06-12T19:42:46.842076Z","iopub.status.idle":"2024-06-12T19:42:46.853483Z","shell.execute_reply.started":"2024-06-12T19:42:46.842047Z","shell.execute_reply":"2024-06-12T19:42:46.852633Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"['Anh cũng làm việc cho hắn ta? ',\n 'Xin lỡi, hôm nay tôi thấy khó chịu Tối qua tôi đã gặp ác mộng ',\n 'Em không cho mụ vinh hạnh đó đâu. ',\n '- Bỏ nó vào túi. ']"},"metadata":{}}]},{"cell_type":"code","source":"english_sentences_valid[0:4]","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:42:47.333339Z","iopub.execute_input":"2024-06-12T19:42:47.334127Z","iopub.status.idle":"2024-06-12T19:42:47.339833Z","shell.execute_reply.started":"2024-06-12T19:42:47.334091Z","shell.execute_reply":"2024-06-12T19:42:47.338946Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"['You can act as him, too?',\n \"I'm sorry. I am nervous today. I had bad dreams.\",\n \"I wouldn't give her that pleasure. It's up to you.\",\n '- Leave that in this bag.']"},"metadata":{}}]},{"cell_type":"markdown","source":"- Setup vocabulary","metadata":{}},{"cell_type":"code","source":"START_TOKEN = '<start>'\nPADDING_TOKEN = '<pad>'\nEND_TOKEN = '<end>'","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:42:48.335724Z","iopub.execute_input":"2024-06-12T19:42:48.336595Z","iopub.status.idle":"2024-06-12T19:42:48.340924Z","shell.execute_reply.started":"2024-06-12T19:42:48.336550Z","shell.execute_reply":"2024-06-12T19:42:48.340068Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"vietnamese_characters = [ ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', \n    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', '?', 'ˌ',\n    'a', 'á', 'à', 'ả', 'ã', 'ạ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ',\n    'b', 'c', 'd', 'đ', 'e', 'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ', \n    'g', 'h', 'i', 'í', 'ì', 'ỉ', 'ĩ', 'ị', 'k', 'l', 'm', 'n', 'o', 'ó', 'ò', 'ỏ', 'õ', 'ọ', \n    'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'p', 'q', 'r', 's', 't', 'u', \n    'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'v', 'x', 'y', 'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ','z','w','f','j'\n]\n\nvietnamese_vocabulary = list(set([START_TOKEN] + vietnamese_characters + [char.upper() for char in vietnamese_characters] + [PADDING_TOKEN, END_TOKEN]))\nlen(vietnamese_vocabulary)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:42:48.753462Z","iopub.execute_input":"2024-06-12T19:42:48.754314Z","iopub.status.idle":"2024-06-12T19:42:48.766223Z","shell.execute_reply.started":"2024-06-12T19:42:48.754282Z","shell.execute_reply":"2024-06-12T19:42:48.765274Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"221"},"metadata":{}}]},{"cell_type":"code","source":"english_vocabulary = [ START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', \n    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', '?', 'ˌ',\n    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', \n    'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', \n    'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n                      PADDING_TOKEN, END_TOKEN\n]\nlen(english_vocabulary)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:42:49.228597Z","iopub.execute_input":"2024-06-12T19:42:49.229205Z","iopub.status.idle":"2024-06-12T19:42:49.238329Z","shell.execute_reply.started":"2024-06-12T19:42:49.229174Z","shell.execute_reply":"2024-06-12T19:42:49.237419Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"87"},"metadata":{}}]},{"cell_type":"markdown","source":"- Check vocabulary","metadata":{}},{"cell_type":"code","source":"def Check_character(sentences,vocabulary):\n    missing_character = []\n    amount_sentences = 0\n    for sentence in sentences:\n        check = False\n        for c in list(set(sentence)):\n            if c not in vocabulary and c not in missing_character:\n                missing_character.append(c)\n                check = True\n        if check:\n            amount_sentences += 1\n    if len(missing_character) == 0:\n        print(\"Suitable vocabulary!\")\n        return None\n    print(f\"Find {missing_character} in vocabulary!\")\n    return amount_sentences","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:42:50.886719Z","iopub.execute_input":"2024-06-12T19:42:50.887348Z","iopub.status.idle":"2024-06-12T19:42:50.893548Z","shell.execute_reply.started":"2024-06-12T19:42:50.887317Z","shell.execute_reply":"2024-06-12T19:42:50.892587Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"vietnam_wrong_sentences = Check_character(vietnam_sentences_train,vietnamese_vocabulary)\nenglish_wrong_sentences = Check_character(english_sentences_train,english_vocabulary)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:42:51.710789Z","iopub.execute_input":"2024-06-12T19:42:51.711458Z","iopub.status.idle":"2024-06-12T19:43:55.559623Z","shell.execute_reply.started":"2024-06-12T19:42:51.711420Z","shell.execute_reply":"2024-06-12T19:43:55.558659Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Find ['♫', '̀', '́', '̉', '♪', '̣', '^', '̃', '{', '\\\\', '}', '»', '«', '̀', '́', '£', '–', 'ð', ';', '@', '[', ']', 'Μ', '\\xad', '¡', '°', '×', '§', '\\x81', 'Æ', '\\x99', '\\x8b', '½', '´', '\\xa0', 'ª', 'º', '³', 'Ð', '_', 'ß', 'Û', '王', '校', '長', '¶', 'Ü', '¢', '甩', '他', '的', '隊', '防', '開', '了', '員', '守', '歌', '影', '明', '以', '拍', '做', '可', '手', '星', '還', '電', 'ō', '是', '傑', '嘛', '阿', '對', '就', '們', '事', '不', '咱', 'ü', 'Ñ', '¹', 'γ', '’', 'Ë', 'ï', '≤', 'Ä', '\\x91', 'ñ', '¯', 'ο', 'ë', 'ä', 'λ', 'ç', '「', '」', '©', 'Ç', '~', 'Þ', 'Η', '®', '合', '嗎', '跟', '照', '我', '¿', '那', '叫', '武', '在', '裡', '學', '功', '夫', '—', '，', '振', '李', '非', '格', '赵', '铎', 'ö', '吧', '噯', '沒', 'Ε', 'Τ', 'Α', '\\x9f', 'ħ', 'ā', 'ī', '走', '·', '江', '湖', '永', '啊', '出', '退', 'і', 'ѕ', 'х', 'Ѕ', '有', '沖', '你', '快', '\\u202d', '當', '生', '意', '敗', '年', '失', '蕭', '愛', '加', '嵐', '油', '¥', '系', '民', '派', '陈', '谈', '贤', '军', '阀', '国', '庆', 'å', 'µ', '拳', '來', '下', '神', '一', '都', '很', '天', '第', '原', '久', '前', '師', '父', '\\x90', '\\x87', '\\u200b', '好', '這', '字', '喔', '熟', '≥', 'Ş', '鍵', '時', '定', '刻', '決', '關', '름', '예', '뭐', '이', '요', '±', '\\x83', '打', '擋', '鐵', '得', '起', '誰', '撐', '回', '頭', '方', '紙', '望', '傘', '水', '把', '樣', '見', '紅', '子', '多', 'ﬂ', '¬', '\\x8d', 'Ï', '件', '後', '禮', '最', '物', '算', '幹', '\\u200c', '大', '團', '而', '結', '家', 'о', 'е', '聽', '說', 'Ö', '》', '！', '哇', '²', 'ֶ', '麼', '辭', '容', '思', '什', '義', '本', '丹', '喬', '強', '森', '比', '柯', '火', '夥', '讓', '兒', '熊', '戰', '抖', '燒', '燃', '顫', 'ń', 'ı', 'ğ', '給', '東', '交', '西', '要', '理', '心', '哥', '礙', '個', '障', '過', '衝', '筆', '？', 'с', 'С', '帶', '看', 'Ō', '亮', '挺', '漂', 'š', 'ć', 'Х', 'Ш', '淌', '渾', '怎', '賤', '拋', '棄', '為', '吃', '淋', '去', '淇', '候', '冰', '才', '坐', '\\x8f', '能', '謝', '薑', '幕', '散', '月', '憶', '光', '段', '截', '老', '飛', '舊', 'Å', '鬥', '人', '\\x89', '賽', '離', '球', '別', 'ø', '標', '切', '磋', '分', '再', '重', '將', '組', '又', 'æ', '面', '畫', '涼', '荒', '變', '¨', '像', '勁', '機', '會', '何', '任', '陪', '場', '同', '葬', '到', '簽', '\\x94', '媽', '婆', '†', 'œ', '`', '友', '小', '動', '朋', '作', '阻', '三', '推', '四', '話', '→', 'ʾ', 'ḥ', '÷', '利', '成', '勝', '冠', '\\x96', '⊲', '實', '其', '女', '也', 'û', '點', '輕', '歹', '識', '相', '全', '安', 'ę', '始', '差', '技', '等', '錢', '賺', '拿', '想', '名', '張', '杯', '乾', '仇', '恩', '情', '怨', '啦', '娘', '找', '爹', '‘', '百', '忘', '搞', '萬', '五', '™', '♥', 'ė', '囉', '哈', '適', '較', 'ş', 'ţ', 'ν', '喂', '勾', '及', '德', '涉', '違', '法', '只', '道', '€', '₫', '參', '資', 'т', 'ы', 'н', 'р', 'й', 'В', 'Б', 'в', '¸', '‐', '跳', '㮥', '᧯', '班', '接', '☺', 'ℒ', 'ℱ', 'ℬ', 'Ω', '真', '掛', '住', '頂', '←', '嬲', '底', '哪', 'İ', '留', '甲', '片', '絕', '符', '地', '耶', '呢', '至', '於', '莉', '吹', '彈', '試', '膚', '破', '肌', '練', '傳', '中', '空', '世', '攔', '\\x9b', '慌', '上', '裝', '倦', '穿', '悶', '釋', '集', '文', '骨', '\\u200e', '碰', '鬼', '代', '幾', '言', '千', '瞞', '豪', '畢', '聰', '信', '另', '外', '己', '經', 'О', '親', '尋', '籃', 'Э', 'м', 'К', 'к', 'И', 'и', 'з', 'у', 'П', 'а', 'ц', 'Î', '木', '佐', '々', '克', '\\x93', '\\x95', '感', '覺', '視', '展', '微', '腳', '稍', '伸', '灌', '幫', '魄', '運', '氣', '間', '由', '構', '鉛', '竿', '丟', 'Ţ', '羅', '竹', '風', '敦', '倫', '送', '需', '貴', '服', '衣', 'ū', '吸', '媒', '體', '次', '引', '批', '靜', '記', 'î', '號', '涯', '句', '\\x9d', '免', '腐', '豆', 'Ζ', '改', '太', '行', '旅', '位', '問', '請', '毒', '報', '管', '“', '”', '提', '值', 'Ο', 'Ν', '億', '\\x97', '知', '士', '実', '選', '新', 'の', '半', '哄', '騙', '樹', '搖', '源', '福', '莫', '群', '歴', '史', '編', '部', '但', '投', '輸', '暗', '馬', '呼', '招', '亂', '漢', '語', '委', '典', '•', '¼', '份', '身', '響', '贏', '„', '無', '精', '采', '該', '遮', '霜', '屏', '極', '品', '尤', '楚', '清', 'ł', '滾', '疆', '邊', '秒', '剩', '钮', '祜', '谱', '禄', 'Υ', '死', '垂', '掙', '扎', 'δ', '然', 'ч', 'ь', '帥', 'ą'] in vocabulary!\nFind [']', '[', '♪', '£', '_', 'É', 'ü', 'ο', 'é', 'ā', '±', '¡', '¯', '–', ';', '@', '~', 'à', 'ñ', '—', '´', '`', '\\xa0', '\\x94', 'Β', 'ö', '\\\\', '}', '{', '\\u200e', 'ō', 'á', 'Μ', '’', '\\x9d', 'Ó', '¢', 'í', 'Α', '²', 'À', '¶', 'è', '¿', 'š', 'Ã', '、', '─', 'Ü', 'Τ', 'Υ', 'ó', 'ã', 'ä', 'ô', '♫', '振', '李', '非', '格', '赵', '铎', 'Η', 'ħ', 'ī', 'ﬂ', '\\ue0e1', '×', 'Æ', '¼', 'ð', 'æ', 'â', '§', 'Κ', '\\u202d', '系', '民', '派', '陈', '谈', '贤', '军', '阀', '国', '庆', '™', 'ë', 'Ε', '^', 'о', 'ν', 'ự', 'ư', 'ợ', 'ì', 'ù', 'º', 'τ', 'ç', 'Ş', 'с', 'е', 'і', 'ѕ', 'а', 'ï', '름', '예', '뭐', '이', '요', '\\x92', '\\x8b', 'ê', '》', '《', 'ﬁ', 'ú', '\\u200b', 'Ö', 'ế', 'ể', 'î', 'ª', 'Е', '\\u202f', '¤', '½', 'È', 'İ', 'ń', '¾', 'ı', 'ğ', 'û', 'Ç', '°', '΄', '·', 'Ν', 'Ō', 'ć', 'Х', 'Ш', 'Á', 'Â', '\\x83', '・', '駛', '”', '“', 'й', 'ầ', 'ø', 'ò', 'å', '÷', 'Ÿ', '€', 'Ä', 'Ë', 'Ò', '¨', 'µ', 'Ð', 'Î', '®', '†', 'ạ', 'ʾ', 'ḥ', '\\x96', '\\x81', 'ę', '‘', '¥', 'ė', '¬', '‚', 'ă', 'ş', 'ţ', '\\u2009', 'т', 'ы', 'н', 'р', 'В', 'х', 'Б', 'в', '‐', '©', 'ý', 'Г', 'œ', '♡', 'Í', '�', 'þ', '釋', '集', '文', '字', '骨', '甲', '¦', 'Ï', 'Э', 'м', 'К', 'к', '»', 'И', 'и', 'з', '«', 'у', 'П', 'ц', '木', '佐', '々', '克', 'ι', 'ь', '＃', 'Å', 'ū', 'Ţ', '羅', '竹', '風', '￡', 'ß', '\\u3000', '慹', 'Ā', 'Ι', '像', '最', '士', '実', '後', '選', '新', '組', '武', 'の', 'õ', '源', '福', 'ữ', '莫', '群', '歴', '史', '編', '部', '\\x93', '\\x90', '\\x9e', '¹', '员', '会', '委', '语', '典', '大', '汉', '„', 'ł', '家', '钮', '祜', '谱', '禄', 'ÿ', 'ą'] in vocabulary!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Lots of characters like symbols, words in other languages. So we will try to remove all sentences which have unknown characters. If the amount of removed sentences are not so many, we can apply this. If so many sentences are removed, we should appy another ways like adding tag 'unknown'.","metadata":{}},{"cell_type":"code","source":"print(f'Train sentences: {len(vietnam_sentences_train)} (vietnam), {len(english_sentences_train)} (english)')\nprint(f'wrong train sentences: {vietnam_wrong_sentences} (vietnam), {english_wrong_sentences} (english)')","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:43:55.561174Z","iopub.execute_input":"2024-06-12T19:43:55.561468Z","iopub.status.idle":"2024-06-12T19:43:55.566338Z","shell.execute_reply.started":"2024-06-12T19:43:55.561443Z","shell.execute_reply":"2024-06-12T19:43:55.565401Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Train sentences: 992248 (vietnam), 992248 (english)\nwrong train sentences: 277 (vietnam), 169 (english)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The number of removed sentences is much smaller than the total number of sentences so we can remove them.","metadata":{}},{"cell_type":"code","source":"def is_valid_sentence(sentence,vocabulary):\n    for c in list(set(sentence)):\n        if c not in vocabulary:\n            return False\n    return True","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:43:55.567671Z","iopub.execute_input":"2024-06-12T19:43:55.568114Z","iopub.status.idle":"2024-06-12T19:43:55.579092Z","shell.execute_reply.started":"2024-06-12T19:43:55.568089Z","shell.execute_reply":"2024-06-12T19:43:55.578236Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"vn_temp = []\neng_temp = []\nfor i in range(0,len(vietnam_sentences_train)):\n    if is_valid_sentence(vietnam_sentences_train[i], vietnamese_vocabulary) and is_valid_sentence(english_sentences_train[i], english_vocabulary):\n        vn_temp.append(vietnam_sentences_train[i])\n        eng_temp.append(english_sentences_train[i])\nvietnam_sentences_train = vn_temp\nenglish_sentences_train = eng_temp\n\nvn_temp = []\neng_temp = []\nfor i in range(0,len(vietnam_sentences_valid)):\n    if is_valid_sentence(vietnam_sentences_valid[i], vietnamese_vocabulary) and is_valid_sentence(english_sentences_valid[i], english_vocabulary):\n        vn_temp.append(vietnam_sentences_valid[i])\n        eng_temp.append(english_sentences_valid[i])\nvietnam_sentences_valid = vn_temp\nenglish_sentences_valid = eng_temp\n\n\n# vietnam_wrong_sentences = Check_character(vietnam_sentences_train,vietnamese_vocabulary)\n# english_wrong_sentences = Check_character(english_sentences_train,english_vocabulary)\n# vietnam_wrong_sentences = Check_character(vietnam_sentences_valid,vietnamese_vocabulary)\n# english_wrong_sentences = Check_character(english_sentences_valid,english_vocabulary)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:43:55.581076Z","iopub.execute_input":"2024-06-12T19:43:55.581338Z","iopub.status.idle":"2024-06-12T19:44:58.720374Z","shell.execute_reply.started":"2024-06-12T19:43:55.581315Z","shell.execute_reply":"2024-06-12T19:44:58.719584Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"index_to_vietnamese = {k:v for k,v in enumerate(vietnamese_vocabulary)}\nvietnamese_to_index = {v:k for k,v in enumerate(vietnamese_vocabulary)}\nindex_to_english = {k:v for k,v in enumerate(english_vocabulary)}\nenglish_to_index = {v:k for k,v in enumerate(english_vocabulary)}","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:44:58.721463Z","iopub.execute_input":"2024-06-12T19:44:58.721745Z","iopub.status.idle":"2024-06-12T19:44:58.727127Z","shell.execute_reply.started":"2024-06-12T19:44:58.721721Z","shell.execute_reply":"2024-06-12T19:44:58.726104Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"- Check Length","metadata":{}},{"cell_type":"code","source":"df_train = pd.DataFrame({\n    'vietnamese_train_length': [len(sentence) for sentence in vietnam_sentences_train],\n    'english_train_length': [len(sentence) for sentence in english_sentences_train],\n})\n\ndf_valid = pd.DataFrame({\n    'vietnamese_valid_length': [len(sentence) for sentence in vietnam_sentences_valid],\n    'english_valid_length': [len(sentence) for sentence in english_sentences_valid],\n})","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:44:58.728279Z","iopub.execute_input":"2024-06-12T19:44:58.728563Z","iopub.status.idle":"2024-06-12T19:44:59.768234Z","shell.execute_reply.started":"2024-06-12T19:44:58.728541Z","shell.execute_reply":"2024-06-12T19:44:59.767442Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"df_train.describe()","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:44:59.769329Z","iopub.execute_input":"2024-06-12T19:44:59.769620Z","iopub.status.idle":"2024-06-12T19:44:59.892498Z","shell.execute_reply.started":"2024-06-12T19:44:59.769595Z","shell.execute_reply":"2024-06-12T19:44:59.891636Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"       vietnamese_train_length  english_train_length\ncount            952120.000000         952120.000000\nmean                 32.334417             30.988058\nstd                  21.854748             22.082578\nmin                   2.000000              1.000000\n25%                  17.000000             15.000000\n50%                  27.000000             26.000000\n75%                  42.000000             40.000000\nmax                 274.000000            416.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>vietnamese_train_length</th>\n      <th>english_train_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>952120.000000</td>\n      <td>952120.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>32.334417</td>\n      <td>30.988058</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>21.854748</td>\n      <td>22.082578</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>2.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>17.000000</td>\n      <td>15.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>27.000000</td>\n      <td>26.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>42.000000</td>\n      <td>40.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>274.000000</td>\n      <td>416.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_valid.describe()","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:44:59.893998Z","iopub.execute_input":"2024-06-12T19:44:59.894382Z","iopub.status.idle":"2024-06-12T19:44:59.909298Z","shell.execute_reply.started":"2024-06-12T19:44:59.894349Z","shell.execute_reply":"2024-06-12T19:44:59.908371Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"       vietnamese_valid_length  english_valid_length\ncount              1890.000000            1890.00000\nmean                 39.059788              39.02381\nstd                  26.224710              26.75412\nmin                   3.000000               3.00000\n25%                  22.000000              22.00000\n50%                  33.000000              33.00000\n75%                  49.000000              49.00000\nmax                 190.000000             188.00000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>vietnamese_valid_length</th>\n      <th>english_valid_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1890.000000</td>\n      <td>1890.00000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>39.059788</td>\n      <td>39.02381</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>26.224710</td>\n      <td>26.75412</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>3.000000</td>\n      <td>3.00000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>22.000000</td>\n      <td>22.00000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>33.000000</td>\n      <td>33.00000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>49.000000</td>\n      <td>49.00000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>190.000000</td>\n      <td>188.00000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print( f\"{97}th percentile length English: {np.percentile(df_train['english_train_length'].tolist(), 97)}\" )\nprint( f\"{97}th percentile length Vietnam: {np.percentile(df_train['vietnamese_train_length'], 97)}\" )","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:44:59.910428Z","iopub.execute_input":"2024-06-12T19:44:59.910786Z","iopub.status.idle":"2024-06-12T19:45:00.030661Z","shell.execute_reply.started":"2024-06-12T19:44:59.910739Z","shell.execute_reply":"2024-06-12T19:45:00.029782Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"97th percentile length English: 86.0\n97th percentile length Vietnam: 87.0\n","output_type":"stream"}]},{"cell_type":"code","source":"MAX_LENGTH = 100","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:45:00.033342Z","iopub.execute_input":"2024-06-12T19:45:00.033627Z","iopub.status.idle":"2024-06-12T19:45:00.037522Z","shell.execute_reply.started":"2024-06-12T19:45:00.033604Z","shell.execute_reply":"2024-06-12T19:45:00.036575Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"vn_temp = []\neng_temp = []\nfor i in range(0,len(vietnam_sentences_train)):\n    if len(vietnam_sentences_train[i]) < MAX_LENGTH - 1 and len(english_sentences_train[i]) < MAX_LENGTH - 1:\n        vn_temp.append(vietnam_sentences_train[i])\n        eng_temp.append(english_sentences_train[i])\nvietnam_sentences_train = vn_temp\nenglish_sentences_train = eng_temp\n\nvn_temp = []\neng_temp = []\nfor i in range(0,len(vietnam_sentences_valid)):\n    if len(vietnam_sentences_valid[i]) < MAX_LENGTH - 1 and len(english_sentences_valid[i]) < MAX_LENGTH - 1:\n        vn_temp.append(vietnam_sentences_valid[i])\n        eng_temp.append(english_sentences_valid[i])\nvietnam_sentences_valid = vn_temp\nenglish_sentences_valid = eng_temp","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:45:00.038670Z","iopub.execute_input":"2024-06-12T19:45:00.039063Z","iopub.status.idle":"2024-06-12T19:45:00.819142Z","shell.execute_reply.started":"2024-06-12T19:45:00.039033Z","shell.execute_reply":"2024-06-12T19:45:00.818396Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Save data for another training (save time, for hugging face error)\nimport os\n\nfolder = 'data'\nif not os.path.exists(folder):\n    os.mkdir(folder)\n\nwith open(\"./data/vietnamese_train.txt\", \"w\",encoding='utf-8') as file:\n    for sentence in vietnam_sentences_train:\n        file.write(f\"{sentence}\\n\")\nwith open(\"./data/vietnamese_valid.txt\", \"w\",encoding='utf-8') as file:\n    for sentence in vietnam_sentences_valid:\n        file.write(f\"{sentence}\\n\")\nwith open(\"./data/english_train.txt\", \"w\",encoding='utf-8') as file:\n    for sentence in english_sentences_train:\n        file.write(f\"{sentence}\\n\")\nwith open(\"./data/english_valid.txt\", \"w\",encoding='utf-8') as file:\n    for sentence in english_sentences_valid:\n        file.write(f\"{sentence}\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:45:00.820132Z","iopub.execute_input":"2024-06-12T19:45:00.820403Z","iopub.status.idle":"2024-06-12T19:45:01.644812Z","shell.execute_reply.started":"2024-06-12T19:45:00.820379Z","shell.execute_reply":"2024-06-12T19:45:01.643988Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"- Setup DataLoader","metadata":{}},{"cell_type":"code","source":"class TextDataset(Dataset):\n\n    def __init__(self, english_sentences, vietnam_sentences):\n        self.english_sentences = english_sentences\n        self.vietnam_sentences = vietnam_sentences\n\n    def __len__(self):\n        return len(self.english_sentences)\n\n    def __getitem__(self, idx):\n        return self.english_sentences[idx], self.vietnam_sentences[idx]","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:45:01.646000Z","iopub.execute_input":"2024-06-12T19:45:01.646798Z","iopub.status.idle":"2024-06-12T19:45:01.653011Z","shell.execute_reply.started":"2024-06-12T19:45:01.646764Z","shell.execute_reply":"2024-06-12T19:45:01.651914Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"data_train = TextDataset(english_sentences_train,vietnam_sentences_train)\nprint(len(data_train))\ndata_train[1]","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:45:01.654139Z","iopub.execute_input":"2024-06-12T19:45:01.654550Z","iopub.status.idle":"2024-06-12T19:45:01.669475Z","shell.execute_reply.started":"2024-06-12T19:45:01.654519Z","shell.execute_reply":"2024-06-12T19:45:01.668573Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"929189\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"(\"I thought we would go to the children's home.\",\n 'Con nghĩ chúng ta nên đến mái ấm. ')"},"metadata":{}}]},{"cell_type":"code","source":"data_valid = TextDataset(english_sentences_valid,vietnam_sentences_valid)\nprint(len(data_valid))\ndata_valid[1]","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:45:01.670536Z","iopub.execute_input":"2024-06-12T19:45:01.670817Z","iopub.status.idle":"2024-06-12T19:45:01.680920Z","shell.execute_reply.started":"2024-06-12T19:45:01.670793Z","shell.execute_reply":"2024-06-12T19:45:01.680128Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"1787\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"(\"I'm sorry. I am nervous today. I had bad dreams.\",\n 'Xin lỡi, hôm nay tôi thấy khó chịu Tối qua tôi đã gặp ác mộng ')"},"metadata":{}}]},{"cell_type":"code","source":"BATCH_SIZE = 30\n\ntrain_loader = DataLoader(data_train, BATCH_SIZE)\nvalid_loader = DataLoader(data_valid, BATCH_SIZE)\niterator = iter(train_loader)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:45:01.682236Z","iopub.execute_input":"2024-06-12T19:45:01.682501Z","iopub.status.idle":"2024-06-12T19:45:01.710877Z","shell.execute_reply.started":"2024-06-12T19:45:01.682479Z","shell.execute_reply":"2024-06-12T19:45:01.710187Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"for batch_num, batch in enumerate(iterator):\n    print(batch)\n    if batch_num > 1:\n        break","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:45:01.711796Z","iopub.execute_input":"2024-06-12T19:45:01.712062Z","iopub.status.idle":"2024-06-12T19:45:01.718055Z","shell.execute_reply.started":"2024-06-12T19:45:01.712039Z","shell.execute_reply":"2024-06-12T19:45:01.717110Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"[('What is it?', \"I thought we would go to the children's home.\", 'Is there something you want to tell your husband?', 'Your master wants to hunt us, burn us, eat our hearts.', 'Or too weak to see this through?', 'OK.', 'The rest are kept in a secure glass viewing boot at the top of the tower.', \"No, I haven't.\", \"Don't touch me.\", 'Come in.', 'We\\'re talking Falkor in \"The Neverending Story.\"', \"- He's an engineer.\", 'Your job is finished.', 'Smart, Nikki.', \"They're trying to save the life of a dear friend.\", 'Yeah, a book doctor for Megan Vernoff.', \"- I'll get the doctor up here.\", 'Why dwell on it?', \"You're a good lad.\", 'Sure.', \"I'm trying to protect your train.\", \"Hey, all I know is you're not safe here anymore.\", \"We mustn't mourn those who give their lives today.\", \"We're not murderers, in spite of what this undertaker says.\", 'And her smile.', 'But still, there is a way for this deluded Queen to find some redemption.', 'Coulson: Did agent Ward give you anything?', \"I'm sorry. I came upon her.\", 'It hurts all the way into his hand.', 'Oh, I did.'), ('Cái gì đó? ', 'Con nghĩ chúng ta nên đến mái ấm. ', 'Có điều gì cô muốn nói với chồng mình không? ', 'Thầy của ngươi muốn săn chúng ta, thiêu chúng ta, ăn tim chúng ta. ', 'Haylàkẻ yếuđuối? ', 'OK. ', 'Chỗ còn lại được để trong một lồng kính ngắm cảnh trên đỉnh toà nhà. ', 'Không, tôi không có. ', 'Đừng có chạm vào người tôi! ', 'Xin mời. ', 'Bự không tưởng được luôn. ', 'Anh ta là một kỹ sư. ', '- Nhiệm vụ đã hoàn thành rồi. - Không. Nhiệm vụ của anh đã hoàn thành. ', 'Thông minh đấy, Nikki. ', 'họ chỉ cố cứu một người bạn thân. ', 'Có, người biên tập sách cho Megan Vernoff. ', 'Tôi sẽ gọi bác sĩ. ', 'Tại sao cứ phải nhắc lại nó? ', 'Chàng trai ngoan lắm. ', 'Chắc rồi. ', 'Tôi đang cố bảo vệ chuyến tàu của anh. ', 'Tôi chỉ biết là ở đây ông không được an toàn. ', 'Chúng ta không cần phải than khóc những người sẽ hy sinh hôm nay. ', 'Chúng ta không phải là những kẻ sát nhân, mặc kệ bọn nhà đòn nói gì. ', 'Và nụ cười của mẹ. ', 'Nhưng vẫn còn cách để ả Nữ hoàng này chuộc tội ', 'Đặc vụ Ward có cho cô biết gì không? ', 'Tôi xin lỗi, tôi đang đi tìm cô ấy. ', 'Cả cánh tay ông ấy đều đau đớn. ', 'tôi có. ')]\n[('Leave a message.', \"You're a devil.\", 'As long as we are together, anywhere is our home.', 'You do?', 'Right down here.', \"What's your wound?\", 'Done.', 'Someone had tipped them off.', \"THE MAN WE'RE LOOKING FOR IS, UH, JUST LIKE YOUR DAD.\", 'What can I do for you?', \"It's about her.\", \"- He's cute.\", '- Computer.', 'Toruk Makto was mighty.', 'You bitch!', 'Come on.', 'I... love you.', 'Meanwhile, a peace rally organized by pop star Gazelle... was marred by protest.', 'Hey, guard.', \"It's like I'm working in the field.\", \"I'd do the same thing to protect my Marines.\", 'You once said that the market mentality would no longer keep the American people stunted.', 'Decaf, I think.', 'I miss you both.', '- Mom.', 'Definitely some private tactical teams. Some kind of rent-a-SWAT.', 'NtEd', 'And we called in two or three medevacs... and they got hit.', \"I CAN'T BELIEVE HE SOLD OUT HIS DAUGHTER TO PROTECT HIS PARTNER.\", 'I did not pick her out.'), ('Hãy để lại lời nhắn! ', 'Đồ quỷ xứ. ', 'Cho tới khi nào hai mẹ con ta chưa chia lìa... thì nơi đâu cũng là nhà. ', 'Bà hiểu? ', 'Ngay dưới này. ', 'Anh bị thương thế nào? ', 'Xong. ', 'Có kẻ đã mật báo. ', 'Người mà bọn cô tìm là, uh, giống như bố cháu. ', 'Tôi có thể giúp gì cho ông? ', 'Nó là về con gái tôi. ', '- Ảnh dễ thương. ', '- Máy vi tính ', 'Toruk Makto rất hùng mạnh. ', 'Thằng khốn! ', 'Lên nào. ', 'Tôi... yêu cô. ', 'Trong lúc đó, cuộc biểu tình ôn hòa của ca sĩ nhạc pop Linh Dương đã bị vấp phải sự phản đối. ', 'Nè, anh lính. ', 'Giống như tớ đang làm việc trên đồng. ', 'Tôi cũng sẽ làm tương tự để bảo vệ lính của mình. ', 'Ông đã từng nói rằng... tâm lý thị trường không còn chỗ cho những người nghèo ở nước Mỹ nữa. ', 'Loại không cafein đi. ', 'Tôi nhớ cả hai người. ', '- Mẹ. ', 'Chắc chắc sẽ có vài đội chiến thuật đặc biệt, dạng như SWAT đánh thuê. ', 'NtEd ', 'Có 2 hoặc 3 cứu thương ra giúp chúng tôi, nhưng rồi họ cũng bị thương. ', 'Tôi không thể tin hắn bỏ con gái mình để bảo vệ đồng bọn. ', 'Con không có chọn nàng ta. ')]\n[(\"Oh, I've learned all right.\", \"It didn't stop you from writing it.\", 'Come on.', 'Your life is adorable.', '- Oh, just a little jaunt. Nothing too wild.', \"I don't mean pleasure, I mean I didn't mind.\", \"You ain't going nowhere.\", 'Ready.', '- You like?', 'You all right there, Private?', \"It's like watching monkeys use tools for the first time. You still pissed about me and Cadence?\", '- Andre.', \"-Who's this remind you of?\", 'They got the word out in the neighborhood.', 'That woman from Ubient.', 'I need at least a mile.', \"I'll take you!\", \"It's heavy.\", 'The Tutt Tutt who move like big running birds and hide their eyes behind trees.', \"Your cock shouldn't go near her till she's slick as a baby seal.\", 'Fire Nation!', 'With the top secret knowledge.', 'Stop the car!', 'Normal.', '-What can I do?', 'Oh, dear.', \"And-and second... What is that thing you're doing with my shoulders?\", 'Where you looking at?', 'What? ...a complete stick deep in the mud.', 'Already did.'), ('Ồ, tôi đã hiểu được nhiều. ', 'Nó đã không ngăn anh viết ra. ', 'Nhanh nào. ', 'Cuộc sống của em dễ thương thật đấy. ', 'Không gì điên loạn lắm đâu ', 'Ý tôi không phải là vui, nhưng cũng không phiền. ', 'Anh sẽ không đi đâu hết. ', 'Sẵn sàng. ', '- Cậu thích chứ? ', 'Cậu ổn chứ, lính mới? ', 'Giống như là lần đầu lũ khỉ biết dùng công cụ vậy. ', '- Andre. ', '- Cái này nhắc anh nhớ tới ai? ', 'Họ đã đánh tiếng trong khu phố rồi. ', 'Người phụ nữ từ Ubient. ', 'Bố cần tối thiểu 1 dặm. ', 'Sợ anh rồi, em khai! ', '- Nặng. ', 'Người TukTuk di chuyển như những con chim và che giấu đôi mắt sau những cây rừng. ', 'Thằng nhỏ không được tới gần cô ta cho tới khi cô ả ẩm ướt như con hải cẩu con. ', 'Hỏa Quốc đấy! ', 'Với những kiến thức tuyệt mật. ', 'Dừng chiếc xe lại! ', 'Bình thường . ', 'Anh có thể làm gì đây? ', 'Ôi trời. ', 'Và thứ hai, động tác mà em đang làm với đôi vai của mình là sao? ', 'Chúng mày nhìn cái gì? ', '- ... không hẳn là cái que trong bùn. ', 'Đã làm. ')]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- Setup model","metadata":{}},{"cell_type":"code","source":"model = Transformer(d_model=512,\n                    ff_hidden=2048,\n                    num_heads=8,\n                    dropout=0.1,\n                    num_blocks=1,\n                    max_length_seq=MAX_LENGTH,\n                    language_to_index=english_to_index,\n                    target_language_to_index=vietnamese_to_index,\n                    start_token=START_TOKEN,\n                    end_token=END_TOKEN,\n                    pad_token=PADDING_TOKEN\n                   )","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:45:01.719368Z","iopub.execute_input":"2024-06-12T19:45:01.720114Z","iopub.status.idle":"2024-06-12T19:45:01.963159Z","shell.execute_reply.started":"2024-06-12T19:45:01.720082Z","shell.execute_reply":"2024-06-12T19:45:01.962262Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:45:01.964403Z","iopub.execute_input":"2024-06-12T19:45:01.964835Z","iopub.status.idle":"2024-06-12T19:45:01.973887Z","shell.execute_reply.started":"2024-06-12T19:45:01.964799Z","shell.execute_reply":"2024-06-12T19:45:01.973090Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"Transformer(\n  (encoder): Encoder(\n    (input_preprocessing): Preprocessing(\n      (token_embedding): TokenEmbedding(\n        (embedding_layer): Embedding(87, 512)\n      )\n      (positional_encoding): PositionalEncoding(\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer_blocks): ModuleList(\n      (0): TransformerBlock(\n        (attention): MultiHeadAttention(\n          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (norm_for_attention): LayerNormalization()\n        (dropout_attention): Dropout(p=0.1, inplace=False)\n        (ff): PositionwiseFeedForward(\n          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n          (relu): ReLU()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (norm_for_ff): LayerNormalization()\n        (dropout_for_ff): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (decoder): Decoder(\n    (output_preprocessing): Preprocessing(\n      (token_embedding): TokenEmbedding(\n        (embedding_layer): Embedding(221, 512)\n      )\n      (positional_encoding): PositionalEncoding(\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer_blocks): ModuleList(\n      (0): TransformerBlock(\n        (attention): MultiHeadAttention(\n          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (norm_for_attention): LayerNormalization()\n        (dropout_attention): Dropout(p=0.1, inplace=False)\n        (cross_attention): MultiHeadAttention(\n          (kv_layer): Linear(in_features=512, out_features=1024, bias=True)\n          (q_layer): Linear(in_features=512, out_features=512, bias=True)\n          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (norm_for_cross_attention): LayerNormalization()\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (ff): PositionwiseFeedForward(\n          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n          (relu): ReLU()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (norm_for_ff): LayerNormalization()\n        (dropout_for_ff): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (linear): Linear(in_features=512, out_features=221, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Loss Function\ncriterian = nn.CrossEntropyLoss(ignore_index=vietnamese_to_index[PADDING_TOKEN],\n                                reduction='none')\n\n# Initialize weight\nfor params in model.parameters():\n    if params.dim() > 1:\n        nn.init.xavier_uniform_(params)\n\n# optimize\noptim = torch.optim.Adam(model.parameters(), lr=1e-4)\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:45:30.606767Z","iopub.execute_input":"2024-06-12T19:45:30.607404Z","iopub.status.idle":"2024-06-12T19:45:31.506721Z","shell.execute_reply.started":"2024-06-12T19:45:30.607372Z","shell.execute_reply":"2024-06-12T19:45:31.505756Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# validation\ndef validation_translator(model, valid_dataloader):\n    iterator = iter(valid_dataloader)\n    valid_loss = []\n    with torch.no_grad():\n        for batch_num, batch in enumerate(iterator):\n            model.eval()\n            language_input = batch[0]\n            language_output = batch[1]\n            \n            # Get mask\n            encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = Masking(language_input, language_output, MAX_LENGTH)\n    \n            # Predict\n            predictions = model(language_input,\n                                language_output,\n                                encoder_self_attention_mask,\n                                decoder_self_attention_mask,\n                                decoder_cross_attention_mask,\n                                encoder_start_token=False,\n                                encoder_end_token=False,\n                                decoder_start_token=True,\n                                decoder_end_token=True)\n    \n            # Loss\n            Truelabels_tokens = model.decoder.output_preprocessing.batch_tokens(batch=language_output,start_token=False,end_token=True)\n    \n            loss = criterian(\n                predictions.view(-1, len(vietnamese_to_index)),\n                Truelabels_tokens.view(-1).to(device)\n            ).to(device)\n            ignore_pad = torch.where(Truelabels_tokens.view(-1) == vietnamese_to_index[PADDING_TOKEN], False, True)\n            loss = loss.sum() / ignore_pad.sum()\n            valid_loss.append(loss.item())\n    return sum(valid_loss) / len(valid_loss)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:45:32.523839Z","iopub.execute_input":"2024-06-12T19:45:32.524366Z","iopub.status.idle":"2024-06-12T19:45:32.534095Z","shell.execute_reply.started":"2024-06-12T19:45:32.524337Z","shell.execute_reply":"2024-06-12T19:45:32.533109Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"- Training","metadata":{}},{"cell_type":"code","source":"model.train()\nmodel.to(device)\nloss_train = []\nloss_valid = []\nhistory = {}\nepochs = 5\n\nfor epoch in range(1,epochs+1):\n    print(f'Epoch {epoch} ' + '-' * (80 - len(str(epoch))))\n    \n    # Training\n    start = time.time()\n    count = 0\n    per = 0\n    iterator = iter(train_loader)\n    length_iter = len(iterator)\n    for batch_num, batch in enumerate(iterator):\n        # Training mode\n        model.train()\n        # Reset Gradient from Backward Pass\n        optim.zero_grad()\n\n        # Get input/output to encoder/decoder\n        language_input = batch[0]\n        language_output = batch[1]\n        # Get mask\n        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = Masking(language_input, language_output, MAX_LENGTH)\n\n        # Predict\n        predictions = model(language_input,\n                            language_output,\n                            encoder_self_attention_mask,\n                            decoder_self_attention_mask,\n                            decoder_cross_attention_mask,\n                            encoder_start_token=False,\n                            encoder_end_token=False,\n                            decoder_start_token=True,\n                            decoder_end_token=True)\n\n        # Loss\n        Truelabels_tokens = model.decoder.output_preprocessing.batch_tokens(batch=language_output,start_token=False,end_token=True)\n\n        loss = criterian(\n            predictions.view(-1, len(vietnamese_to_index)),\n            Truelabels_tokens.view(-1).to(device)\n        ).to(device)\n        ignore_pad = torch.where(Truelabels_tokens.view(-1) == vietnamese_to_index[PADDING_TOKEN], False, True)\n        loss = loss.sum() / ignore_pad.sum()\n\n        # Backward and Optimize\n        loss.backward()\n        optim.step()\n\n        # Each 10%, model will valid \n        if count == length_iter // 10:\n            per += 1\n            print(f\"{per * 10}% Training Progress: time: {round(time.time() - start,2)} seconds - loss: {loss.item()}\")\n            start= time.time()\n            print(f\"- English Input: {language_input[0]}\")\n            print(f\"- Vietnamese True Output: {language_output[0]}\")\n            # Get Sentence of predictions\n            sentence_predict = \"\"\n            for idx in torch.argmax(predictions[0], axis=1):\n                id = int(idx)\n                if id == vietnamese_to_index[END_TOKEN]:\n                    break\n                sentence_predict += index_to_vietnamese[id]\n            print(f\"- Vietnamese Predict: {sentence_predict}\")\n            valid_loss = validation_translator(model=model, valid_dataloader=valid_loader)\n            print(f\"- Validation loss: time: {round(time.time() - start,2)} seconds - loss: {valid_loss}\",end=\"\\n\\n\")\n            # History\n            loss_train.append(loss.item())\n            loss_valid.append(valid_loss)\n            count = 0\n        count += 1   ","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:45:37.184814Z","iopub.execute_input":"2024-06-12T19:45:37.185213Z","iopub.status.idle":"2024-06-12T21:17:10.607464Z","shell.execute_reply.started":"2024-06-12T19:45:37.185182Z","shell.execute_reply":"2024-06-12T21:17:10.606552Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Epoch 1 -------------------------------------------------------------------------------\n10% Training Progress: time: 109.9 seconds - loss: 0.3875023424625397\n- English Input: Here's a surveillance file from the DFS in Mexico.\n- Vietnamese True Output: Đây là tất cả những gì chúng ta có được từ DFS Mexico. \n- Vietnamese Predict: Đây là tất cả những gì chúng ta có được từ D S Mexn    \n- Validation loss: time: 1.32 seconds - loss: 0.2460223543147246\n\n20% Training Progress: time: 109.81 seconds - loss: 0.35357794165611267\n- English Input: Y-you know I'm not a terrorist.\n- Vietnamese True Output: Cô biết tôi không phải là khủng bố! \n- Vietnamese Predict: Cô biết tôi không phải là khủng tàn \n- Validation loss: time: 1.2 seconds - loss: 0.22229043406744797\n\n30% Training Progress: time: 109.72 seconds - loss: 0.25788751244544983\n- English Input: Not till you tell me what the hell you're doing here!\n- Vietnamese True Output: Không, đến khi nào con nói với bố con đang làm gì ở đây! \n- Vietnamese Predict: Không, đến khi nào con nói với bố con đang làm gì ở đây. \n- Validation loss: time: 1.46 seconds - loss: 0.20919304601848127\n\n40% Training Progress: time: 110.0 seconds - loss: 0.2184969037771225\n- English Input: What hostal?\n- Vietnamese True Output: Chỗ trọ nào? \n- Vietnamese Predict: Chỗ trọ nào? \n- Validation loss: time: 1.2 seconds - loss: 0.2015752064685027\n\n50% Training Progress: time: 109.72 seconds - loss: 0.31606751680374146\n- English Input: I need your help!\n- Vietnamese True Output: Bác cần con giúp! \n- Vietnamese Predict: Bác cần con giúp! \n- Validation loss: time: 1.46 seconds - loss: 0.1949114496509234\n\n60% Training Progress: time: 110.0 seconds - loss: 0.2144487053155899\n- English Input: Stop enticing me, sweetheart.\n- Vietnamese True Output: Đừng có mà dụ em. \n- Vietnamese Predict: Đừng có mà dụ em. \n- Validation loss: time: 1.22 seconds - loss: 0.19054643648366135\n\n70% Training Progress: time: 109.84 seconds - loss: 0.3337472081184387\n- English Input: You certainly look terrified, so I suppose we must be.\n- Vietnamese True Output: Trông ông có vẻ sợ hãi nhỉ, vậy coi tôi là khủng bố rồi. \n- Vietnamese Predict: Trông ông có vẻ sợ hãi nhỉ, vậy coi tôi là khủng bố rồi. \n- Validation loss: time: 1.25 seconds - loss: 0.1851848068336646\n\n80% Training Progress: time: 110.06 seconds - loss: 0.17512159049510956\n- English Input: & Default FunctionColors\n- Vietnamese True Output: & Màu Hàm Mặc định \n- Vietnamese Predict: & Màu Hàm Mặc định \n- Validation loss: time: 1.2 seconds - loss: 0.18347618865470092\n\n90% Training Progress: time: 109.82 seconds - loss: 0.18498378992080688\n- English Input: To him, it's his whole life!\n- Vietnamese True Output: Nhưng với ổng, đó là toàn bộ cuộc đời! \n- Vietnamese Predict: Nhưng với ổng, đó là toàn bộ thộc câi  \n- Validation loss: time: 1.2 seconds - loss: 0.18034472092986106\n\n100% Training Progress: time: 109.82 seconds - loss: 0.16690783202648163\n- English Input: Well, uh, how about dead or alive?\n- Vietnamese True Output: Vậy là sống hay chết? \n- Vietnamese Predict: Vậy là sống hay chết? \n- Validation loss: time: 1.2 seconds - loss: 0.17785700627913079\n\nEpoch 2 -------------------------------------------------------------------------------\n10% Training Progress: time: 108.71 seconds - loss: 0.26104089617729187\n- English Input: Here's a surveillance file from the DFS in Mexico.\n- Vietnamese True Output: Đây là tất cả những gì chúng ta có được từ DFS Mexico. \n- Vietnamese Predict: Đây là tất cả những gì chúng ta có được từ DFS Mex.nen \n- Validation loss: time: 1.2 seconds - loss: 0.1758843415727218\n\n20% Training Progress: time: 109.84 seconds - loss: 0.28107842803001404\n- English Input: Y-you know I'm not a terrorist.\n- Vietnamese True Output: Cô biết tôi không phải là khủng bố! \n- Vietnamese Predict: Cô biết tôi không phải là khủng tà  \n- Validation loss: time: 1.22 seconds - loss: 0.1741905430952708\n\n30% Training Progress: time: 109.79 seconds - loss: 0.20342564582824707\n- English Input: Not till you tell me what the hell you're doing here!\n- Vietnamese True Output: Không, đến khi nào con nói với bố con đang làm gì ở đây! \n- Vietnamese Predict: Không, đến khi nào con nói với bố con đang làm gì ở đây. \n- Validation loss: time: 1.21 seconds - loss: 0.1723972755173842\n\n40% Training Progress: time: 109.77 seconds - loss: 0.18077048659324646\n- English Input: What hostal?\n- Vietnamese True Output: Chỗ trọ nào? \n- Vietnamese Predict: Chỗ trọ nào? \n- Validation loss: time: 1.21 seconds - loss: 0.17089230759690205\n\n50% Training Progress: time: 109.91 seconds - loss: 0.2544746696949005\n- English Input: I need your help!\n- Vietnamese True Output: Bác cần con giúp! \n- Vietnamese Predict: Bác cần con giúp! \n- Validation loss: time: 1.21 seconds - loss: 0.16902312623957794\n\n60% Training Progress: time: 109.67 seconds - loss: 0.18833214044570923\n- English Input: Stop enticing me, sweetheart.\n- Vietnamese True Output: Đừng có mà dụ em. \n- Vietnamese Predict: Đừng có mà dụ em. \n- Validation loss: time: 1.2 seconds - loss: 0.16811136715114117\n\n70% Training Progress: time: 109.74 seconds - loss: 0.3067258298397064\n- English Input: You certainly look terrified, so I suppose we must be.\n- Vietnamese True Output: Trông ông có vẻ sợ hãi nhỉ, vậy coi tôi là khủng bố rồi. \n- Vietnamese Predict: Trông ông có vẻ sợ hãi nhỉ, vậy coi tôi là khủng bố rồi. \n- Validation loss: time: 1.21 seconds - loss: 0.16656677064796288\n\n80% Training Progress: time: 109.77 seconds - loss: 0.1551530659198761\n- English Input: & Default FunctionColors\n- Vietnamese True Output: & Màu Hàm Mặc định \n- Vietnamese Predict: & Màu Hàm Mặc định \n- Validation loss: time: 1.21 seconds - loss: 0.16471297691265743\n\n90% Training Progress: time: 109.82 seconds - loss: 0.16159433126449585\n- English Input: To him, it's his whole life!\n- Vietnamese True Output: Nhưng với ổng, đó là toàn bộ cuộc đời! \n- Vietnamese Predict: Nhưng với ổng, đó là toàn bộ.chộc cii. \n- Validation loss: time: 1.2 seconds - loss: 0.16210087599853676\n\n100% Training Progress: time: 109.69 seconds - loss: 0.1430443674325943\n- English Input: Well, uh, how about dead or alive?\n- Vietnamese True Output: Vậy là sống hay chết? \n- Vietnamese Predict: Vậy là sống hay chết? \n- Validation loss: time: 1.2 seconds - loss: 0.16120802958806354\n\nEpoch 3 -------------------------------------------------------------------------------\n10% Training Progress: time: 108.63 seconds - loss: 0.2192208468914032\n- English Input: Here's a surveillance file from the DFS in Mexico.\n- Vietnamese True Output: Đây là tất cả những gì chúng ta có được từ DFS Mexico. \n- Vietnamese Predict: Đây là tất cả những gì chúng ta có được từ DFS Mex.nk. \n- Validation loss: time: 1.21 seconds - loss: 0.16001094554861386\n\n20% Training Progress: time: 109.7 seconds - loss: 0.2401871383190155\n- English Input: Y-you know I'm not a terrorist.\n- Vietnamese True Output: Cô biết tôi không phải là khủng bố! \n- Vietnamese Predict: Cô biết tôi không phải là khủng tà. \n- Validation loss: time: 1.2 seconds - loss: 0.1592447412510713\n\n30% Training Progress: time: 109.87 seconds - loss: 0.18890509009361267\n- English Input: Not till you tell me what the hell you're doing here!\n- Vietnamese True Output: Không, đến khi nào con nói với bố con đang làm gì ở đây! \n- Vietnamese Predict: Không, đến khi nào con nói với bố con đang làm gì ở đâu. \n- Validation loss: time: 1.2 seconds - loss: 0.15812066172560055\n\n40% Training Progress: time: 109.96 seconds - loss: 0.17480997741222382\n- English Input: What hostal?\n- Vietnamese True Output: Chỗ trọ nào? \n- Vietnamese Predict: Chỗ trọ nào? \n- Validation loss: time: 1.2 seconds - loss: 0.15799468227972588\n\n50% Training Progress: time: 109.84 seconds - loss: 0.23460359871387482\n- English Input: I need your help!\n- Vietnamese True Output: Bác cần con giúp! \n- Vietnamese Predict: Bác cần con giúp! \n- Validation loss: time: 1.22 seconds - loss: 0.15749722458422183\n\n60% Training Progress: time: 109.81 seconds - loss: 0.16432920098304749\n- English Input: Stop enticing me, sweetheart.\n- Vietnamese True Output: Đừng có mà dụ em. \n- Vietnamese Predict: Đừng có mà dụ em. \n- Validation loss: time: 1.2 seconds - loss: 0.15580840644737085\n\n70% Training Progress: time: 109.9 seconds - loss: 0.2860378623008728\n- English Input: You certainly look terrified, so I suppose we must be.\n- Vietnamese True Output: Trông ông có vẻ sợ hãi nhỉ, vậy coi tôi là khủng bố rồi. \n- Vietnamese Predict: Trông ông có vẻ sợ hãi nhỉ, vậy coi tôi là khủng bố rồi. \n- Validation loss: time: 1.22 seconds - loss: 0.15470255576074124\n\n80% Training Progress: time: 109.89 seconds - loss: 0.15250802040100098\n- English Input: & Default FunctionColors\n- Vietnamese True Output: & Màu Hàm Mặc định \n- Vietnamese Predict: & Màu Hàm Mặc định \n- Validation loss: time: 1.2 seconds - loss: 0.15332933720201253\n\n90% Training Progress: time: 109.88 seconds - loss: 0.15660147368907928\n- English Input: To him, it's his whole life!\n- Vietnamese True Output: Nhưng với ổng, đó là toàn bộ cuộc đời! \n- Vietnamese Predict: Nhưng với ổng, đó là toàn bộ chộc cời  \n- Validation loss: time: 1.21 seconds - loss: 0.15252882471928994\n\n100% Training Progress: time: 109.88 seconds - loss: 0.13347674906253815\n- English Input: Well, uh, how about dead or alive?\n- Vietnamese True Output: Vậy là sống hay chết? \n- Vietnamese Predict: Vậy là sống hay chết? \n- Validation loss: time: 1.2 seconds - loss: 0.15262499824166298\n\nEpoch 4 -------------------------------------------------------------------------------\n10% Training Progress: time: 108.68 seconds - loss: 0.21030078828334808\n- English Input: Here's a surveillance file from the DFS in Mexico.\n- Vietnamese True Output: Đây là tất cả những gì chúng ta có được từ DFS Mexico. \n- Vietnamese Predict: Đây là tất cả những gì chúng ta có được từ DFS Mex.een \n- Validation loss: time: 1.22 seconds - loss: 0.1516193810850382\n\n20% Training Progress: time: 109.82 seconds - loss: 0.23888731002807617\n- English Input: Y-you know I'm not a terrorist.\n- Vietnamese True Output: Cô biết tôi không phải là khủng bố! \n- Vietnamese Predict: Cô biết tôi không phải là khủng đố. \n- Validation loss: time: 1.2 seconds - loss: 0.1519193443780144\n\n30% Training Progress: time: 109.91 seconds - loss: 0.16960333287715912\n- English Input: Not till you tell me what the hell you're doing here!\n- Vietnamese True Output: Không, đến khi nào con nói với bố con đang làm gì ở đây! \n- Vietnamese Predict: Không, đến khi nào con nói với bố con đang làm gì ở đây  \n- Validation loss: time: 1.2 seconds - loss: 0.15096408873796463\n\n40% Training Progress: time: 109.79 seconds - loss: 0.16528251767158508\n- English Input: What hostal?\n- Vietnamese True Output: Chỗ trọ nào? \n- Vietnamese Predict: Chỗ trọ nào? \n- Validation loss: time: 1.2 seconds - loss: 0.1515328633909424\n\n50% Training Progress: time: 109.91 seconds - loss: 0.23243513703346252\n- English Input: I need your help!\n- Vietnamese True Output: Bác cần con giúp! \n- Vietnamese Predict: Bác cần con giúp! \n- Validation loss: time: 1.21 seconds - loss: 0.15027264809856813\n\n60% Training Progress: time: 109.87 seconds - loss: 0.16549229621887207\n- English Input: Stop enticing me, sweetheart.\n- Vietnamese True Output: Đừng có mà dụ em. \n- Vietnamese Predict: Đừng có mà dụ em. \n- Validation loss: time: 1.2 seconds - loss: 0.1494976321856181\n\n70% Training Progress: time: 109.81 seconds - loss: 0.2811136543750763\n- English Input: You certainly look terrified, so I suppose we must be.\n- Vietnamese True Output: Trông ông có vẻ sợ hãi nhỉ, vậy coi tôi là khủng bố rồi. \n- Vietnamese Predict: Trông ông có vẻ sợ hãi nhỉ, vậy coi tôi là khủng bố rồi. \n- Validation loss: time: 1.22 seconds - loss: 0.14960344266146422\n\n80% Training Progress: time: 109.98 seconds - loss: 0.15492744743824005\n- English Input: & Default FunctionColors\n- Vietnamese True Output: & Màu Hàm Mặc định \n- Vietnamese Predict: & Màu Hàm Mặc định \n- Validation loss: time: 1.2 seconds - loss: 0.14805102497339248\n\n90% Training Progress: time: 109.78 seconds - loss: 0.13721168041229248\n- English Input: To him, it's his whole life!\n- Vietnamese True Output: Nhưng với ổng, đó là toàn bộ cuộc đời! \n- Vietnamese Predict: Nhưng với ổng, đó là toàn bộ thộc sii  \n- Validation loss: time: 1.22 seconds - loss: 0.1476172591249148\n\n100% Training Progress: time: 109.8 seconds - loss: 0.13467159867286682\n- English Input: Well, uh, how about dead or alive?\n- Vietnamese True Output: Vậy là sống hay chết? \n- Vietnamese Predict: Vậy là sống hay chết? \n- Validation loss: time: 1.21 seconds - loss: 0.14783177822828292\n\nEpoch 5 -------------------------------------------------------------------------------\n10% Training Progress: time: 108.63 seconds - loss: 0.19268611073493958\n- English Input: Here's a surveillance file from the DFS in Mexico.\n- Vietnamese True Output: Đây là tất cả những gì chúng ta có được từ DFS Mexico. \n- Vietnamese Predict: Đây là tất cả những gì chúng ta có được từ DFS Mex.ea. \n- Validation loss: time: 1.2 seconds - loss: 0.14677179561307033\n\n20% Training Progress: time: 109.68 seconds - loss: 0.21915781497955322\n- English Input: Y-you know I'm not a terrorist.\n- Vietnamese True Output: Cô biết tôi không phải là khủng bố! \n- Vietnamese Predict: Cô biết tôi không phải là khủng kố. \n- Validation loss: time: 1.2 seconds - loss: 0.14730654023587703\n\n30% Training Progress: time: 109.77 seconds - loss: 0.1692441999912262\n- English Input: Not till you tell me what the hell you're doing here!\n- Vietnamese True Output: Không, đến khi nào con nói với bố con đang làm gì ở đây! \n- Vietnamese Predict: Không, đến khi nào con nói với bố con đang làm gì ở đây! \n- Validation loss: time: 1.2 seconds - loss: 0.1467551783969005\n\n40% Training Progress: time: 109.74 seconds - loss: 0.16025687754154205\n- English Input: What hostal?\n- Vietnamese True Output: Chỗ trọ nào? \n- Vietnamese Predict: Chỗ trọ nào? \n- Validation loss: time: 1.2 seconds - loss: 0.14756864290684463\n\n50% Training Progress: time: 109.75 seconds - loss: 0.2269204556941986\n- English Input: I need your help!\n- Vietnamese True Output: Bác cần con giúp! \n- Vietnamese Predict: Bác cần con giúp! \n- Validation loss: time: 1.2 seconds - loss: 0.14645021526763838\n\n60% Training Progress: time: 109.65 seconds - loss: 0.1590363085269928\n- English Input: Stop enticing me, sweetheart.\n- Vietnamese True Output: Đừng có mà dụ em. \n- Vietnamese Predict: Đừng có mà dụ em. \n- Validation loss: time: 1.2 seconds - loss: 0.1463785583774249\n\n70% Training Progress: time: 109.63 seconds - loss: 0.268985778093338\n- English Input: You certainly look terrified, so I suppose we must be.\n- Vietnamese True Output: Trông ông có vẻ sợ hãi nhỉ, vậy coi tôi là khủng bố rồi. \n- Vietnamese Predict: Trông ông có vẻ sợ hãi nhỉ, vậy coi tôi là khủng bố rồi  \n- Validation loss: time: 1.2 seconds - loss: 0.14590543154627084\n\n80% Training Progress: time: 109.73 seconds - loss: 0.142555370926857\n- English Input: & Default FunctionColors\n- Vietnamese True Output: & Màu Hàm Mặc định \n- Vietnamese Predict: & Màu Hàm Mặc định \n- Validation loss: time: 1.2 seconds - loss: 0.14541721505423386\n\n90% Training Progress: time: 109.72 seconds - loss: 0.12434699386358261\n- English Input: To him, it's his whole life!\n- Vietnamese True Output: Nhưng với ổng, đó là toàn bộ cuộc đời! \n- Vietnamese Predict: Nhưng với ổng, đó là toàn bộ thộc cii  \n- Validation loss: time: 1.2 seconds - loss: 0.1454711215570569\n\n100% Training Progress: time: 109.68 seconds - loss: 0.12920133769512177\n- English Input: Well, uh, how about dead or alive?\n- Vietnamese True Output: Vậy là sống hay chết? \n- Vietnamese Predict: Vậy là sống hay chết? \n- Validation loss: time: 1.22 seconds - loss: 0.14499253338823717\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- Build up a Translate Function","metadata":{}},{"cell_type":"code","source":"def Translate(input_setence):\n    model.eval()\n    input = (input_setence,)\n    output = (\"\",)\n    for index in range(MAX_LENGTH):\n        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = Masking(input, output, MAX_LENGTH)\n        # Predict\n        predictions = model(input,\n                            output,\n                            encoder_self_attention_mask,\n                            decoder_self_attention_mask,\n                            decoder_cross_attention_mask,\n                            encoder_start_token=False,\n                            encoder_end_token=False,\n                            decoder_start_token=True,\n                            decoder_end_token=False)\n        next_token_distribution = predictions[0][index]\n        next_token_index = torch.argmax(next_token_distribution).item()\n        next_token = index_to_vietnamese[next_token_index]\n        if next_token == END_TOKEN:\n            break\n        output = (output[0] + next_token,)\n    return output[0]","metadata":{"execution":{"iopub.status.busy":"2024-06-12T21:17:21.221480Z","iopub.execute_input":"2024-06-12T21:17:21.222093Z","iopub.status.idle":"2024-06-12T21:17:21.229192Z","shell.execute_reply.started":"2024-06-12T21:17:21.222062Z","shell.execute_reply":"2024-06-12T21:17:21.228267Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"Translate(\"Well, uh, how about dead or alive?\")","metadata":{"execution":{"iopub.status.busy":"2024-06-12T21:17:45.053350Z","iopub.execute_input":"2024-06-12T21:17:45.054063Z","iopub.status.idle":"2024-06-12T21:17:45.074776Z","shell.execute_reply.started":"2024-06-12T21:17:45.054035Z","shell.execute_reply":"2024-06-12T21:17:45.073922Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"'KK'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Save model and the weight","metadata":{}},{"cell_type":"code","source":"torch.save(model, 'translator.pth')\ntorch.save(model.state_dict(), 'translator_weights.pth')","metadata":{},"execution_count":null,"outputs":[]}]}