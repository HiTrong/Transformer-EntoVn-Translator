{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Transformer Model - Translator English to Vietnamese - Training","metadata":{}},{"cell_type":"markdown","source":"## Import Library","metadata":{}},{"cell_type":"code","source":"!pip install underthesea","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:29:38.506126Z","iopub.execute_input":"2024-06-15T17:29:38.507065Z","iopub.status.idle":"2024-06-15T17:29:54.068834Z","shell.execute_reply.started":"2024-06-15T17:29:38.507019Z","shell.execute_reply":"2024-06-15T17:29:54.067752Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting underthesea\n  Downloading underthesea-6.8.3-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: Click>=6.0 in /opt/conda/lib/python3.10/site-packages (from underthesea) (8.1.7)\nCollecting python-crfsuite>=0.9.6 (from underthesea)\n  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from underthesea) (3.2.4)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from underthesea) (4.66.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from underthesea) (2.32.3)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from underthesea) (1.4.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from underthesea) (1.2.2)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from underthesea) (6.0.1)\nCollecting underthesea-core==1.0.4 (from underthesea)\n  Downloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl.metadata (1.7 kB)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->underthesea) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (2024.2.2)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (1.11.4)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (3.2.0)\nDownloading underthesea-6.8.3-py3-none-any.whl (20.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl (657 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: underthesea-core, python-crfsuite, underthesea\nSuccessfully installed python-crfsuite-0.9.10 underthesea-6.8.3 underthesea-core-1.0.4\n","output_type":"stream"}]},{"cell_type":"code","source":"# Necessary\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re,string # For Regular Expressions, string handle\nfrom typing import Iterable, List # For building vocab, yield helper\nimport math\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Dataset hugging face\nfrom datasets import load_dataset\n\n# Natural language Processing & Initializing Vocabulary\nfrom underthesea import word_tokenize\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\n\n# Building Transformer and Training\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence # take the max size and add padding_token to the smaller\n","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:29:54.070710Z","iopub.execute_input":"2024-06-15T17:29:54.071048Z","iopub.status.idle":"2024-06-15T17:30:00.527245Z","shell.execute_reply.started":"2024-06-15T17:29:54.071019Z","shell.execute_reply":"2024-06-15T17:30:00.526429Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Load Dataset from Hugging Face","metadata":{}},{"cell_type":"markdown","source":"IWSLT. (n.d.). mt_eng_vietnamese.py. Hugging Face. Retrieved June 14, 2024, from https://huggingface.co/datasets/IWSLT/mt_eng_vietnamese/blob/main/mt_eng_vietnamese.py","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"ncduy/mt-en-vi\")\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:30:00.528500Z","iopub.execute_input":"2024-06-15T17:30:00.529378Z","iopub.status.idle":"2024-06-15T17:30:30.464096Z","shell.execute_reply.started":"2024-06-15T17:30:00.529343Z","shell.execute_reply":"2024-06-15T17:30:30.463027Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/3.43k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c646353a5edf4440bc0a1ec178d893cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/597M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9ab8249ef914ff3b062b393eaf0d1d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/2.45M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13557f0b650f4098acb8d3b82d1b2a5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/2.43M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de7fcb6bd6b545f7912cb1fc9b38089c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2884451 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9b39815033045dc826ecc0e78da2022"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/11316 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"daf1d2b248c54711a5ca236b0c83fb36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/11225 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98c817ecd9f54356a6dec88e7cfea123"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['en', 'vi', 'source'],\n        num_rows: 2884451\n    })\n    validation: Dataset({\n        features: ['en', 'vi', 'source'],\n        num_rows: 11316\n    })\n    test: Dataset({\n        features: ['en', 'vi', 'source'],\n        num_rows: 11225\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"- Just take 500.000 rows from train, all for others","metadata":{}},{"cell_type":"code","source":"df_train = pd.DataFrame(dataset['train'][:100000]).drop(columns=['source'])\ndf_valid = pd.DataFrame(dataset['validation']).drop(columns=['source'])\ndf_test = pd.DataFrame(dataset['test']).drop(columns=['source'])","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:30:30.466541Z","iopub.execute_input":"2024-06-15T17:30:30.466816Z","iopub.status.idle":"2024-06-15T17:30:32.067346Z","shell.execute_reply.started":"2024-06-15T17:30:30.466792Z","shell.execute_reply":"2024-06-15T17:30:32.066470Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df_train.head(3), df_valid.head(3), df_test.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:30:32.068425Z","iopub.execute_input":"2024-06-15T17:30:32.068715Z","iopub.status.idle":"2024-06-15T17:30:32.081056Z","shell.execute_reply.started":"2024-06-15T17:30:32.068690Z","shell.execute_reply":"2024-06-15T17:30:32.080140Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(                                           en  \\\n 0       - Sorry, that question's not on here.   \n 1  He wants you to come with him immediately.   \n 2        I thought we could use some company.   \n \n                                                   vi  \n 0    - Xin lỗi, nhưng mà ở đây không có câu hỏi đấy.  \n 1          Ông ấy muốn bố đi với ông ấy ngay lập tức  \n 2  Tôi nghĩ chúng ta có thể muốn vài người bạn đồ...  ,\n                                                   en  \\\n 0  In August 1764, Bertin permitted the export of...   \n 1  Homeless women used to be invisible to me but ...   \n 2  Pumping water site for artificial infiltration...   \n \n                                                   vi  \n 0  Tháng 8 năm 1764, Bertin lại cho phép xuất khẩ...  \n 1  Tôi từng không hề để ý đến những người phụ nữ ...  \n 2        Bơm nước cho thấm nhân tạo ở quận Sojovice.  ,\n                                                   en  \\\n 0  And what I think the world needs now is more c...   \n 1  The group is named after Bangkok, the capital ...   \n 2  It is surrounded by rivers (Simpson and Coyhai...   \n \n                                                   vi  \n 0  Và tôi nghĩ điều thế giới đang cần bây giờ là ...  \n 1  Nhóm được đặt theo tên của Bangkok, thủ đô của...  \n 2  Nó được bao quanh bởi các con sông (Simpson và...  )"},"metadata":{}}]},{"cell_type":"markdown","source":"- So:\n\nThe training data has 500,000 rows.\n\nThe validation data has 11,316 rows.\n\nThe test data has 11225 rows.","metadata":{}},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"- Check Missing values","metadata":{}},{"cell_type":"code","source":"print(\"Training Data: \")\nprint(df_train.isna().sum())\n\nprint(\"Validation Data: \")\nprint(df_valid.isna().sum())\n\nprint(\"Test Data: \")\nprint(df_test.isna().sum())","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:30:32.082249Z","iopub.execute_input":"2024-06-15T17:30:32.082903Z","iopub.status.idle":"2024-06-15T17:30:32.118634Z","shell.execute_reply.started":"2024-06-15T17:30:32.082869Z","shell.execute_reply":"2024-06-15T17:30:32.117725Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Training Data: \nen    0\nvi    0\ndtype: int64\nValidation Data: \nen    0\nvi    0\ndtype: int64\nTest Data: \nen    0\nvi    0\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- Sentences cleaning: some sentences will have a few mini errors. More than one ' ' or having special symbols for example.","metadata":{}},{"cell_type":"code","source":"def SentenceCleaning(df):\n\n    # Cleaning\n    df['en'] = df['en'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation))) # Make a new trans without punctuation\n    df['vi'] = df['vi'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n\n    df['en'] = df['en'].str.lower() # To easy and smaller vocab we should lower \n    df['vi'] = df['vi'].str.lower()\n\n    df['en'] = df['en'].str.strip() # clear spaces in the beginning and end of sentence\n    df['vi'] = df['vi'].str.strip()\n\n    df['en'] = df['en'].apply(lambda x: re.sub('\\s+',' ',x)) # replace '       ' to ' ' for example\n    df['vi'] = df['vi'].apply(lambda x: re.sub('\\s+',' ',x))\n\n    return df\n    ","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:30:32.120269Z","iopub.execute_input":"2024-06-15T17:30:32.120595Z","iopub.status.idle":"2024-06-15T17:30:32.128839Z","shell.execute_reply.started":"2024-06-15T17:30:32.120565Z","shell.execute_reply":"2024-06-15T17:30:32.128013Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df_train = SentenceCleaning(df_train)\ndf_valid = SentenceCleaning(df_valid)\ndf_test = SentenceCleaning(df_test)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:30:32.130148Z","iopub.execute_input":"2024-06-15T17:30:32.130615Z","iopub.status.idle":"2024-06-15T17:30:36.521473Z","shell.execute_reply.started":"2024-06-15T17:30:32.130581Z","shell.execute_reply":"2024-06-15T17:30:36.520678Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df_train.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:30:36.522797Z","iopub.execute_input":"2024-06-15T17:30:36.523158Z","iopub.status.idle":"2024-06-15T17:30:36.535332Z","shell.execute_reply.started":"2024-06-15T17:30:36.523125Z","shell.execute_reply":"2024-06-15T17:30:36.534402Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                                  en  \\\n0                   sorry that questions not on here   \n1          he wants you to come with him immediately   \n2                i thought we could use some company   \n3  it was founded in 2008 by this anonymous progr...   \n4  with both of these methods no two prints are e...   \n5  from these contexts was born an installation i...   \n6  i have lived to see something which i never ex...   \n7  it is the model for all future relationships w...   \n8                        welcome him as your brother   \n9  so biologists can make all the mutant fruit fl...   \n\n                                                  vi  \n0        xin lỗi nhưng mà ở đây không có câu hỏi đấy  \n1          ông ấy muốn bố đi với ông ấy ngay lập tức  \n2  tôi nghĩ chúng ta có thể muốn vài người bạn đồ...  \n3  nó được sáng lập vào năm 2008 bởi một lập trìn...  \n4  với cả hai phương pháp không có hai bản in nào...  \n5  từ những tình huống này một bố trí không gian ...  \n6  ta đã sống để thấy điều ta không bao giờ mong đợi  \n7  đó là mô hình cho tất cả các mối quan hệ trong...  \n8                chào mừng nó như anh em của các con  \n9  vậy các nhà sinh vật học có thể biến đổi gene ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>vi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sorry that questions not on here</td>\n      <td>xin lỗi nhưng mà ở đây không có câu hỏi đấy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>he wants you to come with him immediately</td>\n      <td>ông ấy muốn bố đi với ông ấy ngay lập tức</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>i thought we could use some company</td>\n      <td>tôi nghĩ chúng ta có thể muốn vài người bạn đồ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>it was founded in 2008 by this anonymous progr...</td>\n      <td>nó được sáng lập vào năm 2008 bởi một lập trìn...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>with both of these methods no two prints are e...</td>\n      <td>với cả hai phương pháp không có hai bản in nào...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>from these contexts was born an installation i...</td>\n      <td>từ những tình huống này một bố trí không gian ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>i have lived to see something which i never ex...</td>\n      <td>ta đã sống để thấy điều ta không bao giờ mong đợi</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>it is the model for all future relationships w...</td>\n      <td>đó là mô hình cho tất cả các mối quan hệ trong...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>welcome him as your brother</td>\n      <td>chào mừng nó như anh em của các con</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>so biologists can make all the mutant fruit fl...</td>\n      <td>vậy các nhà sinh vật học có thể biến đổi gene ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"- Building **Tokenizer** and **Vocabulary**\n\nWith English - The most popular international language, **'torchtext.data.utils.get_tokenizer'** support the default language with 'en' - English. But Vietnamese has not already been supported.\n\nBut get_tokenizer allow us to transmission the function word_token returning a list to get_tokenizer. Then, the problems lead to what framework supports to Vietnamese. It must be Underthesea - the best support to Vietnamese in NLP\n\nYou can read **[torchtext - doc](https://pytorch.org/text/stable/data_utils.html)**!","metadata":{}},{"cell_type":"code","source":"SRC_LANGUAGE = 'en'\nTGT_LANGUAGE = 'vi'","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:30:36.538631Z","iopub.execute_input":"2024-06-15T17:30:36.538961Z","iopub.status.idle":"2024-06-15T17:30:36.542784Z","shell.execute_reply.started":"2024-06-15T17:30:36.538931Z","shell.execute_reply":"2024-06-15T17:30:36.541931Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"- **Tokenizer**","metadata":{}},{"cell_type":"code","source":"tokenizer = {}\n\n# Tokenizer for English\ntokenizer[SRC_LANGUAGE] = get_tokenizer('basic_english')\n\n# Tokenizer for Vietnamese\n# Need help with Underthesea\ndef vi_tokenize(sentence):\n    return word_tokenize(sentence) # word_tokenize from undersea supporting vietnamese\ntokenizer[TGT_LANGUAGE] = get_tokenizer(vi_tokenize)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:30:36.544098Z","iopub.execute_input":"2024-06-15T17:30:36.544460Z","iopub.status.idle":"2024-06-15T17:30:36.552543Z","shell.execute_reply.started":"2024-06-15T17:30:36.544401Z","shell.execute_reply":"2024-06-15T17:30:36.551661Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"print(tokenizer[SRC_LANGUAGE](\"hello can you help me\"))\nprint(tokenizer[TGT_LANGUAGE](\"xin chào bạn có thể giúp tôi không\"))","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:30:36.553635Z","iopub.execute_input":"2024-06-15T17:30:36.553947Z","iopub.status.idle":"2024-06-15T17:30:36.889456Z","shell.execute_reply.started":"2024-06-15T17:30:36.553922Z","shell.execute_reply":"2024-06-15T17:30:36.888508Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"['hello', 'can', 'you', 'help', 'me']\n['xin', 'chào bạn', 'có thể', 'giúp', 'tôi', 'không']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We can clearly see that this Tokenizer will generate dictionaries with words and not characters as we tested before and failed.","metadata":{}},{"cell_type":"markdown","source":"- **Vocabulary**\n\nBuild a Vocab from an iterator.\n```python\ntorchtext.vocab.build_vocab_from_iterator(iterator: Iterable, min_freq: int = 1, specials: Optional[List[str]] = None, special_first: bool = True, max_tokens: Optional[int] = None) → Vocab\n```\n\n> - Parameters:\n\n**iterator** – Iterator used to build Vocab. Must **yield** list or iterator of tokens.\n\n**min_freq** – The minimum frequency needed to include a token in the vocabulary.\n\n**specials** – Special symbols to add. The order of supplied tokens will be preserved.\n\n**special_first** – Indicates whether to insert symbols at the beginning or at the end.\n\n**max_tokens** – If provided, creates the vocab from the max_tokens - len(specials) most frequent tokens.\n\n","metadata":{}},{"cell_type":"code","source":"def yield_token_helper(iterator : Iterable, language: str) -> List[str]:\n    for num_iter, sample_iter in iterator:\n        yield tokenizer[language](sample_iter[language])","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:30:36.890701Z","iopub.execute_input":"2024-06-15T17:30:36.891127Z","iopub.status.idle":"2024-06-15T17:30:36.896232Z","shell.execute_reply.started":"2024-06-15T17:30:36.891091Z","shell.execute_reply":"2024-06-15T17:30:36.895307Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Special Symbols\nUNKNOWN_TOKEN, PADDING_TOKEN, START_TOKEN, END_TOKEN = '<unk>', '<pad>', '<bos>', '<eos>'\nUNKNOWN_IDX, PADDING_IDX, START_IDX, END_IDX = 0,1,2,3","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:30:36.897215Z","iopub.execute_input":"2024-06-15T17:30:36.897508Z","iopub.status.idle":"2024-06-15T17:30:36.906159Z","shell.execute_reply.started":"2024-06-15T17:30:36.897483Z","shell.execute_reply":"2024-06-15T17:30:36.905197Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Building vocab\ndef vocab_building(df):\n    vocab = {}\n    for language in [SRC_LANGUAGE, TGT_LANGUAGE]:\n        iterator = df.iterrows()\n        vocab[language] = build_vocab_from_iterator(\n            iterator=yield_token_helper(iterator, language),\n            min_freq=1,\n            specials=[UNKNOWN_TOKEN, PADDING_TOKEN, START_TOKEN, END_TOKEN], # List special symbols\n            special_first=True\n        )\n\n    for language in [SRC_LANGUAGE, TGT_LANGUAGE]:\n        vocab[language].set_default_index(UNKNOWN_IDX)\n    return vocab","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:30:36.907308Z","iopub.execute_input":"2024-06-15T17:30:36.907572Z","iopub.status.idle":"2024-06-15T17:30:36.915631Z","shell.execute_reply.started":"2024-06-15T17:30:36.907544Z","shell.execute_reply":"2024-06-15T17:30:36.914752Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"vocabulary = vocab_building(pd.concat([df_train, df_valid], ignore_index=True))\nvocabulary","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:30:36.916706Z","iopub.execute_input":"2024-06-15T17:30:36.918239Z","iopub.status.idle":"2024-06-15T17:33:01.410047Z","shell.execute_reply.started":"2024-06-15T17:30:36.918208Z","shell.execute_reply":"2024-06-15T17:33:01.409039Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"{'en': Vocab(), 'vi': Vocab()}"},"metadata":{}}]},{"cell_type":"code","source":"vocabulary[SRC_LANGUAGE]['who']","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:33:01.411096Z","iopub.execute_input":"2024-06-15T17:33:01.411382Z","iopub.status.idle":"2024-06-15T17:33:01.417268Z","shell.execute_reply.started":"2024-06-15T17:33:01.411358Z","shell.execute_reply":"2024-06-15T17:33:01.416346Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"66"},"metadata":{}}]},{"cell_type":"code","source":"vocabulary[TGT_LANGUAGE]['nào']","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:33:01.418501Z","iopub.execute_input":"2024-06-15T17:33:01.418756Z","iopub.status.idle":"2024-06-15T17:33:01.430254Z","shell.execute_reply.started":"2024-06-15T17:33:01.418734Z","shell.execute_reply":"2024-06-15T17:33:01.429339Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"91"},"metadata":{}}]},{"cell_type":"markdown","source":"## Building Complete Model \n\n- Use some of the classes, functions available in torch.nn. If you want a program from scratch, please check ModelBuilding.ipynb\n- Input/Output shape: (seq, batch, feature). We can change this by this parameter:\n\n```python \nbatch_first (bool)\n``` \n\n\n– If True, then the input and output tensors are provided as (batch, seq, feature). Default: False (seq, batch, feature).","metadata":{}},{"cell_type":"code","source":"class TokenEmbedding(nn.Module):\n    \n    def __init__(self, vocab_size:int, d_model:int):\n        super(TokenEmbedding, self).__init__()\n        self.embedding_layer = nn.Embedding(vocab_size, d_model)\n        self.d_model = d_model\n    \n    def forward(self, tokens: torch.Tensor):\n        return self.embedding_layer(tokens.long()) * math.sqrt(self.d_model)\n    \nclass PositionalEncoding(nn.Module):\n    \n    def __init__(self, max_length_seq:int, d_model:int, dropout_rate:float):\n        super(PositionalEncoding, self).__init__()\n        den = torch.exp(- torch.arange(0, d_model, 2)* math.log(10000) / d_model)\n        pos = torch.arange(0, max_length_seq).reshape(max_length_seq, 1)\n        PE = torch.zeros((max_length_seq, d_model))\n        PE[:, 0::2] = torch.sin(pos * den)\n        PE[:, 1::2] = torch.cos(pos * den)\n        PE = PE.unsqueeze(-2)\n        self.register_buffer(\"PE\", PE)\n        self.dropout_layer = nn.Dropout(dropout_rate)\n        \n    def forward(self, token_embedding: torch.Tensor):\n        return self.dropout_layer(token_embedding + self.PE[:token_embedding.size(0), :])\n    \nclass TransformerModel(nn.Module):\n    def __init__(self,\n                 src_vocab_size:int,\n                 tgt_vocab_size:int,\n                 d_model:int,\n                 nhead:int,\n                 num_encoder_layers:int,\n                 num_decoder_layers:int,\n                 dim_feedforward:int,\n                 dropout:float,\n                 max_length_seq=20000):\n        super(TransformerModel,self).__init__()\n        # Input Preprocessing\n        self.input_token_embedding = TokenEmbedding(vocab_size=src_vocab_size,d_model=d_model)\n        \n        # Output Preprocessing\n        self.output_token_embedding = TokenEmbedding(vocab_size=tgt_vocab_size,d_model=d_model)\n        \n        # Positional Encoding\n        self.positional_encoding = PositionalEncoding(max_length_seq=max_length_seq,d_model=d_model,dropout_rate=dropout)\n        \n        # Transformer Architecture with Encoder & Decoder (available nn.Transformer) \n        self.transformer = nn.Transformer(d_model=d_model,\n                                          nhead=nhead,\n                                          num_encoder_layers=num_encoder_layers,\n                                          num_decoder_layers=num_decoder_layers,\n                                          dim_feedforward=dim_feedforward,\n                                          dropout=dropout)\n        \n        # Linear Layer\n        self.generator = nn.Linear(d_model, tgt_vocab_size)\n    \n    def forward(self,\n                src: torch.Tensor,\n                tgt: torch.Tensor,\n                src_mask: torch.Tensor,\n                tgt_mask: torch.Tensor,\n                src_key_padding_mask: torch.Tensor,\n                tgt_key_padding_mask: torch.Tensor,\n                memory_key_padding_mask: torch.Tensor):\n        \n        # Pre-processing\n        src_embedding = self.positional_encoding(self.input_token_embedding(src))\n        tgt_embedding = self.positional_encoding(self.output_token_embedding(tgt))\n        \n        output = self.transformer(src_embedding, tgt_embedding, src_mask, tgt_mask, None, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\n        return self.generator(output)\n    \n    \n    def encode(self, src: torch.Tensor, src_mask: torch.Tensor):\n        return self.transformer.encoder(self.positional_encoding(\n                            self.input_token_embedding(src)), src_mask)\n\n    def decode(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: torch.Tensor):\n        return self.transformer.decoder(self.positional_encoding(\n                          self.output_token_embedding(tgt)), memory,\n                          tgt_mask)\n        ","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:33:01.431389Z","iopub.execute_input":"2024-06-15T17:33:01.431671Z","iopub.status.idle":"2024-06-15T17:33:01.451588Z","shell.execute_reply.started":"2024-06-15T17:33:01.431645Z","shell.execute_reply":"2024-06-15T17:33:01.450659Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"class SupportTransformer:\n    \n    def __init__(self,\n                 device,\n                 src_language:str,\n                 tgt_language:str,\n                 start_idx:int,\n                 end_idx:int,\n                 pad_idx:int,\n                 tokenizer,\n                 vocabulary):\n        self.device = device\n        self.src_language = src_language\n        self.tgt_language = tgt_language\n        self.start_idx = start_idx\n        self.end_idx = end_idx\n        self.pad_idx = pad_idx\n        self.tokenizer = tokenizer\n        self.vocabulary = vocabulary\n    \n    def generate_square_subsequent_mask(self, size):\n        mask = (torch.triu(torch.ones((size, size), device=self.device)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\n    def create_mask(self, src, tgt):\n        src_seq_len = src.shape[0]\n        tgt_seq_len = tgt.shape[0]\n\n        tgt_mask = self.generate_square_subsequent_mask(tgt_seq_len)\n        src_mask = torch.zeros((src_seq_len, src_seq_len),device=self.device).type(torch.bool)\n\n        src_padding_mask = (src == self.pad_idx).transpose(0, 1)\n        tgt_padding_mask = (tgt == self.pad_idx).transpose(0, 1)\n        return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n    \n    def preprocessing_sentece(self, sentence:str, options=True): # True for src, False for tgt\n        lang = self.src_language if options else self.tgt_language\n        # Tokenizer\n        tokens = self.tokenizer[lang](sentence.rstrip(\"\\n\"))\n        # vocabulary: text -> number\n        tokens_idx = self.vocabulary[lang](tokens)\n        # Add start_token, end_token and append\n        return torch.cat((torch.tensor([self.start_idx]),\n                                      torch.tensor(tokens_idx),\n                                      torch.tensor([self.end_idx])))\n        \n    \n    def get_batch(self, df):\n        return list(zip(df[self.src_language], df[self.tgt_language]))\n    \n    def preprocessing_batch(self,batch):\n        src_out, tgt_out = [], []\n        \n        for src_data, tgt_data in batch:\n            src_out.append(self.preprocessing_sentece(src_data,options=True))\n            tgt_out.append(self.preprocessing_sentece(tgt_data,options=False))\n            \n        src_batch, tgt_batch = pad_sequence(src_out, padding_value=self.pad_idx), pad_sequence(tgt_out, padding_value=self.pad_idx)\n    \n        return src_batch, tgt_batch\n    \n    def evaluate(self, model, loss_func, df_valid, batch_size=30, accumulation_steps=5):\n        model.eval()\n        valid_loss = 0 \n        valid_batch = self.get_batch(df_valid)\n        valid_dataloader = DataLoader(valid_batch, batch_size=batch_size, collate_fn=self.preprocessing_batch)\n        for index, (src, tgt) in enumerate(valid_dataloader):\n            src = src.to(self.device)\n            tgt = tgt.to(self.device)\n            \n            tgt_input = tgt[:-1, :] # Without the last word\n            \n            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = self.create_mask(src, tgt_input)\n            \n            # predictions\n            predictions = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n            \n            tgt_out = tgt[1:, :]\n            # Loss\n            loss = loss_func(predictions.reshape(-1, predictions.shape[-1]), tgt_out.reshape(-1))\n            loss = loss / accumulation_steps\n            valid_loss += loss.item()\n            \n        return valid_loss / len(val_dataloader)\n            \n        \n    \n    def train(self, model, optimizer, loss_func, df_train, batch_size = 30, accumulation_steps = 5):\n        model.train()\n        \n        train_loss = 0\n        train_batch = self.get_batch(df_train)\n        train_dataloader = DataLoader(train_batch, batch_size=batch_size, collate_fn=self.preprocessing_batch)\n        \n        # Reset grad\n        optimizer.zero_grad()\n        for index, (src, tgt) in enumerate(train_dataloader):\n            src = src.to(self.device)\n            tgt = tgt.to(self.device)\n            \n            tgt_input = tgt[:-1, :] # Without the last word\n            \n            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = self.create_mask(src, tgt_input)\n            \n            # predictions\n            predictions = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n            \n            tgt_out = tgt[1:, :]\n            # Loss\n            loss = loss_func(predictions.reshape(-1, predictions.shape[-1]), tgt_out.reshape(-1))\n            loss = loss / accumulation_steps\n            loss.backward()\n            \n            if (index+1) % accumulation_steps == 0:\n                optimizer.step()\n                optimizer.zero_grad() # Reset gradients tensor \n            train_loss += loss.item()\n            \n        return train_loss / len(train_dataloader)\n    \n    def generate(self, model, src_sentence):\n        start_symbol = self.start_idx\n        src = self.preprocessing_sentece(src_sentence, True).view(-1, 1)\n        max_len = src.shape[0]\n        src_mask = (torch.zeros(max_len, max_len)).type(torch.bool)\n        \n        src = src.to(self.device)\n        src_mask = src_mask.to(self.device)\n\n        memory = model.encode(src, src_mask)\n        ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(self.device)\n        for i in range(max_len-1):\n            memory = memory.to(self.device)\n            tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n                        .type(torch.bool)).to(self.device)\n            out = model.decode(ys, memory, tgt_mask)\n            out = out.transpose(0, 1)\n            prob = model.generator(out[:, -1])\n            _, next_word = torch.max(prob, dim=1)\n            next_word = next_word.item()\n\n            ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n            if next_word == self.end_idx:\n                break\n        return ys\n    \n    def training(self, model, optimizer, loss_func, earlystopping, df_train, df_valid, epochs=5, batch_size = 30, accumulation_steps = 5, custom_test=None):\n        history = {'train_loss': [], 'valid_loss': []}\n        for epoch in range(1, epochs + 1):\n            print(f\"Epoch {epoch}: \" + (\"-\" * 80))\n            start = time.time()\n            train_loss = self.train(model, optimizer, loss_func, df_train, batch_size, accumulation_steps)\n            valid_loss = self.evaluate(model, loss_func, df_valid, batch_size)\n            print(f\"- Train loss: {train_loss:.3f} - Valid loss: {valid_loss:.3f} - Time training: {(time.time() - start):.3f}\")\n            \n            # custom_test\n            if custom_test != None:\n                print(f\"Input '{self.src_language}': {custom_test}\")\n                print(f\"Output '{self.tgt_language}' generate: {self.generate(model, custom_test)}\",end=\"\\n\\n\")\n            else:\n                print(\"\\n\\n\")\n                \n            # EarylyStopping\n            earlystopping(train_loss, valid_loss)\n            if earlystopping.early_stop:\n                print(\"Early Stopping active!\")\n                break","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:33:01.452719Z","iopub.execute_input":"2024-06-15T17:33:01.452959Z","iopub.status.idle":"2024-06-15T17:33:01.486911Z","shell.execute_reply.started":"2024-06-15T17:33:01.452939Z","shell.execute_reply":"2024-06-15T17:33:01.486016Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## Setup Model","metadata":{}},{"cell_type":"markdown","source":"- **Config**","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nsrc_vocab_size = len(vocabulary[SRC_LANGUAGE])\ntgt_vocab_size = len(vocabulary[TGT_LANGUAGE])\nd_model = 512\nnhead = 8 # d_model must be divisible by nhead\nnum_encoder_layers = 4\nnum_decoder_layers = 4\ndim_feedforward = 512\ndropout = 0.1\nEPOCHS = 10\nBATCH_SIZE = 30","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:33:01.488086Z","iopub.execute_input":"2024-06-15T17:33:01.488370Z","iopub.status.idle":"2024-06-15T17:33:01.525179Z","shell.execute_reply.started":"2024-06-15T17:33:01.488346Z","shell.execute_reply":"2024-06-15T17:33:01.524169Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"- **Model, Optimizer, EarlyStopping and Loss**","metadata":{"execution":{"iopub.status.busy":"2024-06-14T18:52:20.891679Z","iopub.execute_input":"2024-06-14T18:52:20.892297Z","iopub.status.idle":"2024-06-14T18:52:20.898207Z","shell.execute_reply.started":"2024-06-14T18:52:20.892265Z","shell.execute_reply":"2024-06-14T18:52:20.897134Z"}}},{"cell_type":"code","source":"model = TransformerModel(src_vocab_size=src_vocab_size,\n                         tgt_vocab_size=tgt_vocab_size,\n                         d_model=d_model,\n                         nhead=nhead,\n                         num_encoder_layers=num_encoder_layers,\n                         num_decoder_layers=num_decoder_layers,\n                         dim_feedforward=dim_feedforward,\n                         dropout=dropout)\nmodel = model.to(device)\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:33:01.526392Z","iopub.execute_input":"2024-06-15T17:33:01.527133Z","iopub.status.idle":"2024-06-15T17:33:03.499749Z","shell.execute_reply.started":"2024-06-15T17:33:01.527097Z","shell.execute_reply":"2024-06-15T17:33:03.498869Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"TransformerModel(\n  (input_token_embedding): TokenEmbedding(\n    (embedding_layer): Embedding(86219, 512)\n  )\n  (output_token_embedding): TokenEmbedding(\n    (embedding_layer): Embedding(84232, 512)\n  )\n  (positional_encoding): PositionalEncoding(\n    (dropout_layer): Dropout(p=0.1, inplace=False)\n  )\n  (transformer): Transformer(\n    (encoder): TransformerEncoder(\n      (layers): ModuleList(\n        (0-3): 4 x TransformerEncoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (linear1): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=512, out_features=512, bias=True)\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): TransformerDecoder(\n      (layers): ModuleList(\n        (0-3): 4 x TransformerDecoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (multihead_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (linear1): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=512, out_features=512, bias=True)\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n          (dropout3): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (generator): Linear(in_features=512, out_features=84232, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Initializing the weights of paramaters which have more than a dimension\nfor p in model.parameters():\n    if p.dim() > 1:\n        nn.init.xavier_uniform_(p)\n        \n# Loss\nloss_func = nn.CrossEntropyLoss(ignore_index=PADDING_IDX)\n\n# Optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n\n# EarlyStopping\nclass EarlyStopping():\n    def __init__(self, tolerance=5, min_delta=0):\n\n        self.tolerance = tolerance\n        self.min_delta = min_delta\n        self.counter = 0\n        self.early_stop = False\n\n    def __call__(self, train_loss, validation_loss):\n        if (validation_loss - train_loss) > self.min_delta:\n            self.counter +=1\n            if self.counter >= self.tolerance:  \n                self.early_stop = True\n                \nearlystopping  = EarlyStopping(tolerance=5, min_delta=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:33:03.500996Z","iopub.execute_input":"2024-06-15T17:33:03.501353Z","iopub.status.idle":"2024-06-15T17:33:04.258992Z","shell.execute_reply.started":"2024-06-15T17:33:03.501319Z","shell.execute_reply":"2024-06-15T17:33:04.258017Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## Training Model","metadata":{}},{"cell_type":"code","source":"sp = SupportTransformer(device=device,\n                        src_language=SRC_LANGUAGE,\n                        tgt_language=TGT_LANGUAGE,\n                        start_idx=START_IDX,\n                        end_idx=END_IDX,\n                        pad_idx=PADDING_IDX,\n                        tokenizer=tokenizer,\n                        vocabulary=vocabulary\n                       )\n \nsp.training(model=model,\n            optimizer=optimizer,\n            loss_func=loss_func,\n            earlystopping=earlystopping,\n            df_train=df_train,\n            df_valid=df_valid,\n            epochs=EPOCHS,\n            batch_size=BATCH_SIZE,\n            accumulation_steps=5,\n            custom_test=\"do you know me\")","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:33:04.260303Z","iopub.execute_input":"2024-06-15T17:33:04.260891Z","iopub.status.idle":"2024-06-15T17:40:49.190829Z","shell.execute_reply.started":"2024-06-15T17:33:04.260853Z","shell.execute_reply":"2024-06-15T17:40:49.189305Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Epoch 1: --------------------------------------------------------------------------------\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m sp \u001b[38;5;241m=\u001b[39m SupportTransformer(device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m      2\u001b[0m                         src_language\u001b[38;5;241m=\u001b[39mSRC_LANGUAGE,\n\u001b[1;32m      3\u001b[0m                         tgt_language\u001b[38;5;241m=\u001b[39mTGT_LANGUAGE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m                         vocabulary\u001b[38;5;241m=\u001b[39mvocabulary\n\u001b[1;32m      9\u001b[0m                        )\n\u001b[0;32m---> 11\u001b[0m \u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mearlystopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearlystopping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdf_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdf_valid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_valid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcustom_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdo you know me\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[20], line 154\u001b[0m, in \u001b[0;36mSupportTransformer.training\u001b[0;34m(self, model, optimizer, loss_func, earlystopping, df_train, df_valid, epochs, batch_size, accumulation_steps, custom_test)\u001b[0m\n\u001b[1;32m    152\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    153\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain(model, optimizer, loss_func, df_train, batch_size, accumulation_steps)\n\u001b[0;32m--> 154\u001b[0m valid_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Valid loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Time training: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# custom_test\u001b[39;00m\n","Cell \u001b[0;32mIn[20], line 66\u001b[0m, in \u001b[0;36mSupportTransformer.evaluate\u001b[0;34m(self, model, loss_func, df_valid, batch_size, accumulation_steps)\u001b[0m\n\u001b[1;32m     64\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     65\u001b[0m valid_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \n\u001b[0;32m---> 66\u001b[0m valid_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_valid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m valid_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(valid_batch, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, collate_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessing_batch)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, (src, tgt) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(valid_dataloader):\n","Cell \u001b[0;32mIn[20], line 50\u001b[0m, in \u001b[0;36mSupportTransformer.get_batch\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, df):\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msrc_language\u001b[49m\u001b[43m]\u001b[49m, df[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtgt_language]))\n","\u001b[0;31mTypeError\u001b[0m: 'CrossEntropyLoss' object is not subscriptable"],"ename":"TypeError","evalue":"'CrossEntropyLoss' object is not subscriptable","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}