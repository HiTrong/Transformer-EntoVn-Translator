{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bdb8bd9-5949-419a-a6b2-d22990695115",
   "metadata": {},
   "source": [
    "# Transformer Neural Networks - Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c350f7f-2caf-4cbd-8d7d-b16e89caeb63",
   "metadata": {},
   "source": [
    "## 1. Overall Transformer Neural Networks Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea70ff5-e9b7-49ec-b7a3-e8c0de79e6ce",
   "metadata": {},
   "source": [
    "![Transformer Architect Image](./img/transformerarchitect.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0952992-5e25-45ae-af88-db73ba097af4",
   "metadata": {},
   "source": [
    "As we see, the overall architecture of the Transformer is in the upper part. It is probably quite difficult to understand the detailed picture so we need a broader picture. Let's see the next picture!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41818887-f778-49f6-84fd-eb62a834088c",
   "metadata": {},
   "source": [
    "![Transformer Architect Image](./img/transformerblock.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb81cdc-82e3-4705-884e-040fd86664ba",
   "metadata": {},
   "source": [
    "1. Input/Output Pre-processing\n",
    "\n",
    "Token Embeddings\n",
    "- **Function**: Converts each token in the input sequence into a dense vector of fixed size (commonly 512 dimensions).\n",
    "- **Implementation**: An embedding layer maps each token to a dense vector.\n",
    "\n",
    "Positional Encodings\n",
    "- **Function**: Adds information about the position of each token in the sequence to the token embeddings, since the Transformer architecture does not inherently capture sequence order.\n",
    "- **Implementation**: Sinusoidal functions are used to generate positional encodings, which are then added to the token embeddings.\n",
    "\n",
    "2. Encoder\n",
    "\n",
    "Processes the input sequence with a stack of identical layers. Each layer consists of:\n",
    "\n",
    "Multi-Head Self-Attention\n",
    "- **Function**: Allows each token to attend to all other tokens in the sequence, capturing dependencies regardless of their distance in the sequence.\n",
    "- **Implementation**: Multiple attention heads operate in parallel to learn different aspects of the input.\n",
    "\n",
    "Add & Norm\n",
    "- **Function**: Adds the input of each sub-layer to its output (residual connection) and applies layer normalization to stabilize and speed up training.\n",
    "- **Implementation**: Addition followed by normalization.\n",
    "\n",
    "Feed-Forward\n",
    "- **Function**: Applies a fully connected feed-forward network to each position independently and identically.\n",
    "- **Implementation**: Two linear transformations with a ReLU activation in between.\n",
    "\n",
    "3. Decoder\n",
    "\n",
    "Generates the output sequence from the encoded input using a stack of identical layers. Each layer consists of:\n",
    "\n",
    "Masked Multi-Head Self-Attention\n",
    "- **Function**: Prevents attending to future tokens in the sequence during training (autoregressive property).\n",
    "- **Implementation**: Similar to the encoder's self-attention but with a mask to prevent future token attention.\n",
    "\n",
    "Multi-Head Attention over Encoder’s Output\n",
    "- **Function**: Allows each position in the decoder to attend to all positions in the encoder's output.\n",
    "- **Implementation**: Standard multi-head attention mechanism applied to the encoder’s output.\n",
    "\n",
    "Add & Norm\n",
    "- **Function**: Similar to the encoder's Add & Norm, it adds residual connections and normalizes the output.\n",
    "- **Implementation**: Addition followed by normalization.\n",
    "\n",
    "Feed-Forward\n",
    "- **Function**: Similar to the encoder's feed-forward network.\n",
    "- **Implementation**: Two linear transformations with a ReLU activation in between.\n",
    "\n",
    "4. Output Post-processing\n",
    "\n",
    "**Function**\n",
    "- Transforms the decoder’s output into probabilities over the vocabulary.\n",
    "\n",
    "**Implementation**\n",
    "- A linear layer followed by a softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f4159c-646b-4aec-8cff-1b643d350578",
   "metadata": {},
   "source": [
    "## 2. Detailed architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e07bc12-4be4-4563-a951-0c2f8e96dc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afd82f0c-85d0-4b55-a2f2-4bf74cd17e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da606af1-7d77-474b-9be3-efe085e84fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. PyTorch is using GPU.\n",
      "Number of GPUs available:  1\n",
      "GPU name:  NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. PyTorch is using GPU.\")\n",
    "    print(\"Number of GPUs available: \", torch.cuda.device_count())\n",
    "    print(\"GPU name: \", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch is using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a131ee-75f2-4eab-b6f0-9bf0c6152ecc",
   "metadata": {},
   "source": [
    "### 2.1. Input Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb17f2cd-ae99-4352-bed6-a6d981c0be6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32470ad9-3c79-48f7-a0b1-9c57ca6a9f69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7257f87-d6c0-48a7-b194-dc12a5ba33ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ec8e121-a8f8-4d0d-9661-c279989f0171",
   "metadata": {},
   "source": [
    "### 2.2. Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d812fae-7159-4d7f-82d0-008eb8d554ac",
   "metadata": {},
   "source": [
    "#### 2.2.1. Multi-Head Attention Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd46e4a-5467-4700-9170-d5d05ca7d1ec",
   "metadata": {},
   "source": [
    "![Encoder](./img/multihead-attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad45180-fe92-47e4-9e6c-f8c255f2344c",
   "metadata": {},
   "source": [
    "With input include:\n",
    "> - Query: What are we looking for?\n",
    "> - Key: What do we offer for?\n",
    "> - Value: What do we actually offer for?\n",
    "\n",
    "With output include:\n",
    "> - New Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dd73ba6-a780-4753-adab-77b5464a83b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need Query, Key, Value For Example with sequence \"I love you so much\" \n",
    "sequence_length , k_dim, v_dim = 5 , 10 , 10\n",
    "q = torch.randn(sequence_length, k_dim)\n",
    "k = torch.randn(sequence_length, k_dim)\n",
    "v = torch.randn(sequence_length, v_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba49b6de-2467-444e-9e29-4c334d3d9db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9396, -0.7610, -0.7723,  0.2179,  0.3140, -0.2625, -0.8789,  0.6244,\n",
      "         -0.5245,  1.2776],\n",
      "        [-0.7042,  0.1270,  1.6283, -1.5839,  1.0607, -0.1741, -0.5154,  1.0230,\n",
      "         -0.0220,  1.5466],\n",
      "        [ 2.4340,  3.1458, -1.2979, -1.6841, -0.2796,  1.7375, -0.3538,  0.6227,\n",
      "         -0.9432,  0.9826],\n",
      "        [-0.4292,  0.4479,  0.2516, -0.1197,  0.3054, -0.7416,  1.0121, -1.2222,\n",
      "         -0.2376, -2.1837],\n",
      "        [-0.6403,  0.3252,  0.0717,  0.9909, -0.8243, -0.3065, -0.0571, -0.5463,\n",
      "         -0.5385,  0.3860]])\n",
      "tensor([[ 0.2530,  0.7966, -0.2162, -1.2275, -0.7208,  0.2060,  0.6973,  1.0014,\n",
      "         -0.1323, -0.5996],\n",
      "        [-1.8674,  0.7221,  0.1546, -1.6387, -1.7548,  0.6751, -0.9566,  1.4463,\n",
      "         -1.7166,  0.9057],\n",
      "        [ 0.2133,  0.4005, -0.7649, -1.1516, -0.1740,  0.8551, -0.5812,  0.7501,\n",
      "         -0.3315,  0.2571],\n",
      "        [ 0.3450, -0.1597, -1.0515,  0.2874, -1.3148,  0.5086, -0.1085,  1.4433,\n",
      "         -0.5419, -1.5695],\n",
      "        [ 0.3310, -0.8523,  1.1305,  1.5390, -1.2362, -0.7589, -1.4904,  0.8662,\n",
      "         -0.9653, -1.2031]])\n",
      "tensor([[ 0.3206, -0.8451, -1.4655,  0.0448, -0.7504, -0.4821, -0.3502,  1.9636,\n",
      "          0.7334,  0.6107],\n",
      "        [-0.2962,  0.6164, -0.1401, -1.1631, -0.1552, -1.2128,  0.8274, -1.3171,\n",
      "          0.6409, -1.4501],\n",
      "        [ 0.8960,  0.0777, -0.2443, -1.2703,  0.7731,  0.5421,  0.2034,  0.9009,\n",
      "          0.4646, -1.2535],\n",
      "        [ 0.7610,  1.9847, -0.2595,  0.6914,  0.7838, -0.4053,  1.1034, -1.2059,\n",
      "         -0.2854, -0.1248],\n",
      "        [ 1.2129, -0.0500,  0.6680,  0.4828,  1.4782, -0.2555,  0.2094,  0.3646,\n",
      "          0.4776, -0.5227]])\n"
     ]
    }
   ],
   "source": [
    "print(q)\n",
    "print(k)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43c9607-a401-4dbd-b765-ba406b8d3191",
   "metadata": {},
   "source": [
    "- Scaled Dot-Product Attention:\n",
    "Funtion:\n",
    "\n",
    "$$\n",
    "\\text{self attention} = softmax\\bigg(\\frac{Q.K^T}{\\sqrt{d_k}}+M\\bigg)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{new V} = \\text{self attention}.V\n",
    "$$ \n",
    "\n",
    "Q,K,V -(1)-> Matmul(Q,K.T) -(2)-> Scale -(3)-> Masking (Not Required for Encode) -(4)-> Softmax -(5)-> Matmul (. ,V) -> new V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5799c028-d217-4dc3-a407-dbc8129bd3b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9091,  3.8016,  1.0370, -0.5988,  0.4307],\n",
       "        [ 0.4555,  5.6862,  1.6175, -4.7966, -2.3024],\n",
       "        [ 5.9415,  5.6973,  7.4837,  2.3756, -6.1123],\n",
       "        [ 0.7904, -3.9823, -2.6418,  0.3848,  0.0512],\n",
       "        [-1.3507,  1.5958, -1.4197, -0.2317,  2.0358]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: calculate Q.K_t\n",
    "step1 = torch.matmul(q,k.t())\n",
    "step1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b095284-f9be-4e10-951c-2a874f3d6257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.0842), tensor(0.8988), tensor(11.5111))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Scale\n",
    "# Check variance\n",
    "q.var(), k.var(), step1.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6ee4a7-0f56-405e-86e6-38050edbb6fc",
   "metadata": {},
   "source": [
    "As we see, the variance distance between Q, K, matmul(Q,K.T) is very high. We have to scale again so that the softmax function can work effectively, creating a probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cc5b3a3-f8fb-483b-9f75-42bb93f0b3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6037,  1.2022,  0.3279, -0.1894,  0.1362],\n",
       "        [ 0.1440,  1.7981,  0.5115, -1.5168, -0.7281],\n",
       "        [ 1.8789,  1.8016,  2.3665,  0.7512, -1.9329],\n",
       "        [ 0.2500, -1.2593, -0.8354,  0.1217,  0.0162],\n",
       "        [-0.4271,  0.5046, -0.4489, -0.0733,  0.6438]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step2 = step1 / math.sqrt(k_dim)\n",
    "step2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747a282f-bed5-439c-912a-6888b20ed229",
   "metadata": {},
   "source": [
    "But **why** is **the number of Key dimension** and **why** use **square** ?\n",
    "\n",
    "Dividing by the square root of the dimension of the weight vector (in this case k_dim) has an important meaning.\n",
    "\n",
    "Specifically, in the Multi-Head Attention mechanism, after calculating attention scores, we will apply the softmax function to normalize these scores into a probability distribution. Dividing each score by the square root of k_dim helps control the amplitudes of the scores, preventing them from becoming too large or too small. This improves model stability and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "caad7607-8720-4c9a-bdb6-691dc2a4eb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Masking\n",
    "# This is not required in the encoding block but it is required in the decoding block\n",
    "# Masking is used to make sure that the current word/ token/ ... doesn't take the context from the future / generated word. That is cheating!!!!\n",
    "mask = torch.tril(torch.ones(sequence_length,sequence_length))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3ef63a1-ffb1-43b5-ba98-e63100f2e0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6037,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.1440,  1.7981,  0.0000,  0.0000,  0.0000],\n",
       "        [ 1.8789,  1.8016,  2.3665,  0.0000,  0.0000],\n",
       "        [ 0.2500, -1.2593, -0.8354,  0.1217,  0.0000],\n",
       "        [-0.4271,  0.5046, -0.4489, -0.0733,  0.6438]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mask filter\n",
    "step3 = torch.tril(step2)\n",
    "step3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4930851-c43f-4fd4-b04a-b54e37b5fb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6037,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.1440,  1.7981,    -inf,    -inf,    -inf],\n",
       "        [ 1.8789,  1.8016,  2.3665,    -inf,    -inf],\n",
       "        [ 0.2500, -1.2593, -0.8354,  0.1217,    -inf],\n",
       "        [-0.4271,  0.5046, -0.4489, -0.0733,  0.6438]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step3 = torch.where(step3 == 0, float('-inf'), step3)\n",
    "step3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c46613-8ade-4b71-87c2-6e4781c4fc17",
   "metadata": {},
   "source": [
    "**Why** is '-inf' ?\n",
    "\n",
    "When we take this to the Softmax Function(x), $e^{x}$ will go to value '0' if x goes to '-inf'. Otherwise, The value '0' will result in $e^{x}$ equals 1!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81e53cf-7fe1-4c8b-873c-f956a4f7c84e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{softmax} = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5295fdbf-3d48-40d3-b80f-2efbfd6347e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1606, 0.8394, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2814, 0.2604, 0.4582, 0.0000, 0.0000],\n",
       "        [0.4101, 0.0907, 0.1385, 0.3607, 0.0000],\n",
       "        [0.1129, 0.2866, 0.1104, 0.1608, 0.3294]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Softmax\n",
    "step4 = (torch.exp(step3).t() / torch.sum(torch.exp(step3),axis=-1)).t()\n",
    "step4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b266c3aa-a7b8-49dd-8899-8785895b021c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3206, -0.8451, -1.4655,  0.0448, -0.7504, -0.4821, -0.3502,  1.9636,\n",
       "          0.7334,  0.6107],\n",
       "        [-0.1972,  0.3817, -0.3529, -0.9692, -0.2507, -1.0954,  0.6383, -0.7904,\n",
       "          0.6558, -1.1193],\n",
       "        [ 0.4236, -0.0417, -0.5607, -0.8724,  0.1027, -0.2031,  0.2102,  0.6222,\n",
       "          0.5862, -0.7802],\n",
       "        [ 0.5033,  0.4360, -0.7411, -0.0136,  0.0680, -0.3787,  0.3576,  0.3756,\n",
       "          0.3203, -0.0997],\n",
       "        [ 0.5721,  0.3924, -0.0542, -0.1984,  0.5691, -0.4914,  0.4664, -0.1302,\n",
       "          0.4292, -0.6773]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step5: matmul(.,V)\n",
    "step5 = torch.matmul(step4, v)\n",
    "step5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30d5d87-7f2e-4bfe-8a7e-7ac07f1b92b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TorchCuda",
   "language": "python",
   "name": "torchcuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
