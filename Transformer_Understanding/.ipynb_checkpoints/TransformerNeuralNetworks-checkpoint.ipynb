{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bdb8bd9-5949-419a-a6b2-d22990695115",
   "metadata": {},
   "source": [
    "# Transformer Neural Networks - Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c350f7f-2caf-4cbd-8d7d-b16e89caeb63",
   "metadata": {},
   "source": [
    "## 1. Overall Transformer Neural Networks Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea70ff5-e9b7-49ec-b7a3-e8c0de79e6ce",
   "metadata": {},
   "source": [
    "![Transformer Architect Image](./img/transformerarchitect.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0952992-5e25-45ae-af88-db73ba097af4",
   "metadata": {},
   "source": [
    "As we see, the overall architecture of the Transformer is in the upper part. It is probably quite difficult to understand the detailed picture so we need a broader picture. Let's see the next picture!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41818887-f778-49f6-84fd-eb62a834088c",
   "metadata": {},
   "source": [
    "![Transformer Architect Image](./img/transformerblock.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb81cdc-82e3-4705-884e-040fd86664ba",
   "metadata": {},
   "source": [
    "1. Input/Output Pre-processing\n",
    "\n",
    "Token Embeddings\n",
    "- **Function**: Converts each token in the input sequence into a dense vector of fixed size (commonly 512 dimensions).\n",
    "- **Implementation**: An embedding layer maps each token to a dense vector.\n",
    "\n",
    "Positional Encodings\n",
    "- **Function**: Adds information about the position of each token in the sequence to the token embeddings, since the Transformer architecture does not inherently capture sequence order.\n",
    "- **Implementation**: Sinusoidal functions are used to generate positional encodings, which are then added to the token embeddings.\n",
    "\n",
    "2. Encoder\n",
    "\n",
    "Processes the input sequence with a stack of identical layers. Each layer consists of:\n",
    "\n",
    "Multi-Head Self-Attention\n",
    "- **Function**: Allows each token to attend to all other tokens in the sequence, capturing dependencies regardless of their distance in the sequence.\n",
    "- **Implementation**: Multiple attention heads operate in parallel to learn different aspects of the input.\n",
    "\n",
    "Add & Norm\n",
    "- **Function**: Adds the input of each sub-layer to its output (residual connection) and applies layer normalization to stabilize and speed up training.\n",
    "- **Implementation**: Addition followed by normalization.\n",
    "\n",
    "Feed-Forward\n",
    "- **Function**: Applies a fully connected feed-forward network to each position independently and identically.\n",
    "- **Implementation**: Two linear transformations with a ReLU activation in between.\n",
    "\n",
    "3. Decoder\n",
    "\n",
    "Generates the output sequence from the encoded input using a stack of identical layers. Each layer consists of:\n",
    "\n",
    "Masked Multi-Head Self-Attention\n",
    "- **Function**: Prevents attending to future tokens in the sequence during training (autoregressive property).\n",
    "- **Implementation**: Similar to the encoder's self-attention but with a mask to prevent future token attention.\n",
    "\n",
    "Multi-Head Attention over Encoder’s Output\n",
    "- **Function**: Allows each position in the decoder to attend to all positions in the encoder's output.\n",
    "- **Implementation**: Standard multi-head attention mechanism applied to the encoder’s output.\n",
    "\n",
    "Add & Norm\n",
    "- **Function**: Similar to the encoder's Add & Norm, it adds residual connections and normalizes the output.\n",
    "- **Implementation**: Addition followed by normalization.\n",
    "\n",
    "Feed-Forward\n",
    "- **Function**: Similar to the encoder's feed-forward network.\n",
    "- **Implementation**: Two linear transformations with a ReLU activation in between.\n",
    "\n",
    "4. Output Post-processing\n",
    "\n",
    "**Function**\n",
    "- Transforms the decoder’s output into probabilities over the vocabulary.\n",
    "\n",
    "**Implementation**\n",
    "- A linear layer followed by a softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f4159c-646b-4aec-8cff-1b643d350578",
   "metadata": {},
   "source": [
    "## 2. Detailed architecture we need to know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e07bc12-4be4-4563-a951-0c2f8e96dc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afd82f0c-85d0-4b55-a2f2-4bf74cd17e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da606af1-7d77-474b-9be3-efe085e84fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. PyTorch is using GPU.\n",
      "Number of GPUs available:  1\n",
      "GPU name:  NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. PyTorch is using GPU.\")\n",
    "    print(\"Number of GPUs available: \", torch.cuda.device_count())\n",
    "    print(\"GPU name: \", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch is using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a131ee-75f2-4eab-b6f0-9bf0c6152ecc",
   "metadata": {},
   "source": [
    "### 2.1. Input Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e749e28-1ece-4f57-bc04-9b1b92b14c4f",
   "metadata": {},
   "source": [
    "#### 2.1.1. Input Embedding Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04248c1e-0f6d-4587-914e-58bd4d468fde",
   "metadata": {},
   "source": [
    "In this section we will learn about Embedding Layer in Transformer architecture.\n",
    "\n",
    "First, we can imagine the embedding layer as a table used for lookup. The shape of that layer is (Number of words in the vocabulary) x (Number of dimensions we want, usually 512)\n",
    "\n",
    "So, every token (word) will be looked up in that embedding layer and take out the row/column related. The embedding vectors will be ready!\n",
    "\n",
    "Finally, we must know that the value in the embedding layer will be changed in the training progesss.\n",
    "\n",
    "![Input Embedding Block](./img/inputembedding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32470ad9-3c79-48f7-a0b1-9c57ca6a9f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.9993, -0.5038, -0.3130,  ..., -0.5028,  0.2874,  1.5541],\n",
       "        [ 1.1942, -1.0360, -0.0681,  ...,  0.2069,  1.3064,  1.5190],\n",
       "        [ 0.7530, -0.4146, -0.1559,  ..., -0.5897,  0.2449, -1.4349],\n",
       "        ...,\n",
       "        [ 1.8836,  0.9524,  1.3001,  ...,  1.1193, -0.4786,  0.8870],\n",
       "        [-1.7922,  0.1005, -0.9830,  ...,  0.4394, -1.5921, -0.4616],\n",
       "        [-1.1335,  2.0153,  1.2589,  ..., -0.0342,  0.2411, -0.5601]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a embedding layer from torch.nn\n",
    "vocabulary_size = 1000\n",
    "dim_model = 512\n",
    "embedding_layer = nn.Embedding(vocabulary_size,dim_model)\n",
    "embedding_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52949ef7-f2f9-4d11-b111-4f9469e70542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3760, -0.9896,  1.7303,  ...,  1.1147, -0.8789,  0.3700],\n",
       "        [ 1.3608, -1.6796,  0.1257,  ..., -0.6238,  0.5808,  0.9004],\n",
       "        [-0.8332,  0.4980, -2.6389,  ..., -0.6576,  1.4296, -0.0833],\n",
       "        [ 1.0870, -0.2200, -1.0358,  ..., -0.1840, -0.0842,  0.2607]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Up to our setup, we don't need onehot ! we can use dictionary,...\n",
    "sequence = torch.tensor([23,54,28,85])\n",
    "embedding_layer(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8e9d8c-2a4f-4402-acf4-03bd6beeed1c",
   "metadata": {},
   "source": [
    "#### 2.1.2. Positional Encoding Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab692613-5a32-49a4-91f9-60f0f1c61075",
   "metadata": {},
   "source": [
    "Do you get stuck in the question \"What are the differences between the same tokens (words) in the sequence after embedding?\" ?\n",
    "\n",
    "So, Positional Encoding Block is here to help us. This block simply means show the positional feature of each tokens (words) in the sequence.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f6ac51-f913-4144-b783-cac5fd42628b",
   "metadata": {},
   "source": [
    "![Positional Encoding Block](./img/positionalencoding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a29bc7-50bf-4f25-b09b-b3e18d2dd2cb",
   "metadata": {},
   "source": [
    "Recipe:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc31831-92ec-4e61-b878-5cad014908d3",
   "metadata": {},
   "source": [
    "$$\n",
    "PE(\\text{position}, 2i) = \\sin\\bigg( \\frac{ \\text{position} }{10000^\\frac{2i}{d_{model}}} \\bigg)\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE(\\text{position}, 2i+1) = \\cos\\bigg( \\frac{ \\text{position} }{10000^\\frac{2i}{d_{model}}} \\bigg)\n",
    "$$\n",
    "\n",
    "We can rewrite these as\n",
    "\n",
    "$$\n",
    "PE(\\text{position}, i) = \\sin\\bigg( \\frac{ \\text{position} }{10000^\\frac{i}{d_{model}}} \\bigg) \\text{ when i is even}\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE(\\text{position}, i) = \\cos\\bigg( \\frac{ \\text{position} }{10000^\\frac{i-1}{d_{model}}} \\bigg) \\text{ when i is odd}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "05ca41e2-230f-4fc6-9e6d-6b229e8805f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "dim_model = 512\n",
    "sequence_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ac7ea27-6dd3-4593-be4c-9e64b7b6d13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.0366, 1.0746])\n",
      "tensor([1.0000, 1.0366, 1.0746])\n"
     ]
    }
   ],
   "source": [
    "even = torch.arange(0,dim_model,2).float()\n",
    "odd = torch.arange(1,dim_model,2).float()\n",
    "\n",
    "even = torch.pow(10000,even/dim_model)\n",
    "odd = torch.pow(10000,(odd-1)/dim_model)\n",
    "print(even[:3])\n",
    "print(odd[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f8ac2afb-9f44-4768-881a-35e92457acf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "position = torch.arange(0,sequence_length,1).unsqueeze(1)\n",
    "PE = torch.zeros(sequence_length,dim_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a61210aa-836c-4542-b6bf-be0dc4d67730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PE[:,0::2] = torch.sin(po I sition / even)\n",
    "PE[:,1::2] = torch.cos(position / odd)\n",
    "PE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "629f899b-1d78-4177-9223-afd8c8d5c6e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "          0.0000e+00,  1.0000e+00],\n",
       "        [ 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,\n",
       "          1.0366e-04,  1.0000e+00],\n",
       "        [ 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,\n",
       "          2.0733e-04,  1.0000e+00],\n",
       "        ...,\n",
       "        [ 6.5699e-01,  7.5390e-01,  4.5239e-01,  ...,  1.0000e+00,\n",
       "          7.2564e-04,  1.0000e+00],\n",
       "        [ 9.8936e-01, -1.4550e-01,  9.9067e-01,  ...,  1.0000e+00,\n",
       "          8.2931e-04,  1.0000e+00],\n",
       "        [ 4.1212e-01, -9.1113e-01,  6.7637e-01,  ...,  1.0000e+00,\n",
       "          9.3297e-04,  1.0000e+00]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec8e121-a8f8-4d0d-9661-c279989f0171",
   "metadata": {},
   "source": [
    "### 2.2. Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9744736a-ad50-4f8e-97a9-6f70783946aa",
   "metadata": {},
   "source": [
    "Taking the output of Input Pre-processing, we analyze it into Query, Key, Value through a linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5700fde9-d7c7-4b95-b2aa-30e1ccbc8d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length = 4\n",
    "batch_size = 1\n",
    "input_dim = 512\n",
    "d_model = 512\n",
    "x = torch.randn( (batch_size, sequence_length, input_dim) )\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d5fb2a9b-6a63-4be7-b1b5-06d0e692f322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1536])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv_layer = nn.Linear(dim_input , 3 * dim_model)\n",
    "qkv = qkv_layer(x)\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c48ef610-9079-4416-95d5-c78134aee319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'qkv distribution')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y_val = torch.histc(qkv, bins=200, min=-3, max=3)\n",
    "x_val = np.arange(-1, 1, 0.01) * 3\n",
    "plt.bar(x_val, y_val, align='center', color=['forestgreen'])\n",
    "plt.title('qkv distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "238377a8-4f51-4a89-9228-417bd8bc10bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8, 192])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_heads = 8\n",
    "head_dim = d_model // num_heads\n",
    "qkv = qkv.reshape(batch_size, sequence_length, num_heads, 3 * head_dim)\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c76bf8b9-fac0-4b07-95d6-96c05887e7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 192])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv = qkv.permute(0, 2, 1, 3) # [batch_size, num_heads, sequence_length, 3*head_dim]\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7ac1af69-b689-45fc-ab35-08191be757ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q, k, v = qkv.chunk(3, dim=-1)\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d812fae-7159-4d7f-82d0-008eb8d554ac",
   "metadata": {},
   "source": [
    "#### 2.2.1. Multi-Head Attention Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd46e4a-5467-4700-9170-d5d05ca7d1ec",
   "metadata": {},
   "source": [
    "![Encoder](./img/multihead-attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad45180-fe92-47e4-9e6c-f8c255f2344c",
   "metadata": {},
   "source": [
    "With input include:\n",
    "> - Query: What we are looking for\n",
    "> - Key: What we can offer for\n",
    "> - Value: What we actually offer for\n",
    "\n",
    "With output include:\n",
    "> - New Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7dd73ba6-a780-4753-adab-77b5464a83b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need Query, Key, Value For Example\n",
    "sequence_length , k_dim, v_dim = 5 , 10 , 10\n",
    "q = torch.randn(sequence_length, k_dim)\n",
    "k = torch.randn(sequence_length, k_dim)\n",
    "v = torch.randn(sequence_length, v_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba49b6de-2467-444e-9e29-4c334d3d9db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9396, -0.7610, -0.7723,  0.2179,  0.3140, -0.2625, -0.8789,  0.6244,\n",
      "         -0.5245,  1.2776],\n",
      "        [-0.7042,  0.1270,  1.6283, -1.5839,  1.0607, -0.1741, -0.5154,  1.0230,\n",
      "         -0.0220,  1.5466],\n",
      "        [ 2.4340,  3.1458, -1.2979, -1.6841, -0.2796,  1.7375, -0.3538,  0.6227,\n",
      "         -0.9432,  0.9826],\n",
      "        [-0.4292,  0.4479,  0.2516, -0.1197,  0.3054, -0.7416,  1.0121, -1.2222,\n",
      "         -0.2376, -2.1837],\n",
      "        [-0.6403,  0.3252,  0.0717,  0.9909, -0.8243, -0.3065, -0.0571, -0.5463,\n",
      "         -0.5385,  0.3860]])\n",
      "tensor([[ 0.2530,  0.7966, -0.2162, -1.2275, -0.7208,  0.2060,  0.6973,  1.0014,\n",
      "         -0.1323, -0.5996],\n",
      "        [-1.8674,  0.7221,  0.1546, -1.6387, -1.7548,  0.6751, -0.9566,  1.4463,\n",
      "         -1.7166,  0.9057],\n",
      "        [ 0.2133,  0.4005, -0.7649, -1.1516, -0.1740,  0.8551, -0.5812,  0.7501,\n",
      "         -0.3315,  0.2571],\n",
      "        [ 0.3450, -0.1597, -1.0515,  0.2874, -1.3148,  0.5086, -0.1085,  1.4433,\n",
      "         -0.5419, -1.5695],\n",
      "        [ 0.3310, -0.8523,  1.1305,  1.5390, -1.2362, -0.7589, -1.4904,  0.8662,\n",
      "         -0.9653, -1.2031]])\n",
      "tensor([[ 0.3206, -0.8451, -1.4655,  0.0448, -0.7504, -0.4821, -0.3502,  1.9636,\n",
      "          0.7334,  0.6107],\n",
      "        [-0.2962,  0.6164, -0.1401, -1.1631, -0.1552, -1.2128,  0.8274, -1.3171,\n",
      "          0.6409, -1.4501],\n",
      "        [ 0.8960,  0.0777, -0.2443, -1.2703,  0.7731,  0.5421,  0.2034,  0.9009,\n",
      "          0.4646, -1.2535],\n",
      "        [ 0.7610,  1.9847, -0.2595,  0.6914,  0.7838, -0.4053,  1.1034, -1.2059,\n",
      "         -0.2854, -0.1248],\n",
      "        [ 1.2129, -0.0500,  0.6680,  0.4828,  1.4782, -0.2555,  0.2094,  0.3646,\n",
      "          0.4776, -0.5227]])\n"
     ]
    }
   ],
   "source": [
    "print(q)\n",
    "print(k)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43c9607-a401-4dbd-b765-ba406b8d3191",
   "metadata": {},
   "source": [
    "- Scaled Dot-Product Attention:\n",
    "Funtion:\n",
    "\n",
    "$$\n",
    "\\text{self attention} = softmax\\bigg(\\frac{Q.K^T}{\\sqrt{d_k}}+M\\bigg)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{new V} = \\text{self attention}.V\n",
    "$$ \n",
    "\n",
    "Q,K,V -(1)-> Matmul(Q,K.T) -(2)-> Scale -(3)-> Masking (Not Required for Encode) -(4)-> Softmax -(5)-> Matmul (. ,V) -> new V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5799c028-d217-4dc3-a407-dbc8129bd3b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9091,  3.8016,  1.0370, -0.5988,  0.4307],\n",
       "        [ 0.4555,  5.6862,  1.6175, -4.7966, -2.3024],\n",
       "        [ 5.9415,  5.6973,  7.4837,  2.3756, -6.1123],\n",
       "        [ 0.7904, -3.9823, -2.6418,  0.3848,  0.0512],\n",
       "        [-1.3507,  1.5958, -1.4197, -0.2317,  2.0358]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: calculate Q.K_t\n",
    "step1 = torch.matmul(q,k.t())\n",
    "step1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b095284-f9be-4e10-951c-2a874f3d6257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.0842), tensor(0.8988), tensor(11.5111))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Scale\n",
    "# Check variance\n",
    "q.var(), k.var(), step1.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6ee4a7-0f56-405e-86e6-38050edbb6fc",
   "metadata": {},
   "source": [
    "As we see, the variance distance between Q, K, matmul(Q,K.T) is very high. We have to scale again so that the softmax function can work effectively, creating a probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cc5b3a3-f8fb-483b-9f75-42bb93f0b3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6037,  1.2022,  0.3279, -0.1894,  0.1362],\n",
       "        [ 0.1440,  1.7981,  0.5115, -1.5168, -0.7281],\n",
       "        [ 1.8789,  1.8016,  2.3665,  0.7512, -1.9329],\n",
       "        [ 0.2500, -1.2593, -0.8354,  0.1217,  0.0162],\n",
       "        [-0.4271,  0.5046, -0.4489, -0.0733,  0.6438]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step2 = step1 / math.sqrt(k_dim)\n",
    "step2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747a282f-bed5-439c-912a-6888b20ed229",
   "metadata": {},
   "source": [
    "But **why** is **the number of Key dimension** and **why** use **square** ?\n",
    "\n",
    "Dividing by the square root of the dimension of the weight vector (in this case k_dim) has an important meaning.\n",
    "\n",
    "Specifically, in the Multi-Head Attention mechanism, after calculating attention scores, we will apply the softmax function to normalize these scores into a probability distribution. Dividing each score by the square root of k_dim helps control the amplitudes of the scores, preventing them from becoming too large or too small. This improves model stability and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "caad7607-8720-4c9a-bdb6-691dc2a4eb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Masking\n",
    "# This is not required in the encoding block but it is required in the decoding block\n",
    "# Masking is used to make sure that the current word/ token/ ... doesn't take the context from the future / generated word. That is cheating!!!!\n",
    "mask = torch.tril(torch.ones(sequence_length,sequence_length))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3ef63a1-ffb1-43b5-ba98-e63100f2e0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6037,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.1440,  1.7981,  0.0000,  0.0000,  0.0000],\n",
       "        [ 1.8789,  1.8016,  2.3665,  0.0000,  0.0000],\n",
       "        [ 0.2500, -1.2593, -0.8354,  0.1217,  0.0000],\n",
       "        [-0.4271,  0.5046, -0.4489, -0.0733,  0.6438]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mask filter\n",
    "step3 = torch.tril(step2)\n",
    "step3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4930851-c43f-4fd4-b04a-b54e37b5fb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6037,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.1440,  1.7981,    -inf,    -inf,    -inf],\n",
       "        [ 1.8789,  1.8016,  2.3665,    -inf,    -inf],\n",
       "        [ 0.2500, -1.2593, -0.8354,  0.1217,    -inf],\n",
       "        [-0.4271,  0.5046, -0.4489, -0.0733,  0.6438]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step3 = torch.where(step3 == 0, float('-inf'), step3)\n",
    "step3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c46613-8ade-4b71-87c2-6e4781c4fc17",
   "metadata": {},
   "source": [
    "**Why** is '-inf' ?\n",
    "\n",
    "When we take this to the Softmax Function(x), $e^{x}$ will go to value '0' if x goes to '-inf'. Otherwise, The value '0' will result in $e^{x}$ equals 1!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81e53cf-7fe1-4c8b-873c-f956a4f7c84e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{softmax} = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5295fdbf-3d48-40d3-b80f-2efbfd6347e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1606, 0.8394, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2814, 0.2604, 0.4582, 0.0000, 0.0000],\n",
       "        [0.4101, 0.0907, 0.1385, 0.3607, 0.0000],\n",
       "        [0.1129, 0.2866, 0.1104, 0.1608, 0.3294]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Softmax\n",
    "step4 = (torch.exp(step3).t() / torch.sum(torch.exp(step3),axis=-1)).t()\n",
    "step4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b266c3aa-a7b8-49dd-8899-8785895b021c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3206, -0.8451, -1.4655,  0.0448, -0.7504, -0.4821, -0.3502,  1.9636,\n",
       "          0.7334,  0.6107],\n",
       "        [-0.1972,  0.3817, -0.3529, -0.9692, -0.2507, -1.0954,  0.6383, -0.7904,\n",
       "          0.6558, -1.1193],\n",
       "        [ 0.4236, -0.0417, -0.5607, -0.8724,  0.1027, -0.2031,  0.2102,  0.6222,\n",
       "          0.5862, -0.7802],\n",
       "        [ 0.5033,  0.4360, -0.7411, -0.0136,  0.0680, -0.3787,  0.3576,  0.3756,\n",
       "          0.3203, -0.0997],\n",
       "        [ 0.5721,  0.3924, -0.0542, -0.1984,  0.5691, -0.4914,  0.4664, -0.1302,\n",
       "          0.4292, -0.6773]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step5: matmul(.,V)\n",
    "step5 = torch.matmul(step4, v)\n",
    "step5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3581ba-de58-4cc4-8ffb-bf923b107ab1",
   "metadata": {},
   "source": [
    "Then we will have \"concat and linear\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cade4c7-04dd-43a2-a179-aa06165329cb",
   "metadata": {},
   "source": [
    "## 3. Overall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351bf966-0c3c-43d5-9af9-42f0b56aee45",
   "metadata": {},
   "source": [
    "Overall: Other blocks are similar to all upper blocks. So we have a sight in the question \"Why Transformer Neural Networks Model is special and strong\". Like a name \"Attention is all you need!\", We must understand about what's attention ? How's attention ? Why's attention ?. Now we can make our Transformer model base on this architecture and attention mechaism!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f80301-d942-4702-9f07-ab3efa18f9d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TorchCuda",
   "language": "python",
   "name": "torchcuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
