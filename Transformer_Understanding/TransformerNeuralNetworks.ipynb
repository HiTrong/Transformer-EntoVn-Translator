{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bdb8bd9-5949-419a-a6b2-d22990695115",
   "metadata": {},
   "source": [
    "# Transformer Neural Networks - Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c350f7f-2caf-4cbd-8d7d-b16e89caeb63",
   "metadata": {},
   "source": [
    "## 1. Overall Transformer Neural Networks Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea70ff5-e9b7-49ec-b7a3-e8c0de79e6ce",
   "metadata": {},
   "source": [
    "![Transformer Architect Image](./img/transformerarchitect.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0952992-5e25-45ae-af88-db73ba097af4",
   "metadata": {},
   "source": [
    "As we see, the overall architecture of the Transformer is in the upper part. It is probably quite difficult to understand the detailed picture so we need a broader picture. Let's see the next picture!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41818887-f778-49f6-84fd-eb62a834088c",
   "metadata": {},
   "source": [
    "![Transformer Architect Image](./img/transformerblock.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb81cdc-82e3-4705-884e-040fd86664ba",
   "metadata": {},
   "source": [
    "1. Input/Output Pre-processing\n",
    "\n",
    "Token Embeddings\n",
    "- **Function**: Converts each token in the input sequence into a dense vector of fixed size (commonly 512 dimensions).\n",
    "- **Implementation**: An embedding layer maps each token to a dense vector.\n",
    "\n",
    "Positional Encodings\n",
    "- **Function**: Adds information about the position of each token in the sequence to the token embeddings, since the Transformer architecture does not inherently capture sequence order.\n",
    "- **Implementation**: Sinusoidal functions are used to generate positional encodings, which are then added to the token embeddings.\n",
    "\n",
    "2. Encoder\n",
    "\n",
    "Processes the input sequence with a stack of identical layers. Each layer consists of:\n",
    "\n",
    "Multi-Head Self-Attention\n",
    "- **Function**: Allows each token to attend to all other tokens in the sequence, capturing dependencies regardless of their distance in the sequence.\n",
    "- **Implementation**: Multiple attention heads operate in parallel to learn different aspects of the input.\n",
    "\n",
    "Add & Norm\n",
    "- **Function**: Adds the input of each sub-layer to its output (residual connection) and applies layer normalization to stabilize and speed up training.\n",
    "- **Implementation**: Addition followed by normalization.\n",
    "\n",
    "Feed-Forward\n",
    "- **Function**: Applies a fully connected feed-forward network to each position independently and identically.\n",
    "- **Implementation**: Two linear transformations with a ReLU activation in between.\n",
    "\n",
    "3. Decoder\n",
    "\n",
    "Generates the output sequence from the encoded input using a stack of identical layers. Each layer consists of:\n",
    "\n",
    "Masked Multi-Head Self-Attention\n",
    "- **Function**: Prevents attending to future tokens in the sequence during training (autoregressive property).\n",
    "- **Implementation**: Similar to the encoder's self-attention but with a mask to prevent future token attention.\n",
    "\n",
    "Multi-Head Attention over Encoder’s Output\n",
    "- **Function**: Allows each position in the decoder to attend to all positions in the encoder's output.\n",
    "- **Implementation**: Standard multi-head attention mechanism applied to the encoder’s output.\n",
    "\n",
    "Add & Norm\n",
    "- **Function**: Similar to the encoder's Add & Norm, it adds residual connections and normalizes the output.\n",
    "- **Implementation**: Addition followed by normalization.\n",
    "\n",
    "Feed-Forward\n",
    "- **Function**: Similar to the encoder's feed-forward network.\n",
    "- **Implementation**: Two linear transformations with a ReLU activation in between.\n",
    "\n",
    "4. Output Post-processing\n",
    "\n",
    "**Function**\n",
    "- Transforms the decoder’s output into probabilities over the vocabulary.\n",
    "\n",
    "**Implementation**\n",
    "- A linear layer followed by a softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f4159c-646b-4aec-8cff-1b643d350578",
   "metadata": {},
   "source": [
    "## 2. Detailed architecture we need to know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e07bc12-4be4-4563-a951-0c2f8e96dc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afd82f0c-85d0-4b55-a2f2-4bf74cd17e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da606af1-7d77-474b-9be3-efe085e84fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. PyTorch is using GPU.\n",
      "Number of GPUs available:  1\n",
      "GPU name:  NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. PyTorch is using GPU.\")\n",
    "    print(\"Number of GPUs available: \", torch.cuda.device_count())\n",
    "    print(\"GPU name: \", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch is using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a131ee-75f2-4eab-b6f0-9bf0c6152ecc",
   "metadata": {},
   "source": [
    "### 2.1. Token Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04248c1e-0f6d-4587-914e-58bd4d468fde",
   "metadata": {},
   "source": [
    "In this section we will learn about Embedding Layer in Transformer architecture.\n",
    "\n",
    "First, we can imagine the embedding layer as a table / matrix used for lookup. The shape of that layer is (Number of words in the vocabulary) x (Number of dimensions we want, usually 512)\n",
    "\n",
    "So, every token (word) will be looked up in that embedding layer and take out the row/column related. The embedding vectors will be ready!\n",
    "\n",
    "![Input Embedding Block](./img/inputembedding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32470ad9-3c79-48f7-a0b1-9c57ca6a9f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2022,  1.2199, -0.1478,  ...,  2.1072,  1.3513, -0.6855],\n",
       "        [-1.3608, -0.9026,  0.1977,  ...,  1.0848,  0.7755,  1.6727],\n",
       "        [-1.3840, -0.0543, -1.5138,  ..., -1.9281,  0.6493,  2.2330],\n",
       "        ...,\n",
       "        [ 1.3864,  2.2040,  0.9240,  ..., -1.0056,  1.6014,  0.0871],\n",
       "        [ 0.0884,  1.3778, -0.4520,  ...,  0.2092, -1.9162,  0.7261],\n",
       "        [ 1.7428,  0.0964,  0.5249,  ..., -1.7746,  0.4611,  0.8820]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a embedding layer from torch.nn\n",
    "vocabulary_size = 1000\n",
    "dim_model = 512\n",
    "embedding_layer = nn.Embedding(vocabulary_size,dim_model)\n",
    "embedding_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52949ef7-f2f9-4d11-b111-4f9469e70542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3076,  1.4656,  0.4623,  ..., -0.9072,  0.3451, -1.3164],\n",
       "        [ 1.9095,  0.6064, -0.7894,  ..., -0.7032, -1.0532, -0.3142],\n",
       "        [ 0.0540,  0.6348,  0.9248,  ..., -0.6875, -1.5879,  1.2319],\n",
       "        [-0.5922,  0.9335, -0.4050,  ...,  2.0221,  0.3236, -0.3023]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Up to our setup, we don't need onehot ! we can use dictionary,...\n",
    "sequence = torch.tensor([23,54,28,85])\n",
    "embedding_layer(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8e9d8c-2a4f-4402-acf4-03bd6beeed1c",
   "metadata": {},
   "source": [
    "### 2.2. Positional Encoding Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab692613-5a32-49a4-91f9-60f0f1c61075",
   "metadata": {},
   "source": [
    "Do you get stuck in the question \"What are the differences between the same tokens (words) in the sequence after embedding?\" ?\n",
    "\n",
    "So, Positional Encoding Block is here to help us. This block simply means show the positional feature of each tokens (words) in the sequence.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f6ac51-f913-4144-b783-cac5fd42628b",
   "metadata": {},
   "source": [
    "![Positional Encoding Block](./img/positionalencoding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a29bc7-50bf-4f25-b09b-b3e18d2dd2cb",
   "metadata": {},
   "source": [
    "Recipe:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc31831-92ec-4e61-b878-5cad014908d3",
   "metadata": {},
   "source": [
    "$$\n",
    "PE(\\text{position}, 2i) = \\sin\\bigg( \\frac{ \\text{position} }{10000^\\frac{2i}{d_{model}}} \\bigg)\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE(\\text{position}, 2i+1) = \\cos\\bigg( \\frac{ \\text{position} }{10000^\\frac{2i}{d_{model}}} \\bigg)\n",
    "$$\n",
    "\n",
    "We can rewrite these as\n",
    "\n",
    "$$\n",
    "PE(\\text{position}, i) = \\sin\\bigg( \\frac{ \\text{position} }{10000^\\frac{i}{d_{model}}} \\bigg) \\text{ when i is even}\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE(\\text{position}, i) = \\cos\\bigg( \\frac{ \\text{position} }{10000^\\frac{i-1}{d_{model}}} \\bigg) \\text{ when i is odd}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05ca41e2-230f-4fc6-9e6d-6b229e8805f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "dim_model = 512\n",
    "sequence_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ac7ea27-6dd3-4593-be4c-9e64b7b6d13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.0366, 1.0746])\n",
      "tensor([1.0000, 1.0366, 1.0746])\n"
     ]
    }
   ],
   "source": [
    "even = torch.arange(0,dim_model,2).float()\n",
    "odd = torch.arange(1,dim_model,2).float()\n",
    "\n",
    "even = torch.pow(10000,even/dim_model)\n",
    "odd = torch.pow(10000,(odd-1)/dim_model)\n",
    "print(even[:3])\n",
    "print(odd[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8ac2afb-9f44-4768-881a-35e92457acf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "position = torch.arange(0,sequence_length,1).unsqueeze(1)\n",
    "PE = torch.zeros(sequence_length,dim_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a61210aa-836c-4542-b6bf-be0dc4d67730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PE[:,0::2] = torch.sin(position / even)\n",
    "PE[:,1::2] = torch.cos(position / odd)\n",
    "PE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "629f899b-1d78-4177-9223-afd8c8d5c6e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "          0.0000e+00,  1.0000e+00],\n",
       "        [ 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,\n",
       "          1.0366e-04,  1.0000e+00],\n",
       "        [ 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,\n",
       "          2.0733e-04,  1.0000e+00],\n",
       "        ...,\n",
       "        [ 6.5699e-01,  7.5390e-01,  4.5239e-01,  ...,  1.0000e+00,\n",
       "          7.2564e-04,  1.0000e+00],\n",
       "        [ 9.8936e-01, -1.4550e-01,  9.9067e-01,  ...,  1.0000e+00,\n",
       "          8.2931e-04,  1.0000e+00],\n",
       "        [ 4.1212e-01, -9.1113e-01,  6.7637e-01,  ...,  1.0000e+00,\n",
       "          9.3297e-04,  1.0000e+00]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec8e121-a8f8-4d0d-9661-c279989f0171",
   "metadata": {},
   "source": [
    "### 2.3. Query - Key - Value Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9744736a-ad50-4f8e-97a9-6f70783946aa",
   "metadata": {},
   "source": [
    "Taking the output of Input Pre-processing, we analyze it into Query, Key, Value through a linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5700fde9-d7c7-4b95-b2aa-30e1ccbc8d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length = 4\n",
    "batch_size = 1\n",
    "input_dim = 512\n",
    "d_model = 512\n",
    "x = torch.randn( (batch_size, sequence_length, input_dim) )\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5fb2a9b-6a63-4be7-b1b5-06d0e692f322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1536])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv_layer = nn.Linear(input_dim , 3 * dim_model)\n",
    "qkv = qkv_layer(x)\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c48ef610-9079-4416-95d5-c78134aee319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'qkv distribution')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y_val = torch.histc(qkv, bins=200, min=-3, max=3)\n",
    "x_val = np.arange(-1, 1, 0.01) * 3\n",
    "plt.bar(x_val, y_val, align='center', color=['forestgreen'])\n",
    "plt.title('qkv distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "238377a8-4f51-4a89-9228-417bd8bc10bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8, 192])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_heads = 8\n",
    "head_dim = d_model // num_heads\n",
    "qkv = qkv.reshape(batch_size, sequence_length, num_heads, 3 * head_dim)\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c76bf8b9-fac0-4b07-95d6-96c05887e7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 192])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv = qkv.permute(0, 2, 1, 3) # [batch_size, num_heads, sequence_length, 3*head_dim]\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ac1af69-b689-45fc-ab35-08191be757ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q, k, v = qkv.chunk(3, dim=-1)\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d812fae-7159-4d7f-82d0-008eb8d554ac",
   "metadata": {},
   "source": [
    "### 2.4. Multi-Head Attention Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd46e4a-5467-4700-9170-d5d05ca7d1ec",
   "metadata": {},
   "source": [
    "![Encoder](./img/multihead-attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad45180-fe92-47e4-9e6c-f8c255f2344c",
   "metadata": {},
   "source": [
    "With input include:\n",
    "> - Query: What we are looking for\n",
    "> - Key: What we can offer for\n",
    "> - Value: What we actually offer for\n",
    "\n",
    "With output include:\n",
    "> - New Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dd73ba6-a780-4753-adab-77b5464a83b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need Query, Key, Value For Example\n",
    "sequence_length , k_dim, v_dim = 5 , 10 , 10\n",
    "q = torch.randn(sequence_length, k_dim)\n",
    "k = torch.randn(sequence_length, k_dim)\n",
    "v = torch.randn(sequence_length, v_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba49b6de-2467-444e-9e29-4c334d3d9db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5064, -0.4902, -0.6980, -0.4185,  0.0739, -0.4198, -0.0824, -1.0631,\n",
      "          0.0575,  0.4002],\n",
      "        [-0.3253,  1.1266,  0.7962,  2.1140, -0.5149,  0.4325, -0.0416,  0.2087,\n",
      "          0.3154,  1.2865],\n",
      "        [-0.1362, -0.1188, -0.4631,  0.3204,  0.2993,  1.2721,  0.3294, -0.6380,\n",
      "          0.1875, -1.5246],\n",
      "        [ 1.4102,  0.5357, -0.1897, -0.4136,  0.0431, -0.3190, -1.2430,  1.2546,\n",
      "         -0.8923,  0.1642],\n",
      "        [-0.2641,  0.5732, -0.1440,  0.6158, -0.7353,  0.5017,  2.7669, -0.2698,\n",
      "          0.8153, -0.2693]])\n",
      "tensor([[-0.3841,  1.8605, -1.7380, -1.0521, -0.5153,  0.6162, -0.1517, -0.1560,\n",
      "         -0.8213,  0.0636],\n",
      "        [-1.5229, -0.1396, -0.3872, -1.1422, -0.4812, -0.4328,  1.4964,  0.4845,\n",
      "          0.3186, -0.4217],\n",
      "        [ 0.4892,  0.3005,  0.1196,  0.1825,  1.3426, -0.5377, -2.2387, -0.4805,\n",
      "          0.0227,  0.3974],\n",
      "        [ 0.1511,  1.0707,  0.5243, -1.1429,  0.1596,  1.6643, -0.3113, -0.3759,\n",
      "         -0.4656,  0.2899],\n",
      "        [ 1.2737, -0.1430, -0.2879,  0.3581, -1.7662, -0.3135, -1.6861,  0.2601,\n",
      "          0.5301,  2.5334]])\n",
      "tensor([[ 1.7809, -1.1259, -0.1832,  1.7547,  0.0050,  1.0843, -1.7662,  2.0142,\n",
      "         -0.5149,  1.5511],\n",
      "        [ 0.0213, -0.1088,  0.1750, -0.7588, -1.3194,  0.7086, -1.5357,  1.1908,\n",
      "          0.4789,  0.0224],\n",
      "        [ 1.7790,  1.7773, -1.1700,  1.1661, -1.7547, -1.2227, -1.5281,  0.1350,\n",
      "         -2.0840, -0.5590],\n",
      "        [ 0.1320,  0.1722,  1.8997,  1.4644,  0.1026, -1.0546, -0.6599, -1.0193,\n",
      "         -0.5723,  0.7746],\n",
      "        [-1.6321,  1.0027, -0.3193,  0.5708,  0.5788, -0.4566, -1.4302,  0.7409,\n",
      "          0.0941, -0.7567]])\n"
     ]
    }
   ],
   "source": [
    "print(q)\n",
    "print(k)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43c9607-a401-4dbd-b765-ba406b8d3191",
   "metadata": {},
   "source": [
    "- Scaled Dot-Product Attention:\n",
    "Funtion:\n",
    "\n",
    "$$\n",
    "\\text{self attention} = softmax\\bigg(\\frac{Q.K^T}{\\sqrt{d_k}}+M\\bigg)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{new V} = \\text{self attention}.V\n",
    "$$ \n",
    "\n",
    "Q,K,V -(1)-> Matmul(Q,K.T) -(2)-> Scale -(3)-> Masking (Not Required for Encode) -(4)-> Softmax -(5)-> Matmul (. ,V) -> new V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5799c028-d217-4dc3-a407-dbc8129bd3b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1798,  2.4682,  0.1363, -0.8125, -0.8899],\n",
       "        [-1.0584, -2.7273,  0.2479, -0.0434,  4.2770],\n",
       "        [ 0.7274,  0.2293, -1.4139,  1.0161, -5.3205],\n",
       "        [ 1.7370, -3.1650,  3.2069,  1.0142,  4.0153],\n",
       "        [ 0.3941,  4.1943, -7.2719, -0.7053, -4.0004]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: calculate Q.K_t\n",
    "step1 = torch.matmul(q,k.t())\n",
    "step1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b095284-f9be-4e10-951c-2a874f3d6257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.7032), tensor(0.9097), tensor(8.1302))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Scale\n",
    "# Check variance\n",
    "q.var(), k.var(), step1.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6ee4a7-0f56-405e-86e6-38050edbb6fc",
   "metadata": {},
   "source": [
    "As we see, the variance distance between Q, K, matmul(Q,K.T) is very high. We have to scale again so that the softmax function can work effectively, creating a probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cc5b3a3-f8fb-483b-9f75-42bb93f0b3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3731,  0.7805,  0.0431, -0.2569, -0.2814],\n",
       "        [-0.3347, -0.8624,  0.0784, -0.0137,  1.3525],\n",
       "        [ 0.2300,  0.0725, -0.4471,  0.3213, -1.6825],\n",
       "        [ 0.5493, -1.0008,  1.0141,  0.3207,  1.2698],\n",
       "        [ 0.1246,  1.3264, -2.2996, -0.2230, -1.2650]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step2 = step1 / math.sqrt(k_dim)\n",
    "step2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747a282f-bed5-439c-912a-6888b20ed229",
   "metadata": {},
   "source": [
    "But **why** is **the number of Key dimension** and **why** use **square** ?\n",
    "\n",
    "Dividing by the square root of the dimension of the weight vector (in this case k_dim) has an important meaning.\n",
    "\n",
    "Specifically, in the Multi-Head Attention mechanism, after calculating attention scores, we will apply the softmax function to normalize these scores into a probability distribution. Dividing each score by the square root of k_dim helps control the amplitudes of the scores, preventing them from becoming too large or too small. This improves model stability and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "caad7607-8720-4c9a-bdb6-691dc2a4eb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Masking\n",
    "# This is not required in the encoding block but it is required in the decoding block\n",
    "# Masking is used to make sure that the current word/ token/ ... doesn't take the context from the future / generated word. That is cheating!!!!\n",
    "mask = torch.tril(torch.ones(sequence_length,sequence_length))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3ef63a1-ffb1-43b5-ba98-e63100f2e0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3731,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.3347, -0.8624,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.2300,  0.0725, -0.4471,  0.0000,  0.0000],\n",
       "        [ 0.5493, -1.0008,  1.0141,  0.3207,  0.0000],\n",
       "        [ 0.1246,  1.3264, -2.2996, -0.2230, -1.2650]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mask filter\n",
    "step3 = torch.tril(step2)\n",
    "step3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4930851-c43f-4fd4-b04a-b54e37b5fb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3731,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-0.3347, -0.8624,    -inf,    -inf,    -inf],\n",
       "        [ 0.2300,  0.0725, -0.4471,    -inf,    -inf],\n",
       "        [ 0.5493, -1.0008,  1.0141,  0.3207,    -inf],\n",
       "        [ 0.1246,  1.3264, -2.2996, -0.2230, -1.2650]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step3 = torch.where(step3 == 0, float('-inf'), step3)\n",
    "step3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c46613-8ade-4b71-87c2-6e4781c4fc17",
   "metadata": {},
   "source": [
    "**Why** is '-inf' ?\n",
    "\n",
    "When we take this to the Softmax Function(x), $e^{x}$ will go to value '0' if x goes to '-inf'. Otherwise, The value '0' will result in $e^{x}$ equals 1!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81e53cf-7fe1-4c8b-873c-f956a4f7c84e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{softmax} = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5295fdbf-3d48-40d3-b80f-2efbfd6347e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.6290, 0.3710, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4233, 0.3616, 0.2151, 0.0000, 0.0000],\n",
       "        [0.2778, 0.0590, 0.4422, 0.2210, 0.0000],\n",
       "        [0.1862, 0.6194, 0.0165, 0.1315, 0.0464]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Softmax\n",
    "step4 = (torch.exp(step3).t() / torch.sum(torch.exp(step3),axis=-1)).t()\n",
    "step4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b266c3aa-a7b8-49dd-8899-8785895b021c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7809, -1.1259, -0.1832,  1.7547,  0.0050,  1.0843, -1.7662,  2.0142,\n",
       "         -0.5149,  1.5511],\n",
       "        [ 1.1280, -0.7485, -0.0503,  0.8221, -0.4864,  0.9449, -1.6807,  1.7087,\n",
       "         -0.1462,  0.9839],\n",
       "        [ 1.1442, -0.1337, -0.2659,  0.7192, -0.8524,  0.4523, -1.6316,  1.3123,\n",
       "         -0.4930,  0.5445],\n",
       "        [ 1.3118,  0.5048, -0.1380,  1.2820, -0.8296, -0.4308, -1.4028,  0.4642,\n",
       "         -1.1628,  0.3562],\n",
       "        [ 0.3158, -0.1786,  0.2900,  0.0951, -0.8048,  0.4607, -1.4584,  1.0151,\n",
       "          0.0954,  0.3603]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step5: matmul(.,V)\n",
    "step5 = torch.matmul(step4, v)\n",
    "step5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d60ecf-297b-4100-95d0-0817fbf88796",
   "metadata": {},
   "source": [
    "### 2.5. Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dca0db1-9996-4827-a48e-18aea64aa71b",
   "metadata": {},
   "source": [
    "![layernormalization](./img/layernormalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa68b481-9eaa-4dd8-9f0b-97b7b4896ca1",
   "metadata": {},
   "source": [
    "Because the values in vectors have a wide both positive and negative range, the normalization will capture these values to a smaller range (around 0) for a stable training, preventing vanishing descent, higher speed. But why does Transformer use **Layer Normalization** instead of **Batch Normalization**?\n",
    "\n",
    "Layer Normalization is preferred in Transformers because it better suits the sequential and variable-length nature of the input data, maintains the independence of token processing, handles smaller batch sizes more effectively, and has been empirically shown to improve model performance and stability in NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb104ca-09d5-4755-b0b2-6a5204151017",
   "metadata": {},
   "source": [
    "Given an input vector $( x = (x_1, x_2, \\ldots, x_d))$:\n",
    "\n",
    "1. **Compute the mean**:\n",
    "   $$\n",
    "   \\mu = \\frac{1}{d} \\sum_{i=1}^d x_i\n",
    "   $$\n",
    "\n",
    "2. **Compute the variance**:\n",
    "   $$\n",
    "   \\sigma^2 = \\frac{1}{d} \\sum_{i=1}^d (x_i - \\mu)^2\n",
    "   $$\n",
    "\n",
    "3. **Normalize the input**: where $(\\epsilon)$ is a small constant to avoid division by zero.\n",
    "\n",
    "   $$\n",
    "   \\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "   $$\n",
    "\n",
    "4. **Scale and shift** (optional, learnable parameters): where $(\\gamma)$ and $(\\beta)$ are learnable parameters (scale and shift).\n",
    "   $$\n",
    "   y_i = \\gamma \\hat{x}_i + \\beta\n",
    "   $$\n",
    "\n",
    "Final output:\n",
    "   $$\n",
    "   y = \\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e2fbe40-c772-4792-880b-e04eefbe3693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.Tensor([[[0.2, 0.1, 0.3], [0.5, 0.1, 0.1]]])\n",
    "B, S, E = inputs.size() # Batch Seq Embedding\n",
    "inputs = inputs.reshape(S, B, E)\n",
    "inputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "147b1569-7090-47ea-aba5-c27edf8a9405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3]), torch.Size([1, 3]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter_shape = inputs.size()[-2:]\n",
    "gamma = nn.Parameter(torch.ones(parameter_shape))\n",
    "beta =  nn.Parameter(torch.zeros(parameter_shape))\n",
    "gamma.size(), beta.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08dbf610-c671-4fad-925f-12375d26f244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -2]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims = [-(i + 1) for i in range(len(parameter_shape))]\n",
    "dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d25e98f7-3bab-41a4-aa13-d91f5fd8941f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = inputs.mean(dim=dims, keepdim=True)\n",
    "mean.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d59501d0-c1f0-46ab-9155-56160cc4a467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2000]],\n",
       "\n",
       "        [[0.2333]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff37cf87-3121-4975-b383-0a682ee6e82f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0817]],\n",
       "\n",
       "        [[0.1886]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
    "epsilon = 1e-5\n",
    "std = (var + epsilon).sqrt()\n",
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95415d91-55e3-43d4-8484-caf690948891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000, -1.2238,  1.2238]],\n",
       "\n",
       "        [[ 1.4140, -0.7070, -0.7070]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = (inputs - mean) / std\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f85e1d1-0d47-4e6a-a212-3268fc4dc1e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000, -1.2238,  1.2238]],\n",
       "\n",
       "        [[ 1.4140, -0.7070, -0.7070]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = gamma * y + beta\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cade4c7-04dd-43a2-a179-aa06165329cb",
   "metadata": {},
   "source": [
    "## 3. Overall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351bf966-0c3c-43d5-9af9-42f0b56aee45",
   "metadata": {},
   "source": [
    "Overall: Other blocks are similar to all upper blocks. So we have a sight in the question \"Why Transformer Neural Networks Model is special and strong\". Like a name \"Attention is all you need!\", We must understand about what's attention ? How's attention ? Why's attention ?. Now we can make our Transformer model base on this architecture and attention mechaism!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cb6271-384e-47e4-95cb-99e13ef9dc31",
   "metadata": {},
   "source": [
    "## 4. References\n",
    "- Thanks for Ajay Halthor - [@CodeEmporium](https://www.youtube.com/@CodeEmporium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3496506-6d55-4fa8-ab24-be912fe6c5a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TorchCuda",
   "language": "python",
   "name": "torchcuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
