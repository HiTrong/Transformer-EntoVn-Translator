{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7937cc48-41a4-4b3f-95ed-7b21950dba72",
   "metadata": {},
   "source": [
    "# Transformer Model - Translator English to Vietnamese - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b9f4e4-e01a-4621-96a8-dd35ae913f69",
   "metadata": {},
   "source": [
    "- Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "946e505c-32cf-46b5-9a68-e227ad319776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. PyTorch is using GPU.\n",
      "Number of GPUs available:  1\n",
      "GPU name:  NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from datasets import load_dataset\n",
    "from MyTransformer import Transformer\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b90561-0481-44e2-94b5-3c9cdb83a1c4",
   "metadata": {},
   "source": [
    "- Load Dataset from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f325ccfe-cf63-4512-b74e-49cbb7e39a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 992248\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"kaitchup/opus-Vietnamese-to-English\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5beda784-b857-49dd-a690-ee2a2e5710fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cái gì đó? ###>What is it?',\n",
       " \"Con nghĩ chúng ta nên đến mái ấm. ###>I thought we would go to the children's home.\",\n",
       " 'Có điều gì cô muốn nói với chồng mình không? ###>Is there something you want to tell your husband?',\n",
       " 'Thầy của ngươi muốn săn chúng ta, thiêu chúng ta, ăn tim chúng ta. ###>Your master wants to hunt us, burn us, eat our hearts.',\n",
       " 'Haylàkẻ yếuđuối? ###>Or too weak to see this through?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['text'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6550a417-2e5e-4e8f-8d17-fe87412d3f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(992248, 992248)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_train = list(map(lambda x: x.split('###>'), dataset['train']['text']))\n",
    "vietnam_sentences_train = list(map(lambda x : x[0], sentences_train))\n",
    "english_sentences_train = list(map(lambda x : x[1], sentences_train))\n",
    "len(vietnam_sentences_train), len(english_sentences_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0bd0429-4362-4eef-8542-a7db41ff27db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_valid = list(map(lambda x: x.split('###>'), dataset['validation']['text']))\n",
    "vietnam_sentences_valid = list(map(lambda x : x[0], sentences_valid))\n",
    "english_sentences_valid = list(map(lambda x : x[1], sentences_valid))\n",
    "len(vietnam_sentences_valid), len(english_sentences_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "252e9b40-4b2d-47f9-9b34-42456d83c7c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anh cũng làm việc cho hắn ta? ',\n",
       " 'Xin lỡi, hôm nay tôi thấy khó chịu Tối qua tôi đã gặp ác mộng ',\n",
       " 'Em không cho mụ vinh hạnh đó đâu. ',\n",
       " '- Bỏ nó vào túi. ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vietnam_sentences_valid[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a79b280-5009-4a17-86ee-daac8ab23d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You can act as him, too?',\n",
       " \"I'm sorry. I am nervous today. I had bad dreams.\",\n",
       " \"I wouldn't give her that pleasure. It's up to you.\",\n",
       " '- Leave that in this bag.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences_valid[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a015d7-31be-4e54-8443-170aaac549ab",
   "metadata": {},
   "source": [
    "- Setup vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "254b2400-b2f0-4c4a-bd6c-a15cd8e69cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = '<start>'\n",
    "PADDING_TOKEN = '<pad>'\n",
    "END_TOKEN = '<end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22b08de4-058c-48df-8cb0-58ab61ef81b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vietnamese_characters = [ ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', \n",
    "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', '?', 'ˌ',\n",
    "    'a', 'á', 'à', 'ả', 'ã', 'ạ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ',\n",
    "    'b', 'c', 'd', 'đ', 'e', 'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ', \n",
    "    'g', 'h', 'i', 'í', 'ì', 'ỉ', 'ĩ', 'ị', 'k', 'l', 'm', 'n', 'o', 'ó', 'ò', 'ỏ', 'õ', 'ọ', \n",
    "    'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'p', 'q', 'r', 's', 't', 'u', \n",
    "    'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'v', 'x', 'y', 'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ','z','w','f','j'\n",
    "]\n",
    "\n",
    "vietnamese_vocabulary = [START_TOKEN] + vietnamese_characters + [char.upper() for char in vietnamese_characters] + [PADDING_TOKEN, END_TOKEN]\n",
    "len(vietnamese_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21d1180f-dbdb-4fb1-bd10-522455479dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_vocabulary = [ START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', \n",
    "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', '?', 'ˌ',\n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', \n",
    "    'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', \n",
    "    'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "                      PADDING_TOKEN, END_TOKEN\n",
    "]\n",
    "len(english_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd21f039-db3e-4ba2-a463-9a26e720f038",
   "metadata": {},
   "source": [
    "- Check vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05880fc7-449f-4323-b185-1abcd8407662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Check_character(sentences,vocabulary):\n",
    "    missing_character = []\n",
    "    amount_sentences = 0\n",
    "    for sentence in sentences:\n",
    "        check = False\n",
    "        for c in list(set(sentence)):\n",
    "            if c not in vocabulary and c not in missing_character:\n",
    "                missing_character.append(c)\n",
    "                check = True\n",
    "        if check:\n",
    "            amount_sentences += 1\n",
    "    if len(missing_character) == 0:\n",
    "        print(\"Suitable vocabulary!\")\n",
    "        return None\n",
    "    print(f\"Find {missing_character} in vocabulary!\")\n",
    "    return amount_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb766a8a-32a3-4ca9-9270-f7a1287bcbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find ['♫', '̀', '́', '̉', '♪', '̣', '^', '̃', '}', '\\\\', '{', '«', '»', '́', '̀', '£', '–', 'ð', ';', '@', '[', ']', 'Μ', '\\xad', '°', '¡', '×', '³', '§', '\\x8b', '\\x81', '´', 'ª', 'º', '\\x99', 'Æ', '\\xa0', '½', 'Ð', '_', 'Û', 'ß', '校', '王', '長', '¶', '¢', 'Ü', '隊', '他', '甩', '守', '開', '防', '員', '的', '了', '手', '電', '以', '還', '拍', '影', '星', '可', '做', '歌', '明', 'ō', '嘛', '就', '是', '不', '們', '對', '傑', '咱', '事', '阿', 'ü', '¹', 'Ñ', 'γ', '’', 'Ë', 'ï', '≤', 'Ä', '\\x91', 'ñ', '¯', 'ο', 'ë', 'ä', 'λ', 'ç', '」', '「', '©', 'Ç', '~', 'Þ', 'Η', '®', '合', '照', '我', '跟', '嗎', '¿', '叫', '裡', '在', '夫', '學', '武', '功', '那', '—', '，', '振', '非', '李', '格', '赵', '铎', 'ö', '吧', '噯', '沒', 'Α', 'Ε', 'Τ', '\\x9f', 'ī', 'ħ', 'ā', '走', '·', '永', '江', '湖', '退', '啊', '出', 'Ѕ', 'ѕ', 'х', 'і', '你', '有', '沖', '快', '\\u202d', '生', '年', '敗', '失', '當', '意', '加', '嵐', '油', '愛', '蕭', '¥', '国', '军', '庆', '谈', '贤', '阀', '陈', '民', '派', '系', 'å', 'µ', '天', '久', '原', '神', '拳', '一', '下', '都', '來', '第', '前', '很', '父', '師', '\\x87', '\\x90', '\\u200b', '好', '喔', '這', '熟', '字', '≥', 'Ş', '決', '定', '關', '鍵', '刻', '時', '요', '름', '뭐', '예', '이', '±', '\\x83', '打', '擋', '鐵', '得', '方', '撐', '回', '頭', '水', '傘', '把', '紙', '起', '誰', '望', '見', '樣', '紅', '多', '子', 'ﬂ', '\\x8d', '¬', 'Ï', '算', '物', '後', '禮', '最', '件', '幹', '\\u200c', '大', '結', '團', '家', '而', 'е', 'о', '說', '聽', 'Ö', '》', '！', '哇', '²', 'ֶ', '辭', '什', '容', '義', '麼', '思', '本', '柯', '比', '森', '強', '丹', '喬', '燒', '抖', '夥', '火', '顫', '熊', '讓', '戰', '兒', '燃', 'ń', 'ğ', 'ı', '東', '要', '西', '交', '給', '障', '心', '衝', '礙', '哥', '理', '過', '個', '筆', '？', 'с', 'С', '看', '帶', 'Ō', '亮', '挺', '漂', 'š', 'ć', 'Ш', 'Х', '渾', '淌', '賤', '怎', '拋', '棄', '為', '去', '冰', '才', '淇', '淋', '吃', '候', '坐', '\\x8f', '能', '謝', '憶', '老', '薑', '截', '幕', '光', '散', '飛', '段', '舊', '月', 'Å', '鬥', '人', '\\x89', '離', '球', '賽', '別', 'ø', '切', '標', '磋', '將', '重', '再', '組', '分', '又', 'æ', '荒', '畫', '面', '涼', '變', '¨', '像', '勁', '會', '何', '機', '任', '葬', '場', '同', '陪', '到', '簽', '\\x94', '媽', '婆', '†', 'œ', '`', '作', '小', '友', '朋', '動', '四', '話', '三', '推', '阻', '→', 'ʾ', 'ḥ', '÷', '勝', '利', '冠', '成', '\\x96', '⊲', '實', '也', '女', '其', 'û', '歹', '輕', '識', '點', '相', '全', '安', 'ę', '始', '差', '技', '賺', '錢', '等', '想', '張', '名', '拿', '杯', '乾', '情', '仇', '恩', '怨', '啦', '找', '爹', '娘', '‘', '搞', '忘', '五', '百', '萬', '♥', '™', 'ė', '囉', '哈', '較', '適', 'ţ', 'ş', 'ν', '喂', '及', '法', '涉', '勾', '德', '道', '只', '違', '€', '₫', '資', '參', 'ы', 'й', 'В', 'Б', 'в', 'т', 'р', 'н', '¸', '‐', '跳', '㮥', '᧯', '班', '接', '☺', 'ℱ', 'ℬ', 'Ω', 'ℒ', '真', '掛', '住', '頂', '←', '嬲', '哪', '底', 'İ', '留', '甲', '片', '符', '絕', '地', '耶', '莉', '於', '至', '呢', '破', '吹', '彈', '試', '肌', '練', '膚', '中', '空', '傳', '攔', '世', '\\x9b', '穿', '悶', '裝', '慌', '倦', '上', '集', '釋', '文', '骨', '\\u200e', '碰', '鬼', '代', '言', '幾', '千', '豪', '畢', '瞞', '聰', '信', '己', '另', '外', '經', 'О', '親', '尋', '籃', 'з', 'Э', 'К', 'у', 'и', 'к', 'м', 'И', 'ц', 'а', 'П', 'Î', '克', '佐', '木', '々', '\\x95', '\\x93', '感', '覺', '視', '腳', '稍', '微', '伸', '展', '灌', '幫', '氣', '魄', '運', '間', '構', '由', '丟', '鉛', '竿', 'Ţ', '風', '竹', '羅', '倫', '敦', '送', '需', '衣', '服', '貴', 'ū', '批', '體', '吸', '引', '媒', '次', '靜', '記', 'î', '號', '涯', '句', '\\x9d', '豆', '腐', '免', 'Ζ', '改', '旅', '太', '行', '問', '請', '位', '毒', '報', '管', '”', '“', '值', '提', 'Ν', 'Ο', '億', '\\x97', '知', '新', '選', '実', 'の', '士', '搖', '半', '騙', '樹', '哄', '莫', '福', '源', '歴', '史', '部', '群', '編', '但', '馬', '暗', '輸', '投', '亂', '呼', '招', '語', '漢', '典', '委', '•', '¼', '份', '響', '身', '贏', '„', '采', '精', '無', '遮', '該', '霜', '屏', '尤', '品', '極', '楚', '清', 'ł', '疆', '邊', '滾', '剩', '秒', '谱', '钮', '禄', '祜', 'Υ', '死', '扎', '掙', '垂', 'δ', '然', 'ч', 'ь', '帥', 'ą'] in vocabulary!\n",
      "Find [']', '[', '♪', '£', '_', 'É', 'ü', 'ο', 'é', 'ā', '±', '¡', '¯', '–', ';', '@', '~', 'à', 'ñ', '—', '´', '`', '\\xa0', '\\x94', 'Β', 'ö', '}', '\\\\', '{', '\\u200e', 'ō', 'á', 'Μ', '’', '\\x9d', 'Ó', '¢', 'í', 'Α', '²', 'À', '¶', 'è', '¿', 'š', 'Ã', '、', '─', 'Ü', 'Τ', 'Υ', 'ó', 'ã', 'ä', 'ô', '♫', '振', '非', '李', '格', '赵', '铎', 'Η', 'ī', 'ħ', 'ﬂ', '\\ue0e1', '×', 'â', 'æ', '¼', 'Æ', 'ð', '§', 'Κ', '\\u202d', '国', '军', '庆', '谈', '贤', '阀', '陈', '民', '派', '系', '™', 'ë', 'Ε', '^', 'о', 'ν', 'ư', 'ợ', 'ự', 'ì', 'ù', 'º', 'τ', 'ç', 'Ş', 'с', 'і', 'ѕ', 'а', 'е', 'ï', '요', '름', '뭐', '예', '이', '\\x92', '\\x8b', 'ê', '《', '》', 'ﬁ', 'ú', '\\u200b', 'Ö', 'ế', 'ể', 'î', 'ª', 'Е', '\\u202f', '¤', '½', 'È', 'İ', 'ń', '¾', 'ı', 'ğ', 'û', 'Ç', '°', '΄', '·', 'Ν', 'Ō', 'ć', 'Ш', 'Х', 'Á', 'Â', '\\x83', '・', '駛', '“', '”', 'й', 'ầ', 'ø', 'ò', '÷', 'å', 'Ÿ', '€', 'Ä', 'Ë', 'µ', 'Î', 'Ð', 'Ò', '¨', '®', '†', 'ạ', 'ʾ', 'ḥ', '\\x96', '\\x81', 'ę', '‘', '¥', 'ė', '‚', '¬', 'ă', 'ţ', 'ş', '\\u2009', 'ы', 'В', 'Б', 'в', 'т', 'р', 'н', 'х', '‐', '©', 'ý', 'Г', 'œ', '♡', 'Í', '�', 'þ', '集', '甲', '釋', '文', '字', '骨', '¦', 'Ï', 'з', 'Э', 'К', '«', '»', 'у', 'и', 'к', 'м', 'И', 'ц', 'П', '克', '佐', '木', '々', 'ι', 'ь', '＃', 'Å', 'ū', 'Ţ', '風', '竹', '羅', '￡', 'ß', '\\u3000', '慹', 'Ā', 'Ι', '新', '後', '選', '組', '実', '最', 'の', '武', '士', '像', 'õ', '莫', '福', 'ữ', '源', '歴', '史', '部', '群', '編', '\\x90', '\\x9e', '\\x93', '¹', '语', '汉', '大', '会', '典', '员', '委', '„', 'ł', '谱', '钮', '禄', '家', '祜', 'ÿ', 'ą'] in vocabulary!\n"
     ]
    }
   ],
   "source": [
    "vietnam_wrong_sentences = Check_character(vietnam_sentences_train,vietnamese_vocabulary)\n",
    "english_wrong_sentences = Check_character(english_sentences_train,english_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162e6919-0cb9-4389-9acb-4582e4182ab1",
   "metadata": {},
   "source": [
    "Lots of characters like symbols, words in other languages. So we will try to remove all sentences which have unknown characters. If the amount of removed sentences are not so many, we can apply this. If so many sentences are removed, we should appy another ways like adding tag 'unknown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10dbac59-63eb-4eec-8b0c-b453df8f91a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sentences: 992248 (vietnam), 992248 (english)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vietnam_wrong_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain sentences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(vietnam_sentences_train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (vietnam), \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(english_sentences_train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (english)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwrong train sentences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mvietnam_wrong_sentences\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (vietnam), \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menglish_wrong_sentences\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (english)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vietnam_wrong_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "print(f'Train sentences: {len(vietnam_sentences_train)} (vietnam), {len(english_sentences_train)} (english)')\n",
    "print(f'wrong train sentences: {vietnam_wrong_sentences} (vietnam), {english_wrong_sentences} (english)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780adb4a-e0ef-4f2d-a1b0-5b0067db41c1",
   "metadata": {},
   "source": [
    "The number of removed sentences is much smaller than the total number of sentences so we can remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e61f3d6-e3e1-4acc-904c-1d29d5992aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_sentence(sentence,vocabulary):\n",
    "    for c in list(set(sentence)):\n",
    "        if c not in vocabulary:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8187d85-bd25-4894-8863-bc123c445612",
   "metadata": {},
   "outputs": [],
   "source": [
    "vn_temp = []\n",
    "eng_temp = []\n",
    "for i in range(0,len(vietnam_sentences_train)):\n",
    "    if is_valid_sentence(vietnam_sentences_train[i], vietnamese_vocabulary) and is_valid_sentence(english_sentences_train[i], english_vocabulary):\n",
    "        vn_temp.append(vietnam_sentences_train[i])\n",
    "        eng_temp.append(english_sentences_train[i])\n",
    "vietnam_sentences_train = vn_temp\n",
    "english_sentences_train = eng_temp\n",
    "\n",
    "vn_temp = []\n",
    "eng_temp = []\n",
    "for i in range(0,len(vietnam_sentences_valid)):\n",
    "    if is_valid_sentence(vietnam_sentences_valid[i], vietnamese_vocabulary) and is_valid_sentence(english_sentences_valid[i], english_vocabulary):\n",
    "        vn_temp.append(vietnam_sentences_valid[i])\n",
    "        eng_temp.append(english_sentences_valid[i])\n",
    "vietnam_sentences_valid = vn_temp\n",
    "english_sentences_valid = eng_temp\n",
    "\n",
    "\n",
    "# vietnam_wrong_sentences = Check_character(vietnam_sentences_train,vietnamese_vocabulary)\n",
    "# english_wrong_sentences = Check_character(english_sentences_train,english_vocabulary)\n",
    "# vietnam_wrong_sentences = Check_character(vietnam_sentences_valid,vietnamese_vocabulary)\n",
    "# english_wrong_sentences = Check_character(english_sentences_valid,english_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57527974-e618-446b-9caf-2ce057b0c7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_vietnamese = {k:v for k,v in enumerate(vietnamese_vocabulary)}\n",
    "vietnamese_to_index = {v:k for k,v in enumerate(vietnamese_vocabulary)}\n",
    "index_to_english = {k:v for k,v in enumerate(english_vocabulary)}\n",
    "english_to_index = {v:k for k,v in enumerate(english_vocabulary)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c410bc7-9b1f-4a15-ba29-30c36430ca5f",
   "metadata": {},
   "source": [
    "- Check Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cdb1ebb-c019-4519-8a2b-36414a20507a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame({\n",
    "    'vietnamese_train_length': [len(sentence) for sentence in vietnam_sentences_train],\n",
    "    'english_train_length': [len(sentence) for sentence in english_sentences_train],\n",
    "})\n",
    "\n",
    "df_valid = pd.DataFrame({\n",
    "    'vietnamese_valid_length': [len(sentence) for sentence in vietnam_sentences_valid],\n",
    "    'english_valid_length': [len(sentence) for sentence in english_sentences_valid],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45035cf8-85dc-4726-b55d-533ee3e94283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vietnamese_train_length</th>\n",
       "      <th>english_train_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>952120.000000</td>\n",
       "      <td>952120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>32.334417</td>\n",
       "      <td>30.988058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>21.854748</td>\n",
       "      <td>22.082578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>42.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>274.000000</td>\n",
       "      <td>416.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       vietnamese_train_length  english_train_length\n",
       "count            952120.000000         952120.000000\n",
       "mean                 32.334417             30.988058\n",
       "std                  21.854748             22.082578\n",
       "min                   2.000000              1.000000\n",
       "25%                  17.000000             15.000000\n",
       "50%                  27.000000             26.000000\n",
       "75%                  42.000000             40.000000\n",
       "max                 274.000000            416.000000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "131b9c4a-4f93-4682-afc3-2a7990d9458e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vietnamese_valid_length</th>\n",
       "      <th>english_valid_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1890.000000</td>\n",
       "      <td>1890.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>39.059788</td>\n",
       "      <td>39.02381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>26.224710</td>\n",
       "      <td>26.75412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>22.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>33.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>49.000000</td>\n",
       "      <td>49.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>190.000000</td>\n",
       "      <td>188.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       vietnamese_valid_length  english_valid_length\n",
       "count              1890.000000            1890.00000\n",
       "mean                 39.059788              39.02381\n",
       "std                  26.224710              26.75412\n",
       "min                   3.000000               3.00000\n",
       "25%                  22.000000              22.00000\n",
       "50%                  33.000000              33.00000\n",
       "75%                  49.000000              49.00000\n",
       "max                 190.000000             188.00000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2344b8f-7421-4556-99d1-bdcf7af1e970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97th percentile length English: 86.0\n",
      "97th percentile length Vietnam: 87.0\n"
     ]
    }
   ],
   "source": [
    "print( f\"{97}th percentile length English: {np.percentile(df_train['english_train_length'].tolist(), 97)}\" )\n",
    "print( f\"{97}th percentile length Vietnam: {np.percentile(df_train['vietnamese_train_length'], 97)}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "091b2228-5c22-486e-bef3-b69ae65fc399",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50477800-e795-4580-b00b-d9d4dc243589",
   "metadata": {},
   "outputs": [],
   "source": [
    "vn_temp = []\n",
    "eng_temp = []\n",
    "for i in range(0,len(vietnam_sentences_train)):\n",
    "    if len(vietnam_sentences_train[i]) <= MAX_LENGTH and len(english_sentences_train[i]) <= MAX_LENGTH:\n",
    "        vn_temp.append(vietnam_sentences_train[i])\n",
    "        eng_temp.append(english_sentences_train[i])\n",
    "vietnam_sentences_train = vn_temp\n",
    "english_sentences_train = eng_temp\n",
    "\n",
    "vn_temp = []\n",
    "eng_temp = []\n",
    "for i in range(0,len(vietnam_sentences_valid)):\n",
    "    if len(vietnam_sentences_valid[i]) <= MAX_LENGTH and len(english_sentences_valid[i]) <= MAX_LENGTH:\n",
    "        vn_temp.append(vietnam_sentences_valid[i])\n",
    "        eng_temp.append(english_sentences_valid[i])\n",
    "vietnam_sentences_valid = vn_temp\n",
    "english_sentences_valid = eng_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75c7d524-a029-4ee4-8d12-bc143540712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data for another training (save time, for hugging face error)\n",
    "import os\n",
    "\n",
    "folder = 'data'\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "\n",
    "with open(\"./data/vietnamese_train.txt\", \"w\",encoding='utf-8') as file:\n",
    "    for sentence in vietnam_sentences_train:\n",
    "        file.write(f\"{sentence}\\n\")\n",
    "with open(\"./data/vietnamese_valid.txt\", \"w\",encoding='utf-8') as file:\n",
    "    for sentence in vietnam_sentences_valid:\n",
    "        file.write(f\"{sentence}\\n\")\n",
    "with open(\"./data/english_train.txt\", \"w\",encoding='utf-8') as file:\n",
    "    for sentence in english_sentences_train:\n",
    "        file.write(f\"{sentence}\\n\")\n",
    "with open(\"./data/english_valid.txt\", \"w\",encoding='utf-8') as file:\n",
    "    for sentence in english_sentences_valid:\n",
    "        file.write(f\"{sentence}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151a802d-0bc2-4777-9c3c-101882a5d57d",
   "metadata": {},
   "source": [
    "- Setup DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0bee1283-e738-4a9e-9d98-7fdddb51f31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2tokens(sentence:str):\n",
    "    return [c for c in sentence]\n",
    "\n",
    "def encode_tokens(tokens, vocab):\n",
    "    encode_tokens = [vocab[x] for x in tokens]\n",
    "    return encode_tokens\n",
    "\n",
    "def preprocessing(sentences, vocab, max_length_seq, pad_char,start_char=None, end_char=None):\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        if start_char != None:\n",
    "            tokens = [vocab[start_char]] + tokens\n",
    "        if end_char != None:\n",
    "            tokens = tokens + [vocab[end_char]]\n",
    "        for i in range(len(tokens),max_length_seq):\n",
    "            tokens = tokens + [vocab[pad_char]]\n",
    "        num_pads = max_length_seq - len(sentence)\n",
    "        result.append(tokens)\n",
    "    return torch.tensor(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a129327-45e3-486d-87af-8b5f6df6846a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1790, 102])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = preprocessing(vietnam_sentences_valid, vietnamese_to_index, MAX_LENGTH, PADDING_TOKEN,START_TOKEN, END_TOKEN)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d48997ba-99c5-4a64-9e4b-32d83b91a6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, english_sentences, vietnam_sentences):\n",
    "        self.english_sentences = preprocessing(english_sentences, english_to_index, MAX_LENGTH, PADDING_TOKEN, None, None)\n",
    "        self.vietnam_sentences = preprocessing(vietnam_sentences, vietnamese_to_index, MAX_LENGTH,START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.english_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.english_sentences[idx], self.vietnam_sentences[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e64ac4fb-dacc-4431-8294-b95fa18b3706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "931237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 0, 67,  1, 52, 40, 47, 53, 39, 40, 52,  1, 55, 37,  1, 55, 47, 53, 44,\n",
       "         36,  1, 39, 47,  1, 52, 47,  1, 52, 40, 37,  1, 35, 40, 41, 44, 36, 50,\n",
       "         37, 46,  8, 51,  1, 40, 47, 45, 37, 15, 85, 85, 85, 85, 85, 85, 85, 85,\n",
       "         85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85,\n",
       "         85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85,\n",
       "         85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 86]),\n",
       " tensor([  0, 177,  79,  78, 126,  78,  67,  68,  73, 126,  52,  68, 103,  78,\n",
       "          67, 126, 101,  33, 126,  78,  61,  78, 126,  54,  62,  78, 126,  77,\n",
       "          34,  69, 126,  46,  77, 140, 126, 251, 251, 251, 251, 251, 251, 251,\n",
       "         251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251,\n",
       "         251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251,\n",
       "         251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251,\n",
       "         251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251,\n",
       "         251, 251, 251, 252]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = TextDataset(english_sentences_train,vietnam_sentences_train)\n",
    "print(len(data_train))\n",
    "data_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ff39c77-7014-4c28-ab30-3249fb30e61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 0, 67,  8, 45,  1, 51, 47, 50, 50, 57, 15,  1, 67,  1, 33, 45,  1, 46,\n",
       "         37, 50, 54, 47, 53, 51,  1, 52, 47, 36, 33, 57, 15,  1, 67,  1, 40, 33,\n",
       "         36,  1, 34, 33, 36,  1, 36, 50, 37, 33, 45, 51, 15, 85, 85, 85, 85, 85,\n",
       "         85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85,\n",
       "         85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85,\n",
       "         85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 86]),\n",
       " tensor([  0, 240,  69,  78, 126,  76,  95,  69, 138, 126,  68,  85,  77, 126,\n",
       "          78,  33, 116, 126, 101,  85,  69, 126, 101,  68,  46, 116, 126,  75,\n",
       "          68,  80, 126,  52,  68,  74, 102, 126, 226,  86,  69, 126,  98, 102,\n",
       "          33, 126, 101,  85,  69, 126,  54,  37, 126,  67,  44,  97, 126,  34,\n",
       "          52, 126,  77,  90,  78,  67, 126, 251, 251, 251, 251, 251, 251, 251,\n",
       "         251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251,\n",
       "         251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251,\n",
       "         251, 251, 251, 252]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_valid = TextDataset(english_sentences_valid,vietnam_sentences_valid)\n",
    "print(len(data_valid))\n",
    "data_valid[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "375ce297-ce07-4818-a0fa-939b152fdf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 30\n",
    "\n",
    "train_loader = DataLoader(data_train, BATCH_SIZE)\n",
    "valid_loader = DataLoader(data_valid, len(data_valid))\n",
    "iterator = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bb83e3c-ec65-4c1b-8613-3db3353aebab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 0, 81, 40,  ..., 85, 85, 86],\n",
      "        [ 0, 67,  1,  ..., 85, 85, 86],\n",
      "        [ 0, 67, 51,  ..., 85, 85, 86],\n",
      "        ...,\n",
      "        [ 0, 67,  8,  ..., 85, 85, 86],\n",
      "        [ 0, 67, 52,  ..., 85, 85, 86],\n",
      "        [ 0, 73, 40,  ..., 85, 85, 86]]), tensor([[  0, 177,  34,  ..., 251, 251, 252],\n",
      "        [  0, 177,  79,  ..., 251, 251, 252],\n",
      "        [  0, 177,  80,  ..., 251, 251, 252],\n",
      "        ...,\n",
      "        [  0, 226,  85,  ..., 251, 251, 252],\n",
      "        [  0, 177,  36,  ..., 251, 251, 252],\n",
      "        [  0, 101,  85,  ..., 251, 251, 252]])]\n",
      "[tensor([[ 0, 70, 37,  ..., 85, 85, 86],\n",
      "        [ 0, 83, 47,  ..., 85, 85, 86],\n",
      "        [ 0, 59, 51,  ..., 85, 85, 86],\n",
      "        ...,\n",
      "        [ 0, 59, 46,  ..., 85, 85, 86],\n",
      "        [ 0, 67,  1,  ..., 85, 85, 86],\n",
      "        [ 0, 67,  1,  ..., 85, 85, 86]]), tensor([[  0, 193,  37,  ..., 251, 251, 252],\n",
      "        [  0, 179,  87,  ..., 251, 251, 252],\n",
      "        [  0, 177,  68,  ..., 251, 251, 252],\n",
      "        ...,\n",
      "        [  0, 177,  80,  ..., 251, 251, 252],\n",
      "        [  0, 226,  85,  ..., 251, 251, 252],\n",
      "        [  0, 177,  79,  ..., 251, 251, 252]])]\n",
      "[tensor([[ 0, 73, 40,  ..., 85, 85, 86],\n",
      "        [ 0, 67, 52,  ..., 85, 85, 86],\n",
      "        [ 0, 61, 47,  ..., 85, 85, 86],\n",
      "        ...,\n",
      "        [ 0, 81, 40,  ..., 85, 85, 86],\n",
      "        [ 0, 81, 40,  ..., 85, 85, 86],\n",
      "        [ 0, 59, 44,  ..., 85, 85, 86]]), tensor([[  0, 212, 138,  ..., 251, 251, 252],\n",
      "        [  0, 203,  80,  ..., 251, 251, 252],\n",
      "        [  0, 203,  68,  ..., 251, 251, 252],\n",
      "        ...,\n",
      "        [  0, 177,  68,  ..., 251, 251, 252],\n",
      "        [  0, 139, 126,  ..., 251, 251, 252],\n",
      "        [  0, 179,  37,  ..., 251, 251, 252]])]\n"
     ]
    }
   ],
   "source": [
    "for batch_num, batch in enumerate(iterator):\n",
    "    print(batch)\n",
    "    if batch_num > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491b8bab-d6d4-4e85-bc46-4478cbe4c1d3",
   "metadata": {},
   "source": [
    "- Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab2aa26b-0318-47af-b11f-45e5975e744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(d_model=512,\n",
    "                    vocab_size=len(english_vocabulary),\n",
    "                    target_vocab_size=len(vietnamese_vocabulary),\n",
    "                    max_length_seq=MAX_LENGTH + 2,\n",
    "                    num_blocks=2,\n",
    "                    expansion_factor=4,\n",
    "                    num_heads=8\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f4bedce-49a9-437d-9c99-562b900b518a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (token_emb): TokenEmbedding(\n",
       "      (embedding_layer): Embedding(87, 512)\n",
       "    )\n",
       "    (pos_encode): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0-1): 2 x TransformerBlock(\n",
       "        (multihead_attention): MultiHeadAttention(\n",
       "          (query): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (key): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (value): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (token_emb): TokenEmbedding(\n",
       "      (embedding_layer): Embedding(253, 512)\n",
       "    )\n",
       "    (pos_encode): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (decoder_blocks): ModuleList(\n",
       "      (0-1): 2 x DecoderBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (query): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (key): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (value): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (transformer_block): TransformerBlock(\n",
       "          (multihead_attention): MultiHeadAttention(\n",
       "            (query): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear_layer): Linear(in_features=512, out_features=253, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23d628eb-9c6c-417d-870a-6508ffda12d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "criterian = nn.CrossEntropyLoss(ignore_index=vietnamese_to_index[PADDING_TOKEN],\n",
    "                                reduction='none')\n",
    "\n",
    "# Initialize weight\n",
    "for params in model.parameters():\n",
    "    if params.dim() > 1:\n",
    "        nn.init.xavier_uniform_(params)\n",
    "\n",
    "# optimize\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c95f36-5dba-45f5-96fe-ae9f1a17f758",
   "metadata": {},
   "source": [
    "- Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72338114-d943-4e64-8e7e-8da68599bb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 -------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m language_input \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     21\u001b[0m language_output \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 22\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Loss\u001b[39;00m\n\u001b[0;32m     25\u001b[0m labels \u001b[38;5;241m=\u001b[39m language_output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\Transformer-EntoVn-Translator\\MyTransformer.py:387\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, source, target)\u001b[0m\n\u001b[0;32m    385\u001b[0m trg_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_trg_mask(target)\n\u001b[0;32m    386\u001b[0m enc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(source)\n\u001b[1;32m--> 387\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    388\u001b[0m output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_layer(outputs), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\Transformer-EntoVn-Translator\\MyTransformer.py:338\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, x, encoder_output, mask)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;66;03m# Go to Transformer Blocks (Encode)\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_blocks:\n\u001b[1;32m--> 338\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\Transformer-EntoVn-Translator\\MyTransformer.py:283\u001b[0m, in \u001b[0;36mDecoderBlock.forward\u001b[1;34m(self, query, key, x, mask)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, key, x, mask): \u001b[38;5;66;03m# Different from Encoder\u001b[39;00m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;66;03m# Masked Multi-Head Attention\u001b[39;00m\n\u001b[1;32m--> 283\u001b[0m     decoder_attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Add & Norm\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# Add\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     decoder_attention_added \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(decoder_attention \u001b[38;5;241m+\u001b[39m x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\Transformer-EntoVn-Translator\\MyTransformer.py:129\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, query, key, value, mask)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# if mask (in decoder)\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 129\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     product \u001b[38;5;241m=\u001b[39m product\u001b[38;5;241m.\u001b[39mmasked_fill(mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-1e20\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;66;03m# -inf for softmax -> 0\u001b[39;00m\n\u001b[0;32m    132\u001b[0m product \u001b[38;5;241m=\u001b[39m product \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "model.to(device)\n",
    "loss_train = []\n",
    "loss_valid = []\n",
    "history = {}\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    print(f'Epoch {epoch} ' + '-' * (20- len(str(epoch))))\n",
    "    iterator = iter(train_loader)\n",
    "    length_iter = len(iterator)\n",
    "    # Training\n",
    "    for batch_num, batch in enumerate(iterator):\n",
    "        # Training mode\n",
    "        model.train()\n",
    "        # Reset Gradient from Backward Pass\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # Predict\n",
    "        language_input = batch[0].to(device)\n",
    "        language_output = batch[1].to(device)\n",
    "        predictions = model(language_input, language_output)\n",
    "\n",
    "        # Loss\n",
    "        labels = language_output.view(-1).to(device)\n",
    "        loss = criterian(\n",
    "            predictions.view(-1, len(vietnamese_vocabulary)),\n",
    "            labels\n",
    "        ).to(device)\n",
    "        ignore_pad = torch.where(labels == vietnamese_to_index[PADDING_TOKEN], False, True)\n",
    "        loss = loss.sum() / ignore_pad.sum()\n",
    "\n",
    "        # Backward and Optimize\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # History\n",
    "        loss_train.append(loss.item())\n",
    "\n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        iterator = iter(valid_loader)\n",
    "        for batch_num, batch in enumerate(iterator):\n",
    "            # Valid model\n",
    "            model.eval()\n",
    "\n",
    "            # Predict\n",
    "            language_input = batch[0].to(device)\n",
    "            language_output = batch[1].to(device)\n",
    "            predictions = model(language_input, language_output)\n",
    "\n",
    "            # Loss\n",
    "            labels = language_output.view(-1).to(device)\n",
    "            loss = criterian(\n",
    "                predictions.view(-1, len(vietnamese_vocabulary)),\n",
    "                labels\n",
    "            ).to(device)\n",
    "            ignore_pad = torch.where(labels == vietnamese_to_index[PADDING_TOKEN], False, True)\n",
    "            loss = loss.sum() / ignore_pad.sum()\n",
    "\n",
    "            # History\n",
    "            loss_valid.append(loss.item())\n",
    "\n",
    "    # Save history\n",
    "    history[epoch] = [sum(loss_train) / len(loss_train), sum(loss_valid) / len(loss_valid)]\n",
    "\n",
    "    # Result Train & Valid\n",
    "    print(f'{length_iter}/{length_iter}: Training loss: {history[epoch][0]} - Validation loss: {history[epoch][1]}')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c6c14a52-ecec-476f-89cf-b67488296f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [1, 2, 3]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d2303c-9f84-45ec-b06d-bc9fefba214d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TorchCuda",
   "language": "python",
   "name": "torchcuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
