{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7937cc48-41a4-4b3f-95ed-7b21950dba72",
   "metadata": {},
   "source": [
    "# Transformer Model - Translator English to Vietnamese - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b9f4e4-e01a-4621-96a8-dd35ae913f69",
   "metadata": {},
   "source": [
    "- Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "946e505c-32cf-46b5-9a68-e227ad319776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from datasets import load_dataset\n",
    "from MyTransformer import Transformer, Masking\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b90561-0481-44e2-94b5-3c9cdb83a1c4",
   "metadata": {},
   "source": [
    "- Load Dataset from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f325ccfe-cf63-4512-b74e-49cbb7e39a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 992248\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"kaitchup/opus-Vietnamese-to-English\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5beda784-b857-49dd-a690-ee2a2e5710fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cái gì đó? ###>What is it?',\n",
       " \"Con nghĩ chúng ta nên đến mái ấm. ###>I thought we would go to the children's home.\",\n",
       " 'Có điều gì cô muốn nói với chồng mình không? ###>Is there something you want to tell your husband?',\n",
       " 'Thầy của ngươi muốn săn chúng ta, thiêu chúng ta, ăn tim chúng ta. ###>Your master wants to hunt us, burn us, eat our hearts.',\n",
       " 'Haylàkẻ yếuđuối? ###>Or too weak to see this through?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['text'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6550a417-2e5e-4e8f-8d17-fe87412d3f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(992248, 992248)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_train = list(map(lambda x: x.split('###>'), dataset['train']['text']))\n",
    "vietnam_sentences_train = list(map(lambda x : x[0], sentences_train))\n",
    "english_sentences_train = list(map(lambda x : x[1], sentences_train))\n",
    "len(vietnam_sentences_train), len(english_sentences_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0bd0429-4362-4eef-8542-a7db41ff27db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_valid = list(map(lambda x: x.split('###>'), dataset['validation']['text']))\n",
    "vietnam_sentences_valid = list(map(lambda x : x[0], sentences_valid))\n",
    "english_sentences_valid = list(map(lambda x : x[1], sentences_valid))\n",
    "len(vietnam_sentences_valid), len(english_sentences_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "252e9b40-4b2d-47f9-9b34-42456d83c7c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anh cũng làm việc cho hắn ta? ',\n",
       " 'Xin lỡi, hôm nay tôi thấy khó chịu Tối qua tôi đã gặp ác mộng ',\n",
       " 'Em không cho mụ vinh hạnh đó đâu. ',\n",
       " '- Bỏ nó vào túi. ']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vietnam_sentences_valid[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a79b280-5009-4a17-86ee-daac8ab23d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You can act as him, too?',\n",
       " \"I'm sorry. I am nervous today. I had bad dreams.\",\n",
       " \"I wouldn't give her that pleasure. It's up to you.\",\n",
       " '- Leave that in this bag.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences_valid[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a015d7-31be-4e54-8443-170aaac549ab",
   "metadata": {},
   "source": [
    "- Setup vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "254b2400-b2f0-4c4a-bd6c-a15cd8e69cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = '<start>'\n",
    "PADDING_TOKEN = '<pad>'\n",
    "END_TOKEN = '<end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22b08de4-058c-48df-8cb0-58ab61ef81b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vietnamese_characters = [ ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', \n",
    "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', '?', 'ˌ',\n",
    "    'a', 'á', 'à', 'ả', 'ã', 'ạ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ',\n",
    "    'b', 'c', 'd', 'đ', 'e', 'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ', \n",
    "    'g', 'h', 'i', 'í', 'ì', 'ỉ', 'ĩ', 'ị', 'k', 'l', 'm', 'n', 'o', 'ó', 'ò', 'ỏ', 'õ', 'ọ', \n",
    "    'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'p', 'q', 'r', 's', 't', 'u', \n",
    "    'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'v', 'x', 'y', 'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ','z','w','f','j'\n",
    "]\n",
    "\n",
    "vietnamese_vocabulary = list(set([START_TOKEN] + vietnamese_characters + [char.upper() for char in vietnamese_characters] + [PADDING_TOKEN, END_TOKEN]))\n",
    "len(vietnamese_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21d1180f-dbdb-4fb1-bd10-522455479dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_vocabulary = [ START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', \n",
    "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', '?', 'ˌ',\n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', \n",
    "    'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', \n",
    "    'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "                      PADDING_TOKEN, END_TOKEN\n",
    "]\n",
    "len(english_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd21f039-db3e-4ba2-a463-9a26e720f038",
   "metadata": {},
   "source": [
    "- Check vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05880fc7-449f-4323-b185-1abcd8407662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Check_character(sentences,vocabulary):\n",
    "    missing_character = []\n",
    "    amount_sentences = 0\n",
    "    for sentence in sentences:\n",
    "        check = False\n",
    "        for c in list(set(sentence)):\n",
    "            if c not in vocabulary and c not in missing_character:\n",
    "                missing_character.append(c)\n",
    "                check = True\n",
    "        if check:\n",
    "            amount_sentences += 1\n",
    "    if len(missing_character) == 0:\n",
    "        print(\"Suitable vocabulary!\")\n",
    "        return None\n",
    "    print(f\"Find {missing_character} in vocabulary!\")\n",
    "    return amount_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb766a8a-32a3-4ca9-9270-f7a1287bcbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find ['♫', '̀', '́', '̉', '♪', '̣', '^', '̃', '\\\\', '}', '{', '»', '«', '̀', '́', '£', '–', 'ð', ';', '@', ']', '[', 'Μ', '°', '×', '\\xad', '¡', 'ª', '§', '\\x99', '\\x81', '\\xa0', '\\x8b', '³', '´', 'Æ', 'º', '½', 'Ð', '_', 'Û', 'ß', '王', '校', '長', '¶', 'Ü', '¢', '甩', '了', '他', '防', '員', '的', '守', '開', '隊', '星', '影', '電', '可', '明', '做', '手', '以', '歌', '還', '拍', 'ō', '傑', '們', '就', '是', '阿', '咱', '對', '事', '不', '嘛', 'ü', '¹', 'Ñ', 'γ', '’', 'Ë', 'ï', '≤', '\\x91', 'Ä', 'ñ', '¯', 'ο', 'ë', 'ä', 'λ', 'ç', '」', '「', '©', 'Ç', '~', 'Þ', 'Η', '®', '合', '嗎', '跟', '我', '照', '¿', '武', '叫', '裡', '學', '夫', '那', '功', '在', '—', '，', '赵', '非', '格', '铎', '李', '振', 'ö', '噯', '沒', '吧', 'Τ', 'Ε', 'Α', '\\x9f', 'ī', 'ā', 'ħ', '走', '·', '永', '退', '江', '出', '湖', '啊', 'Ѕ', 'ѕ', 'і', 'х', '沖', '有', '快', '你', '\\u202d', '年', '當', '敗', '生', '意', '失', '加', '嵐', '蕭', '愛', '油', '¥', '阀', '谈', '国', '民', '贤', '庆', '派', '陈', '军', '系', 'å', 'µ', '第', '下', '拳', '很', '前', '神', '一', '天', '都', '久', '來', '原', '師', '父', '\\x87', '\\x90', '\\u200b', '熟', '喔', '這', '好', '字', '≥', 'Ş', '鍵', '定', '關', '時', '決', '刻', '뭐', '요', '이', '름', '예', '\\x83', '±', '鐵', '得', '擋', '打', '紙', '把', '回', '頭', '傘', '水', '誰', '方', '撐', '望', '起', '多', '見', '紅', '樣', '子', 'ﬂ', '\\x8d', '¬', 'Ï', '件', '後', '禮', '最', '物', '算', '幹', '\\u200c', '大', '家', '結', '團', '而', 'о', 'е', '聽', '說', 'Ö', '》', '哇', '！', '²', 'ֶ', '辭', '思', '本', '什', '義', '麼', '容', '強', '柯', '丹', '森', '喬', '比', '夥', '火', '戰', '讓', '兒', '燃', '燒', '抖', '顫', '熊', 'ń', 'ğ', 'ı', '西', '要', '東', '交', '給', '理', '衝', '心', '個', '哥', '礙', '過', '障', '筆', '？', 'с', 'С', '看', '帶', 'Ō', '亮', '挺', '漂', 'ć', 'š', 'Ш', 'Х', '淌', '渾', '怎', '賤', '為', '拋', '棄', '去', '才', '候', '吃', '冰', '淇', '淋', '坐', '\\x8f', '能', '謝', '薑', '幕', '光', '老', '舊', '飛', '月', '散', '截', '段', '憶', 'Å', '鬥', '人', '\\x89', '球', '賽', '離', '別', 'ø', '切', '標', '磋', '將', '組', '重', '再', '分', '又', 'æ', '涼', '變', '畫', '面', '荒', '¨', '像', '勁', '何', '任', '機', '會', '葬', '同', '陪', '場', '到', '簽', '\\x94', '媽', '婆', '†', 'œ', '`', '小', '友', '朋', '動', '作', '話', '四', '阻', '三', '推', '→', 'ʾ', 'ḥ', '÷', '勝', '利', '冠', '成', '\\x96', '⊲', '其', '也', '女', '實', 'û', '歹', '輕', '相', '識', '點', '全', '安', 'ę', '差', '始', '技', '等', '賺', '錢', '想', '拿', '名', '張', '乾', '杯', '恩', '怨', '仇', '情', '爹', '啦', '娘', '找', '‘', '萬', '百', '忘', '搞', '五', '™', '♥', 'ė', '哈', '囉', '較', '適', 'ş', 'ţ', 'ν', '喂', '法', '違', '勾', '德', '及', '只', '道', '涉', '€', '₫', '參', '資', 'ы', 'р', 'в', 'В', 'т', 'Б', 'н', 'й', '¸', '‐', '跳', '᧯', '㮥', '班', '接', '☺', 'ℒ', 'ℱ', 'ℬ', 'Ω', '真', '掛', '住', '頂', '←', '嬲', '底', '哪', 'İ', '甲', '片', '留', '符', '絕', '地', '耶', '於', '呢', '至', '莉', '膚', '練', '吹', '肌', '彈', '破', '試', '空', '世', '中', '攔', '傳', '\\x9b', '慌', '裝', '悶', '穿', '上', '倦', '釋', '骨', '文', '集', '\\u200e', '鬼', '碰', '代', '幾', '言', '千', '瞞', '畢', '豪', '信', '聰', '經', '另', '外', '己', 'О', '親', '尋', '籃', 'И', 'м', 'и', 'ц', 'з', 'у', 'П', 'К', 'к', 'Э', 'а', 'Î', '佐', '々', '克', '木', '\\x93', '\\x95', '感', '覺', '視', '腳', '展', '稍', '伸', '微', '灌', '幫', '運', '魄', '氣', '間', '構', '由', '鉛', '竿', '丟', 'Ţ', '風', '羅', '竹', '倫', '敦', '需', '送', '衣', '貴', '服', 'ū', '批', '次', '引', '體', '吸', '媒', '靜', '記', 'î', '涯', '句', '號', '\\x9d', '免', '豆', '腐', 'Ζ', '改', '行', '旅', '太', '請', '問', '位', '毒', '管', '報', '”', '“', '值', '提', 'Ν', 'Ο', '億', '\\x97', '知', '選', '実', '新', '士', 'の', '樹', '搖', '哄', '半', '騙', '莫', '福', '源', '史', '群', '編', '歴', '部', '但', '投', '輸', '馬', '暗', '呼', '亂', '招', '漢', '典', '委', '語', '•', '¼', '份', '響', '身', '贏', '„', '采', '無', '精', '該', '屏', '遮', '霜', '尤', '極', '品', '楚', '清', 'ł', '邊', '滾', '疆', '秒', '剩', '禄', '祜', '谱', '钮', 'Υ', '掙', '死', '垂', '扎', 'δ', '然', 'ч', 'ь', '帥', 'ą'] in vocabulary!\n",
      "Find [']', '[', '♪', '£', '_', 'É', 'ü', 'ο', 'é', 'ā', '±', '¡', '¯', '–', ';', '@', '~', 'à', 'ñ', '—', '´', '`', '\\xa0', '\\x94', 'Β', 'ö', '}', '\\\\', '{', '\\u200e', 'ō', 'á', 'Μ', '’', '\\x9d', 'Ó', '¢', 'í', 'Α', '²', 'À', '¶', 'è', '¿', 'š', 'Ã', '、', '─', 'Ü', 'Τ', 'Υ', 'ó', 'ã', 'ä', 'ô', '♫', '赵', '非', '格', '铎', '李', '振', 'Η', 'ī', 'ħ', 'ﬂ', '\\ue0e1', '×', 'ð', '¼', 'Æ', 'â', 'æ', '§', 'Κ', '\\u202d', '阀', '谈', '国', '民', '贤', '庆', '派', '陈', '军', '系', '™', 'ë', 'Ε', '^', 'о', 'ν', 'ự', 'ư', 'ợ', 'ì', 'ù', 'º', 'τ', 'ç', 'Ş', 'е', 'с', 'ѕ', 'а', 'і', 'ï', '뭐', '요', '이', '름', '예', '\\x92', '\\x8b', 'ê', '》', '《', 'ﬁ', 'ú', '\\u200b', 'Ö', 'ể', 'ế', 'î', 'ª', 'Е', '\\u202f', '¤', '½', 'È', 'İ', 'ń', '¾', 'ı', 'ğ', 'û', 'Ç', '°', '΄', '·', 'Ν', 'Ō', 'ć', 'Ш', 'Х', 'Á', '\\x83', 'Â', '駛', '・', '“', '”', 'й', 'ầ', 'ø', 'ò', 'å', '÷', 'Ÿ', '€', '¨', 'Î', 'Ð', 'Ë', 'Ò', 'Ä', 'µ', '®', '†', 'ạ', 'ʾ', 'ḥ', '\\x96', '\\x81', 'ę', '‘', '¥', 'ė', '‚', '¬', 'ş', 'ă', 'ţ', '\\u2009', 'ы', 'р', 'в', 'В', 'т', 'х', 'Б', 'н', '‐', '©', 'ý', 'Г', 'œ', '♡', 'Í', '�', 'þ', '釋', '字', '甲', '骨', '文', '集', 'Ï', '¦', 'И', 'м', 'и', '»', 'ц', 'з', 'у', 'П', 'К', '«', 'к', 'Э', '佐', '々', '克', '木', 'ι', 'ь', '＃', 'Å', 'ū', 'Ţ', '風', '羅', '竹', '￡', 'ß', '\\u3000', '慹', 'Ā', 'Ι', '選', '武', '後', '像', '実', '新', '士', 'の', '組', '最', 'õ', '莫', '福', '源', 'ữ', '史', '群', '編', '歴', '部', '\\x9e', '\\x93', '\\x90', '¹', '典', '委', '大', '语', '员', '汉', '会', '„', 'ł', '禄', '祜', '谱', '家', '钮', 'ÿ', 'ą'] in vocabulary!\n"
     ]
    }
   ],
   "source": [
    "vietnam_wrong_sentences = Check_character(vietnam_sentences_train,vietnamese_vocabulary)\n",
    "english_wrong_sentences = Check_character(english_sentences_train,english_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162e6919-0cb9-4389-9acb-4582e4182ab1",
   "metadata": {},
   "source": [
    "Lots of characters like symbols, words in other languages. So we will try to remove all sentences which have unknown characters. If the amount of removed sentences are not so many, we can apply this. If so many sentences are removed, we should appy another ways like adding tag 'unknown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10dbac59-63eb-4eec-8b0c-b453df8f91a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sentences: 992248 (vietnam), 992248 (english)\n",
      "wrong train sentences: 277 (vietnam), 169 (english)\n"
     ]
    }
   ],
   "source": [
    "print(f'Train sentences: {len(vietnam_sentences_train)} (vietnam), {len(english_sentences_train)} (english)')\n",
    "print(f'wrong train sentences: {vietnam_wrong_sentences} (vietnam), {english_wrong_sentences} (english)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780adb4a-e0ef-4f2d-a1b0-5b0067db41c1",
   "metadata": {},
   "source": [
    "The number of removed sentences is much smaller than the total number of sentences so we can remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e61f3d6-e3e1-4acc-904c-1d29d5992aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_sentence(sentence,vocabulary):\n",
    "    for c in list(set(sentence)):\n",
    "        if c not in vocabulary:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8187d85-bd25-4894-8863-bc123c445612",
   "metadata": {},
   "outputs": [],
   "source": [
    "vn_temp = []\n",
    "eng_temp = []\n",
    "for i in range(0,len(vietnam_sentences_train)):\n",
    "    if is_valid_sentence(vietnam_sentences_train[i], vietnamese_vocabulary) and is_valid_sentence(english_sentences_train[i], english_vocabulary):\n",
    "        vn_temp.append(vietnam_sentences_train[i])\n",
    "        eng_temp.append(english_sentences_train[i])\n",
    "vietnam_sentences_train = vn_temp\n",
    "english_sentences_train = eng_temp\n",
    "\n",
    "vn_temp = []\n",
    "eng_temp = []\n",
    "for i in range(0,len(vietnam_sentences_valid)):\n",
    "    if is_valid_sentence(vietnam_sentences_valid[i], vietnamese_vocabulary) and is_valid_sentence(english_sentences_valid[i], english_vocabulary):\n",
    "        vn_temp.append(vietnam_sentences_valid[i])\n",
    "        eng_temp.append(english_sentences_valid[i])\n",
    "vietnam_sentences_valid = vn_temp\n",
    "english_sentences_valid = eng_temp\n",
    "\n",
    "\n",
    "# vietnam_wrong_sentences = Check_character(vietnam_sentences_train,vietnamese_vocabulary)\n",
    "# english_wrong_sentences = Check_character(english_sentences_train,english_vocabulary)\n",
    "# vietnam_wrong_sentences = Check_character(vietnam_sentences_valid,vietnamese_vocabulary)\n",
    "# english_wrong_sentences = Check_character(english_sentences_valid,english_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57527974-e618-446b-9caf-2ce057b0c7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_vietnamese = {k:v for k,v in enumerate(vietnamese_vocabulary)}\n",
    "vietnamese_to_index = {v:k for k,v in enumerate(vietnamese_vocabulary)}\n",
    "index_to_english = {k:v for k,v in enumerate(english_vocabulary)}\n",
    "english_to_index = {v:k for k,v in enumerate(english_vocabulary)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c410bc7-9b1f-4a15-ba29-30c36430ca5f",
   "metadata": {},
   "source": [
    "- Check Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cdb1ebb-c019-4519-8a2b-36414a20507a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame({\n",
    "    'vietnamese_train_length': [len(sentence) for sentence in vietnam_sentences_train],\n",
    "    'english_train_length': [len(sentence) for sentence in english_sentences_train],\n",
    "})\n",
    "\n",
    "df_valid = pd.DataFrame({\n",
    "    'vietnamese_valid_length': [len(sentence) for sentence in vietnam_sentences_valid],\n",
    "    'english_valid_length': [len(sentence) for sentence in english_sentences_valid],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45035cf8-85dc-4726-b55d-533ee3e94283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vietnamese_train_length</th>\n",
       "      <th>english_train_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>952120.000000</td>\n",
       "      <td>952120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>32.334417</td>\n",
       "      <td>30.988058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>21.854748</td>\n",
       "      <td>22.082578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>42.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>274.000000</td>\n",
       "      <td>416.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       vietnamese_train_length  english_train_length\n",
       "count            952120.000000         952120.000000\n",
       "mean                 32.334417             30.988058\n",
       "std                  21.854748             22.082578\n",
       "min                   2.000000              1.000000\n",
       "25%                  17.000000             15.000000\n",
       "50%                  27.000000             26.000000\n",
       "75%                  42.000000             40.000000\n",
       "max                 274.000000            416.000000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "131b9c4a-4f93-4682-afc3-2a7990d9458e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vietnamese_valid_length</th>\n",
       "      <th>english_valid_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1890.000000</td>\n",
       "      <td>1890.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>39.059788</td>\n",
       "      <td>39.02381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>26.224710</td>\n",
       "      <td>26.75412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>22.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>33.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>49.000000</td>\n",
       "      <td>49.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>190.000000</td>\n",
       "      <td>188.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       vietnamese_valid_length  english_valid_length\n",
       "count              1890.000000            1890.00000\n",
       "mean                 39.059788              39.02381\n",
       "std                  26.224710              26.75412\n",
       "min                   3.000000               3.00000\n",
       "25%                  22.000000              22.00000\n",
       "50%                  33.000000              33.00000\n",
       "75%                  49.000000              49.00000\n",
       "max                 190.000000             188.00000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2344b8f-7421-4556-99d1-bdcf7af1e970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97th percentile length English: 86.0\n",
      "97th percentile length Vietnam: 87.0\n"
     ]
    }
   ],
   "source": [
    "print( f\"{97}th percentile length English: {np.percentile(df_train['english_train_length'].tolist(), 97)}\" )\n",
    "print( f\"{97}th percentile length Vietnam: {np.percentile(df_train['vietnamese_train_length'], 97)}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "091b2228-5c22-486e-bef3-b69ae65fc399",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50477800-e795-4580-b00b-d9d4dc243589",
   "metadata": {},
   "outputs": [],
   "source": [
    "vn_temp = []\n",
    "eng_temp = []\n",
    "for i in range(0,len(vietnam_sentences_train)):\n",
    "    if len(vietnam_sentences_train[i]) < MAX_LENGTH - 1 and len(english_sentences_train[i]) < MAX_LENGTH - 1:\n",
    "        vn_temp.append(vietnam_sentences_train[i])\n",
    "        eng_temp.append(english_sentences_train[i])\n",
    "vietnam_sentences_train = vn_temp\n",
    "english_sentences_train = eng_temp\n",
    "\n",
    "vn_temp = []\n",
    "eng_temp = []\n",
    "for i in range(0,len(vietnam_sentences_valid)):\n",
    "    if len(vietnam_sentences_valid[i]) < MAX_LENGTH - 1 and len(english_sentences_valid[i]) < MAX_LENGTH - 1:\n",
    "        vn_temp.append(vietnam_sentences_valid[i])\n",
    "        eng_temp.append(english_sentences_valid[i])\n",
    "vietnam_sentences_valid = vn_temp\n",
    "english_sentences_valid = eng_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75c7d524-a029-4ee4-8d12-bc143540712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data for another training (save time, for hugging face error)\n",
    "import os\n",
    "\n",
    "folder = 'data'\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "\n",
    "with open(\"./data/vietnamese_train.txt\", \"w\",encoding='utf-8') as file:\n",
    "    for sentence in vietnam_sentences_train:\n",
    "        file.write(f\"{sentence}\\n\")\n",
    "with open(\"./data/vietnamese_valid.txt\", \"w\",encoding='utf-8') as file:\n",
    "    for sentence in vietnam_sentences_valid:\n",
    "        file.write(f\"{sentence}\\n\")\n",
    "with open(\"./data/english_train.txt\", \"w\",encoding='utf-8') as file:\n",
    "    for sentence in english_sentences_train:\n",
    "        file.write(f\"{sentence}\\n\")\n",
    "with open(\"./data/english_valid.txt\", \"w\",encoding='utf-8') as file:\n",
    "    for sentence in english_sentences_valid:\n",
    "        file.write(f\"{sentence}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151a802d-0bc2-4777-9c3c-101882a5d57d",
   "metadata": {},
   "source": [
    "- Setup DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d48997ba-99c5-4a64-9e4b-32d83b91a6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, english_sentences, vietnam_sentences):\n",
    "        self.english_sentences = english_sentences\n",
    "        self.vietnam_sentences = vietnam_sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.english_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.english_sentences[idx], self.vietnam_sentences[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e64ac4fb-dacc-4431-8294-b95fa18b3706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"I thought we would go to the children's home.\",\n",
       " 'Con nghĩ chúng ta nên đến mái ấm. ')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = TextDataset(english_sentences_train[:10000],vietnam_sentences_train[:10000])\n",
    "print(len(data_train))\n",
    "data_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1ff39c77-7014-4c28-ab30-3249fb30e61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"I'm sorry. I am nervous today. I had bad dreams.\",\n",
       " 'Xin lỡi, hôm nay tôi thấy khó chịu Tối qua tôi đã gặp ác mộng ')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_valid = TextDataset(english_sentences_valid,vietnam_sentences_valid)\n",
    "print(len(data_valid))\n",
    "data_valid[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "375ce297-ce07-4818-a0fa-939b152fdf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 30\n",
    "\n",
    "train_loader = DataLoader(data_train, BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(data_valid, BATCH_SIZE)\n",
    "iterator = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1bb83e3c-ec65-4c1b-8613-3db3353aebab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Randa!', \"- Nothing, really, I'm--\", '- We liberated it.', \"It's no good.\", \"That's the coolest thing that ever happened to me.\", 'And credit card statements.', 'Looks like she survived the procedure.', 'How do you know what kind of goddamn day it is?', \"You've got...\", 'I have never done that before.', 'Awesome.', \"I'm here!\", 'Am I right?', 'Please stop.', 'Hi.', 'Everything in this world is magic, except to the magician.', 'Kingpin.', 'Even if your intentions are good, it can backfire drastically.', 'That bugger moved so fast,', 'More supporting evidence?', \"I'm sitting right here.\", 'Can I have your card?', \"I'll see you later.\", '- Who are you?', \"Doesn't matter what will trigger of the bombs.\", 'Dancing.', '- Yeah. You look after our place for me, all right?', '- Reddick have to call in sick?', \"Ace, let's party!\", \"You're correct.\"), ('Randa! ', '- Thật ra thì không có gì. ', '- Nó đã được giải phóng. ', 'Không được. ', 'Đây là chuyện tuyệt với nhất từng xảy ra với tôi. ', 'Và bản kê khai thẻ tín dụng. ', 'Có vẻ như cổ đã sống sót qua ca mổ. ', 'Làm sao cậu biết được hôm nay là ngày như thế nào? ', 'Em bị... ', 'em chưa bao giờ xxx thế này. ', 'Đá hay đấy. ', 'Tôi đây! ', 'Đúng chứ? ', 'Làm ơn dừng lại đi. ', 'Hi. ', 'Mọi thứ trên thế giới này đều là ma thuật, ngoại trừ ảo thuật gia. ', 'Kingpin. ', 'Thậm chí nếu cháu có thiện chí, nó có thể phản tác dụng trầm trọng. ', 'Cái tay đó di chuyển nhanh lắm. ', 'Thêm chứng cứ à? ', 'Hừm, chưa vào tay tôi thôi. ', 'Tôi có thể lấy xe anh không? ', 'Gặp lại sau nhé. ', '- Ông là ai? ', 'Không gì có thể ngăn cản được việc nổ bom. ', 'Múa ', 'Trông coi nơi này cho bố nhé. ', 'Bernie nghỉ vì bệnh hả? ', 'Ace ,đi đi! ', 'Anh đúng. ')]\n",
      "[('U.S. Navy.', 'Find all files created or & modified:', 'If we leave you in peace, will you do the same?', 'They gave me drugs.', 'What do you say?', 'Maybe you could take me shopping.', 'Sorry.', 'Richie?', 'All right?', 'Just keep it simple.', \"- I'm sorry, Naka.\", 'You are very sick.', 'Fusco, the Armenians.', 'What the fuck were you thinking?', \"But you like him, though, don't you?\", 'I gathered everyone this time, not just to loot and plunder!', '- These three are gonna walk.', \"I'm not lying.\", 'Typical.', '- I suggest you fill up a gun.', 'I want you to give.', 'What you got?', 'Blooper reel.', 'County Road 66, vicinity Needham Meadow.', 'Sure.', 'Yes, of course.', '- Go away.', 'Wake up! Wake up!', 'Gavin?', 'Of course, I am.'), ('Hải quân Mỹ! ', 'Tìm & mọi tập tin được tạo hay sửa đổi: ', 'Nếu chúng tôi dời đi trong hòa bình thì các bạn cũng sẽ làm vậy chứ? ', 'Họ cho tôi uống thuốc . ', 'Các đệ thấy sao? ', 'Có lẽ cô nên dẫn tôi đi mua sắm. ', 'Xin lỗi. ', 'Richie. ', 'Được không? ', 'Chỉ đơn giản vậy thôi. ', 'Sao? ', 'Ngài yếu lắm rồi. ', 'Fusco, đám Armenia! ', 'Anh nghĩ sao mà lại làm thế? ', 'Nhưng cô thích ông ta, phải không? ', 'Ta tập hợp mọi người lại, không chỉ để cướp bóc! ', '- Ba người này sẽ được thả ra đấy. ', '...không nói dối mà, thiệt đó... ', 'Như bao người khác. ', '- Tôi đề nghị cô nên nạp đạn đầy khẩu súng đi. ', 'Ta muốn nàng tự trao mình cho ta. ', 'Anh có gì? ', 'Blooper reel. ', 'Đường 66, gần bãi cỏ Needham. ', 'Được. ', 'Phải, tất nhiên. ', '- Biến đi. ', 'Tỉnh lại, tỉnh lại đi. ', '- Gavin? ', 'Cha luôn tự hào về còn. ')]\n",
      "[(\"Watch says a lot about a man... Who he is, where he's going.\", 'I want to tell you with my last breath...', '- Huh. Okay!', '- What manner of Harith?', 'Fine.', 'Detective, please, get me a coffee.', 'I think Beca should take my solo.', 'Sam, Sam.', 'Leonard Vanderwick... does that mean anything to you?', 'Shit.', 'Runners at first and third.', 'Your notable deaths, just name a year.', 'Yeah, I love the ocean.', '-Good night, miss Daisy.', 'I certainly had to get tough with you in my day.', '- You get it.', 'What is it?', 'It brings them up to the surface where I can spear them.', 'You are incorrigible.', 'Maybe some asshole gave it to her.', \"- I'm all ears.\", 'Follow me.', \"And thanks to Stacy that dream's about to come true.\", 'Man: Come on back.', 'What?', 'What the hell?', 'My great-great grandfather painted this after his return.', 'I want to take you for a spin', 'I... Hi.', 'No, not exactly.'), ('Đồng hồ nói rất nhiều về người đàn ông... anh ta đi đâu. ', 'Cho nên anh muốn nói với em trong hơi thở cuối cùng... ', '- Được rồi! ', '- Hạng nào của người Harith? ', '- Được thôi. ', 'Thanh tra, xin hãy cho tôi một ly cà phê. ', 'Tớ nghĩ Beca nên hát phần solo của tớ. ', 'Sam, Sam. ', 'Leonard Vanderwick... nghe có quen không? ', 'Cứt thật! ', 'Chạy gôn ở gôn số một và gôn số ba. ', 'Những lần mà cậu chết, cho tôi cái năm ', 'Vâng, tôi yêu đại dương. ', '-Chúc ngủ ngon cô Daisy. ', 'Ta chắc chắn rằng họ đã gặp khó khăn với các em trong ngày của ta. ', '- Cô nhận được nó. ', 'Cái gì đó? ', 'Nó đưa chúng lên mặt nước để anh có thể xiên chúng. ', 'Anh thật hết thuốc chữa. ', 'Có thể là từ một thằng khốn nào đó. ', '- Tôi đang nghe đây. ', '- Theo tôi. ', 'Và cám ơn Stacy đã làm giấc mơ đó thành hiện thực. ', 'Quay lại đi. ', 'Hả? ', 'Cái mẹ gì? ', 'Đây là bức tranh kỵ tôi vẽ sau khi trở về từ nước ngoài. ', 'Anh muốn đưa em đi dạo một vòng. ', 'Xin chào! ', 'Không, không hẳn. ')]\n"
     ]
    }
   ],
   "source": [
    "for batch_num, batch in enumerate(iterator):\n",
    "    print(batch)\n",
    "    if batch_num > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491b8bab-d6d4-4e85-bc46-4478cbe4c1d3",
   "metadata": {},
   "source": [
    "- Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ab2aa26b-0318-47af-b11f-45e5975e744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(d_model=512,\n",
    "                    ff_hidden=2048,\n",
    "                    num_heads=8,\n",
    "                    dropout=0.1,\n",
    "                    num_blocks=1,\n",
    "                    max_length_seq=MAX_LENGTH,\n",
    "                    language_to_index=english_to_index,\n",
    "                    target_language_to_index=vietnamese_to_index,\n",
    "                    start_token=START_TOKEN,\n",
    "                    end_token=END_TOKEN,\n",
    "                    pad_token=PADDING_TOKEN\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3f4bedce-49a9-437d-9c99-562b900b518a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (input_preprocessing): Preprocessing(\n",
       "      (token_embedding): TokenEmbedding(\n",
       "        (embedding_layer): Embedding(87, 512)\n",
       "      )\n",
       "      (positional_encoding): PositionalEncoding(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm_for_attention): LayerNormalization()\n",
       "        (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "        (ff): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm_for_ff): LayerNormalization()\n",
       "        (dropout_for_ff): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (output_preprocessing): Preprocessing(\n",
       "      (token_embedding): TokenEmbedding(\n",
       "        (embedding_layer): Embedding(221, 512)\n",
       "      )\n",
       "      (positional_encoding): PositionalEncoding(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm_for_attention): LayerNormalization()\n",
       "        (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "        (cross_attention): MultiHeadAttention(\n",
       "          (kv_layer): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (q_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm_for_cross_attention): LayerNormalization()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ff): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm_for_ff): LayerNormalization()\n",
       "        (dropout_for_ff): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=221, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "23d628eb-9c6c-417d-870a-6508ffda12d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "criterian = nn.CrossEntropyLoss(ignore_index=vietnamese_to_index[PADDING_TOKEN],\n",
    "                                reduction='none')\n",
    "\n",
    "# Initialize weight\n",
    "for params in model.parameters():\n",
    "    if params.dim() > 1:\n",
    "        nn.init.xavier_uniform_(params)\n",
    "\n",
    "# optimize\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6fa27af0-42ed-4e03-8ca3-374f3023ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation\n",
    "def validation_translator(model, valid_dataloader):\n",
    "    iterator = iter(valid_dataloader)\n",
    "    valid_loss = []\n",
    "    with torch.no_grad():\n",
    "        for batch_num, batch in enumerate(iterator):\n",
    "            model.eval()\n",
    "            language_input = batch[0]\n",
    "            language_output = batch[1]\n",
    "            \n",
    "            # Get mask\n",
    "            encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = Masking(language_input, language_output, MAX_LENGTH)\n",
    "    \n",
    "            # Predict\n",
    "            predictions = model(language_input,\n",
    "                                language_output,\n",
    "                                encoder_self_attention_mask,\n",
    "                                decoder_self_attention_mask,\n",
    "                                decoder_cross_attention_mask,\n",
    "                                encoder_start_token=False,\n",
    "                                encoder_end_token=False,\n",
    "                                decoder_start_token=True,\n",
    "                                decoder_end_token=True)\n",
    "    \n",
    "            # Loss\n",
    "            Truelabels_tokens = model.decoder.output_preprocessing.batch_tokens(batch=language_output,start_token=False,end_token=True)\n",
    "    \n",
    "            loss = criterian(\n",
    "                predictions.view(-1, len(vietnamese_to_index)),\n",
    "                Truelabels_tokens.view(-1).to(device)\n",
    "            ).to(device)\n",
    "            ignore_pad = torch.where(Truelabels_tokens.view(-1) == vietnamese_to_index[PADDING_TOKEN], False, True)\n",
    "            loss = loss.sum() / ignore_pad.sum()\n",
    "            valid_loss.append(loss.item())\n",
    "    return sum(valid_loss) / len(valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c95f36-5dba-45f5-96fe-ae9f1a17f758",
   "metadata": {},
   "source": [
    "- Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "72338114-d943-4e64-8e7e-8da68599bb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 -------------------------------------------------------------------------------\n",
      "10% Training Progress: time: 18.41 seconds - loss: 3.0740585327148438\n",
      "- English Input: I, who lived at that time, would know better.\n",
      "- Vietnamese True Output: Tình cảnh lúc đó chỉ có ta là rõ nhất. \n",
      "- Vietnamese Predict: Cô   c    chn ci c i c  n  tn ccn\n",
      "20% Training Progress: time: 16.9 seconds - loss: 2.970031499862671\n",
      "- English Input: - Mom.\n",
      "- Vietnamese True Output: - Mẹ. \n",
      "- Vietnamese Predict: T h   \n",
      "30% Training Progress: time: 17.11 seconds - loss: 2.693114995956421\n",
      "- English Input: Don't go anywhere.\n",
      "- Vietnamese True Output: Đừng đi đâu cả. \n",
      "- Vietnamese Predict: Cn g ci nnn đh  \n",
      "40% Training Progress: time: 17.79 seconds - loss: 2.8159708976745605\n",
      "- English Input: This reminds me of home when I was a kid\n",
      "- Vietnamese True Output: ở đây làm cho tôi nhớ đến gia đình hồi tôi còn nhỏ \n",
      "- Vietnamese Predict: C ti  tn  t   thi c   m n n   t    t   \n",
      "50% Training Progress: time: 17.04 seconds - loss: 2.658778190612793\n",
      "- English Input: What if you're wrong?\n",
      "- Vietnamese True Output: Nếu bà sai thì sao? \n",
      "- Vietnamese Predict: Chn l  thi th  cn?? \n",
      "60% Training Progress: time: 16.9 seconds - loss: 2.681424140930176\n",
      "- English Input: Case, help him out of there!\n",
      "- Vietnamese True Output: Có đường tắt bên trái. \n",
      "- Vietnamese Predict: Ai chnig côn t n đhay. \n",
      "70% Training Progress: time: 18.4 seconds - loss: 2.451977252960205\n",
      "- English Input: Maybe you took it home.\n",
      "- Vietnamese True Output: Có thể giành quyền nuôi con bé. \n",
      "- Vietnamese Predict: Ch thô tôyng tuy n nh?n.\n",
      "80% Training Progress: time: 18.68 seconds - loss: 2.022101640701294\n",
      "- English Input: Preoccupied.\n",
      "- Vietnamese True Output: Tôi quen rồi. \n",
      "- Vietnamese Predict: Tôi lain lai  \n",
      "90% Training Progress: time: 18.83 seconds - loss: 2.0569798946380615\n",
      "- English Input: Give me a cigarette.\n",
      "- Vietnamese True Output: Hãy cho anh một điếu thuốc. \n",
      "- Vietnamese Predict: Cii chô anh c t ti.t.\n",
      "100% Training Progress: time: 17.45 seconds - loss: 1.6702290773391724\n",
      "- English Input: Or so it seems.\n",
      "- Vietnamese True Output: Hay có vẻ như vậy. \n",
      "- Vietnamese Predict: Tiy có va nh  \n",
      "Epoch 2 -------------------------------------------------------------------------------\n",
      "10% Training Progress: time: 17.65 seconds - loss: 1.4571025371551514\n",
      "- English Input: A life.\n",
      "- Vietnamese True Output: Một cuộc sống . \n",
      "- Vietnamese Predict: -ât cạ th\n",
      "20% Training Progress: time: 16.84 seconds - loss: 1.4816665649414062\n",
      "- English Input: This is Mika\n",
      "- Vietnamese True Output: Cô ấy là Mika \n",
      "- Vietnamese Predict: Cô ry là bi . \n",
      "30% Training Progress: time: 16.86 seconds - loss: 1.4177314043045044\n",
      "- English Input: Vodka only.\n",
      "- Vietnamese True Output: Ở đây chỉ vodka thôi. \n",
      "- Vietnamese Predict: C hâu hhô ri? h \n",
      "40% Training Progress: time: 16.89 seconds - loss: 1.133004069328308\n",
      "- English Input: Wake up Wake up\n",
      "- Vietnamese True Output: Con sâu lười, dậy mau \n",
      "- Vietnamese Predict: Con láu lười, \n",
      "50% Training Progress: time: 17.46 seconds - loss: 0.9875676035881042\n",
      "- English Input: Fuck.\n",
      "- Vietnamese True Output: Mẹ kiếp. \n",
      "- Vietnamese Predict: Mì kiin  \n",
      "60% Training Progress: time: 17.04 seconds - loss: 1.1311490535736084\n",
      "- English Input: I was actually dying. Look at this.\n",
      "- Vietnamese True Output: - Anh thực sự sắp chết ư - tôi thực sự sắp chết rồi \n",
      "- Vietnamese Predict: - Ahh thức sự sộp chểt ư n tôi thôc ti tachtha  nai \n",
      "70% Training Progress: time: 17.31 seconds - loss: 0.8604855537414551\n",
      "- English Input: It was my turn, that's all.\n",
      "- Vietnamese True Output: Và lần này đến phiên tôi, thế thôi. \n",
      "- Vietnamese Predict: Đà làn này đến phiện tôi, thết\n",
      "80% Training Progress: time: 16.85 seconds - loss: 0.556281328201294\n",
      "- English Input: I'll be back.\n",
      "- Vietnamese True Output: Em sẽ trở lại sau. \n",
      "- Vietnamese Predict: Mm sẽ trở lại \n",
      "90% Training Progress: time: 16.8 seconds - loss: 0.6570476293563843\n",
      "- English Input: Father's upset his baby didn't come home for supper.\n",
      "- Vietnamese True Output: Giống như là tìm về nhà để được an toàn vậy \n",
      "- Vietnamese Predict: Viêng như là tìm vì nhà để được an toàn vêy \n",
      "100% Training Progress: time: 16.85 seconds - loss: 0.6422605514526367\n",
      "- English Input: tell him to come here.\n",
      "- Vietnamese True Output: Bảo ông ta ra đây. \n",
      "- Vietnamese Predict: Bảo ông ta ra đây. \n",
      "Epoch 3 -------------------------------------------------------------------------------\n",
      "10% Training Progress: time: 17.65 seconds - loss: 0.6434069871902466\n",
      "- English Input: You mustn 't let your faith be overshadowed by guilt.\n",
      "- Vietnamese True Output: Đừng để mặc cảm tội lỗi làm phai mờ đức tin của con. \n",
      "- Vietnamese Predict: Đừng để mộc cảm tội lỗi làm phai mà đi  tin c a mon..\n",
      "20% Training Progress: time: 17.11 seconds - loss: 0.6858490109443665\n",
      "- English Input: I'm building an evil army.\n",
      "- Vietnamese True Output: Tao đang xây dựng 1 quân đoàn ma quỷ. \n",
      "- Vietnamese Predict: Tao đang xây dểng ở quân đư.  đấ.\n",
      "30% Training Progress: time: 16.86 seconds - loss: 0.6093255877494812\n",
      "- English Input: An error occurred while trying to write to this file.\n",
      "- Vietnamese True Output: Gặp lỗi khi thử ghi vào tập tin này. NAME OF TRANSLATORS \n",
      "- Vietnamese Predict: H p lỗi khi thì ghi vào tộp tin này. NAA  lT TắAn ê  n .nt      ộ                              ộ    \n",
      "40% Training Progress: time: 16.81 seconds - loss: 0.7000986337661743\n",
      "- English Input: How much could he possibly drink?\n",
      "- Vietnamese True Output: Ngài ấy có thể uốn tới múc nào nhỉ? \n",
      "- Vietnamese Predict: Ngài ấy có thể uốn tái múc nào nhô  \n",
      "50% Training Progress: time: 16.81 seconds - loss: 0.46845805644989014\n",
      "- English Input: I don't know, but you need to take care of them.\n",
      "- Vietnamese True Output: Tôi không biết, nhưng cô cần phải xử lý họ đi. \n",
      "- Vietnamese Predict: Tôi không biết, nhưng cô cần khải đd lở hắ đi. \n",
      "60% Training Progress: time: 16.93 seconds - loss: 0.4133859872817993\n",
      "- English Input: Fast-forward me till I'm better.\n",
      "- Vietnamese True Output: Tua đến lúc hết bệnh nào. \n",
      "- Vietnamese Predict: Tua đến lúc hết bệnh nào. \n",
      "70% Training Progress: time: 17.09 seconds - loss: 0.4846251904964447\n",
      "- English Input: 2006 IUCN Red List of Threatened Species.\n",
      "- Vietnamese True Output: 2006 IUCN Red List of Threatened Species. \n",
      "- Vietnamese Predict: M00d UUCN Red List o  Threatened Spepies. \n",
      "80% Training Progress: time: 16.86 seconds - loss: 0.6555059552192688\n",
      "- English Input: All right, all right.\n",
      "- Vietnamese True Output: Vậy sao? \n",
      "- Vietnamese Predict: Vậy sao? \n",
      "90% Training Progress: time: 16.84 seconds - loss: 0.5314909219741821\n",
      "- English Input: I'm Kay's assistant.\n",
      "- Vietnamese True Output: Trợ lý của bà Kay. \n",
      "- Vietnamese Predict: Trệ lã của bà nay. \n",
      "100% Training Progress: time: 16.84 seconds - loss: 0.5289111733436584\n",
      "- English Input: - Good. And sexy!\n",
      "- Vietnamese True Output: Và quyến rũ nữa. \n",
      "- Vietnamese Predict: Và quyến rự nữú. \n",
      "Epoch 4 -------------------------------------------------------------------------------\n",
      "10% Training Progress: time: 18.11 seconds - loss: 0.5315241813659668\n",
      "- English Input: I have no immunity to it.\n",
      "- Vietnamese True Output: Tôi không miễn dịch được với bệnh đó. \n",
      "- Vietnamese Predict: Tôi không miễn dịch được tac.titg tư  \n",
      "20% Training Progress: time: 16.96 seconds - loss: 0.5105790495872498\n",
      "- English Input: - Let me see.\n",
      "- Vietnamese True Output: - Để em xem nào. \n",
      "- Vietnamese Predict: - Để em xem ng ? \n",
      "30% Training Progress: time: 16.83 seconds - loss: 0.5415249466896057\n",
      "- English Input: I'm Chase.\n",
      "- Vietnamese True Output: Mình là Chase. \n",
      "- Vietnamese Predict: Mình là Choon. \n",
      "40% Training Progress: time: 16.84 seconds - loss: 0.33068791031837463\n",
      "- English Input: We're only gonna get one shot.\n",
      "- Vietnamese True Output: Chúng ta chỉ có một cơ hội thôi. \n",
      "- Vietnamese Predict: Chúng ta chỉ có một cơ hội thôi  \n",
      "50% Training Progress: time: 16.82 seconds - loss: 0.4471605718135834\n",
      "- English Input: You didn't sleep at all last night, did you?\n",
      "- Vietnamese True Output: Tối qua con không hề ngủ, đúng không? \n",
      "- Vietnamese Predict: Tối qua con không hề ngủ, đúng không? \n",
      "60% Training Progress: time: 16.83 seconds - loss: 0.7687878012657166\n",
      "- English Input: - We'll think up something.\n",
      "- Vietnamese True Output: - Chúng ta sẽ nghĩ ra cái gì? \n",
      "- Vietnamese Predict: - Chúng ta sẽ nghĩ ra cái gì  \n",
      "70% Training Progress: time: 16.85 seconds - loss: 0.6325973868370056\n",
      "- English Input: Maleficent, the border guards...\n",
      "- Vietnamese True Output: Maleficent, lính giữ cổng \n",
      "- Vietnamese Predict: Maleficent, lính giữ cổng \n",
      "80% Training Progress: time: 16.81 seconds - loss: 0.4132906198501587\n",
      "- English Input: You know what's more important than the rules, though?\n",
      "- Vietnamese True Output: Anh biết có cái gì quan trọng hơn nữa không? \n",
      "- Vietnamese Predict: Anh biết có cái gì quan trọng hơn nữa không? \n",
      "90% Training Progress: time: 16.83 seconds - loss: 0.3615208864212036\n",
      "- English Input: - Take care of yourself and be careful.\n",
      "- Vietnamese True Output: Hãy bảo trọng và nhớ cẩn thận. Được. \n",
      "- Vietnamese Predict: Hãy bảo trọng và nhớ củn thận. \n",
      "100% Training Progress: time: 17.32 seconds - loss: 0.4917662739753723\n",
      "- English Input: Not caught.\n",
      "- Vietnamese True Output: Không được rồi. Vào! \n",
      "- Vietnamese Predict: Không được cồi. \n",
      "Epoch 5 -------------------------------------------------------------------------------\n",
      "10% Training Progress: time: 18.45 seconds - loss: 0.24853835999965668\n",
      "- English Input: Okay.\n",
      "- Vietnamese True Output: Được rồi mẹ. \n",
      "- Vietnamese Predict: Được gồi.\n",
      "20% Training Progress: time: 16.87 seconds - loss: 0.43231475353240967\n",
      "- English Input: - Am not.\n",
      "- Vietnamese True Output: - Tôi không đần. \n",
      "- Vietnamese Predict: - Tôi không.ning \n",
      "30% Training Progress: time: 16.88 seconds - loss: 0.5411449670791626\n",
      "- English Input: -Thank you, sweetie.\n",
      "- Vietnamese True Output: cưng. \n",
      "- Vietnamese Predict: cưng. \n",
      "40% Training Progress: time: 16.97 seconds - loss: 0.42153120040893555\n",
      "- English Input: It is an honor to...\n",
      "- Vietnamese True Output: Thật vinh dự được... \n",
      "- Vietnamese Predict: Thật vinh dự được... \n",
      "50% Training Progress: time: 18.69 seconds - loss: 0.3372766375541687\n",
      "- English Input: I'm fine.\n",
      "- Vietnamese True Output: Em không sao. \n",
      "- Vietnamese Predict: Em không đann \n",
      "60% Training Progress: time: 17.87 seconds - loss: 0.44159096479415894\n",
      "- English Input: Look I...\n",
      "- Vietnamese True Output: Tôi...tôi phải nói chúng ta... \n",
      "- Vietnamese Predict: Tôi...tôi đhô .th  thông.cô    \n",
      "70% Training Progress: time: 18.14 seconds - loss: 0.3678342401981354\n",
      "- English Input: Relax.\n",
      "- Vietnamese True Output: Thư giãn nào. \n",
      "- Vietnamese Predict: Thư gi   cà   \n",
      "80% Training Progress: time: 17.63 seconds - loss: 0.4030626714229584\n",
      "- English Input: You should call for help.\n",
      "- Vietnamese True Output: Kêu cứu đi! \n",
      "- Vietnamese Predict: Kêu cứu đi! \n",
      "90% Training Progress: time: 17.29 seconds - loss: 0.3199368417263031\n",
      "- English Input: So glad you're staying with us.\n",
      "- Vietnamese True Output: Rất mừng cô đã tới chỗ chúng tôi. \n",
      "- Vietnamese Predict: Rất mừng cô đã tới chỗ chúng tôn  \n",
      "100% Training Progress: time: 17.22 seconds - loss: 0.20603282749652863\n",
      "- English Input: Right here!\n",
      "- Vietnamese True Output: Thì nó đây! \n",
      "- Vietnamese Predict: Thì nó đây! \n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "model.to(device)\n",
    "loss_train = []\n",
    "loss_valid = []\n",
    "history = {}\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    print(f'Epoch {epoch} ' + '-' * (80 - len(str(epoch))))\n",
    "    \n",
    "    # Training\n",
    "    start = time.time()\n",
    "    count = 0\n",
    "    per = 0\n",
    "    iterator = iter(train_loader)\n",
    "    length_iter = len(iterator)\n",
    "    for batch_num, batch in enumerate(iterator):\n",
    "        # Training mode\n",
    "        model.train()\n",
    "        # Reset Gradient from Backward Pass\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # Get input/output to encoder/decoder\n",
    "        language_input = batch[0]\n",
    "        language_output = batch[1]\n",
    "        # Get mask\n",
    "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = Masking(language_input, language_output, MAX_LENGTH)\n",
    "\n",
    "        # Predict\n",
    "        predictions = model(language_input,\n",
    "                            language_output,\n",
    "                            encoder_self_attention_mask,\n",
    "                            decoder_self_attention_mask,\n",
    "                            decoder_cross_attention_mask,\n",
    "                            encoder_start_token=False,\n",
    "                            encoder_end_token=False,\n",
    "                            decoder_start_token=True,\n",
    "                            decoder_end_token=True)\n",
    "\n",
    "        # Loss\n",
    "        Truelabels_tokens = model.decoder.output_preprocessing.batch_tokens(batch=language_output,start_token=False,end_token=True)\n",
    "\n",
    "        loss = criterian(\n",
    "            predictions.view(-1, len(vietnamese_to_index)),\n",
    "            Truelabels_tokens.view(-1).to(device)\n",
    "        ).to(device)\n",
    "        ignore_pad = torch.where(Truelabels_tokens.view(-1) == vietnamese_to_index[PADDING_TOKEN], False, True)\n",
    "        loss = loss.sum() / ignore_pad.sum()\n",
    "\n",
    "        # Backward and Optimize\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # Each 10%, model will valid \n",
    "        if count == length_iter // 10:\n",
    "            per += 1\n",
    "            print(f\"{per * 10}% Training Progress: time: {round(time.time() - start,2)} seconds - loss: {loss.item()}\")\n",
    "            start= time.time()\n",
    "            print(f\"- English Input: {language_input[0]}\")\n",
    "            print(f\"- Vietnamese True Output: {language_output[0]}\")\n",
    "            # Get Sentence of predictions\n",
    "            sentence_predict = \"\"\n",
    "            for idx in torch.argmax(predictions[0], axis=1):\n",
    "                id = int(idx)\n",
    "                if id == vietnamese_to_index[END_TOKEN]:\n",
    "                    break\n",
    "                sentence_predict += index_to_vietnamese[id]\n",
    "            print(f\"- Vietnamese Predict: {sentence_predict}\")\n",
    "            # valid_loss = validation_translator(model=model, valid_dataloader=valid_loader)\n",
    "            # print(f\"- Validation loss: time: {round(time.time() - start,2)} seconds - loss: {valid_loss}\",end=\"\\n\\n\")\n",
    "            # History\n",
    "            loss_train.append(loss.item())\n",
    "            # loss_valid.append(valid_loss)\n",
    "            count = 0\n",
    "        count += 1   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36e59f3-f90e-4c71-8284-6e1dbd1506b0",
   "metadata": {},
   "source": [
    "- Build up a Translate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "dfda3228-49bd-450a-aef5-175ecd977b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Translate(input_setence):\n",
    "    model.eval()\n",
    "    input = (input_setence,)\n",
    "    output = (\"\",)\n",
    "    for index in range(MAX_LENGTH):\n",
    "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = Masking(input, output, MAX_LENGTH)\n",
    "        # Predict\n",
    "        predictions = model(input,\n",
    "                            output,\n",
    "                            encoder_self_attention_mask,\n",
    "                            decoder_self_attention_mask,\n",
    "                            decoder_cross_attention_mask,\n",
    "                            encoder_start_token=False,\n",
    "                            encoder_end_token=False,\n",
    "                            decoder_start_token=True,\n",
    "                            decoder_end_token=False)\n",
    "        next_token_distribution = predictions[0][index]\n",
    "        next_token_index = torch.argmax(next_token_distribution)\n",
    "        next_token = index_to_vietnamese[int(next_token_index)]\n",
    "        if next_token == END_TOKEN:\n",
    "            break\n",
    "        output = (output[0] + next_token,)\n",
    "    return output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6171d303-9b92-469b-892d-3b2c4e3c0b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OOOOOOO'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Translate(\"Hey Bro pleassssase help!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a475c342-fa7f-4341-b552-41bbb0cfab2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9560501b-6d01-459f-9d39-13c360a13f51",
   "metadata": {},
   "source": [
    "- Save model and the weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0c75ef90-eb5b-4dbd-9028-de089554e196",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'translator.pth')\n",
    "torch.save(model.state_dict(), 'translator_weights.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TorchCuda",
   "language": "python",
   "name": "torchcuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
