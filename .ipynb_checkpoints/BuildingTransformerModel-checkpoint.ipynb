{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce87a780-5d95-4d1f-b044-e916bee3cd14",
   "metadata": {},
   "source": [
    "# Building Transformer Model - Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f715195-33e0-4cd2-bea6-3877af2256dd",
   "metadata": {},
   "source": [
    "![Transformer Architecture](./Transformer_Understanding/img/transformerblock.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60ca720-9523-4748-a135-504f14310a63",
   "metadata": {},
   "source": [
    "Base on the upper picture, we will make our model with Transformer Architecture following the things we have been researching in the \"[Transformer Understanding](./Transformer_Understanding/TransformerNeuralNetworks.ipynb)\" part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8398fa25-9656-40a2-8a69-eb01e46cb1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99343a2f-e0c9-4aa9-a6b1-2d101f8132cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d0c0e63-2dca-4b55-bbc3-3f51283811c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. PyTorch is using GPU.\n",
      "Number of GPUs available:  1\n",
      "GPU name:  NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. PyTorch is using GPU.\")\n",
    "    print(\"Number of GPUs available: \", torch.cuda.device_count())\n",
    "    print(\"GPU name: \", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch is using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44143e5c-22f3-4b4b-ba98-850d5512a972",
   "metadata": {},
   "source": [
    "## Start with these small blocks, functions, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaa117c-7311-44bd-a4e4-a7b4a44c2eec",
   "metadata": {},
   "source": [
    "- **Token Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8baa904-44bb-44ff-ae49-90ff801da980",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom Token Embedding Layer\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size,d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Converting tokens to embedding vectors\n",
    "        :param x: Tensor contain tokens with shape (sequence_length)\n",
    "        :return: Tensor contain embedding tokens with shape (sequence_length, d_model)\n",
    "        \"\"\"\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc915e44-67a7-42fc-9897-4dc5b76eb6b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1881,  1.4106, -0.0508,  ..., -0.0776,  0.0657, -1.6559],\n",
       "        [ 0.2590, -0.9922,  1.7796,  ..., -0.6304,  0.6049, -1.0139],\n",
       "        [-1.2311,  1.1663,  0.7043,  ..., -0.4209,  0.2823, -0.1984]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For example\n",
    "ex_token_emb = TokenEmbedding(1000, 512)\n",
    "ex_token_emb(torch.tensor([1,3,5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96829a7-6706-45a0-b4ee-be3fa0a4bcac",
   "metadata": {},
   "source": [
    "- **Positional Encodings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc315389-fd3c-4e1a-b581-71e4e1dbceb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom Positional Encoding Layer\n",
    "    \"\"\"\n",
    "    def __init__(self, max_sequence_length, d_model):\n",
    "        super().__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Creating a Tensor contain Positional Encoding\n",
    "        :return: Tensor contain Positional Encoding with shape (max_sequence_length, d_model)\n",
    "        \"\"\"\n",
    "        even_index = torch.arange(0,self.d_model,2).float()\n",
    "        odd_index = torch.arange(1,self.d_model,2).float()\n",
    "        even_denominator = torch.pow(10000,even_index/self.d_model)\n",
    "        odd_denominator = torch.pow(10000,(odd_index-1)/self.d_model)\n",
    "        position = torch.arange(0,self.max_sequence_length,1).unsqueeze(1)\n",
    "        PE = torch.zeros(self.max_sequence_length,self.d_model)\n",
    "        PE[:,0::2] = torch.sin(position / even_denominator)\n",
    "        PE[:,1::2] = torch.cos(position / odd_denominator)\n",
    "        return PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88685a40-5a14-490a-8060-daf4e3b228da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "          0.0000e+00,  1.0000e+00],\n",
       "        [ 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,\n",
       "          1.0366e-04,  1.0000e+00],\n",
       "        [ 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,\n",
       "          2.0733e-04,  1.0000e+00]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Example\n",
    "ex_pe = PositionalEncoding(3, 512)\n",
    "ex_pe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173b4a28-fb8b-4399-87a7-6e639bce61f8",
   "metadata": {},
   "source": [
    "- **Query-Key-Value Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0d90dda-8fd8-404b-bbd9-e33273890167",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QKVLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.linear_layer = nn.Linear(d_model, d_model * 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, sequence_length, d_model = x.size()\n",
    "        qkv = self.linear_layer(x)\n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        return q, k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3214692a-60ea-4c99-b349-5b632076fb7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2, 10, 5]),\n",
       " torch.Size([2, 2, 10, 5]),\n",
       " torch.Size([2, 2, 10, 5]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Example\n",
    "ex_qkv = QKVLayer(10, 2)\n",
    "q,k,v = ex_qkv(torch.randn(2,10,10))\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7606b106-b21c-4d61-8a5f-cfd0123b1de1",
   "metadata": {},
   "source": [
    "- **Multi-Head Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "979f0ca7-888c-49d0-80d5-bbca4fa33848",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = QKVLayer(d_model,num_heads)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self,x, mask=None):\n",
    "        def ScaledDotProduct(q, k, v, mask=None):\n",
    "            d_k = q.size()[-1]\n",
    "            scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "            if mask is not None:\n",
    "                scaled = scaled.permute(1, 0, 2, 3) + mask\n",
    "                scaled = scaled.permute(1, 0, 2, 3)\n",
    "            attention = F.softmax(scaled, dim=-1)\n",
    "            values = torch.matmul(attention, v)\n",
    "            return values, attention\n",
    "\n",
    "        batch_size, sequence_length, d_model = x.size()\n",
    "        q, k, v = self.qkv_layer.forward(x)\n",
    "        values, attention = ScaledDotProduct(q, k, v, mask)\n",
    "        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
    "        out = self.linear_layer(values)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c25eae2-3aab-49ed-9e63-6df247cc8dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3140, -0.3788, -0.5676, -0.1718,  0.1243,  0.1269, -0.2300,\n",
       "           0.1349, -0.4820,  0.2326],\n",
       "         [-0.2262, -0.2845, -0.4488, -0.3127,  0.0116,  0.0524, -0.0241,\n",
       "           0.1813, -0.4163,  0.0696],\n",
       "         [-0.2650, -0.3136, -0.4473, -0.2717,  0.0507,  0.0525, -0.0499,\n",
       "           0.2105, -0.4399,  0.0838],\n",
       "         [-0.4085, -0.3837, -0.4819, -0.1319,  0.1462,  0.0694, -0.1749,\n",
       "           0.1999, -0.5573,  0.1795],\n",
       "         [-0.3296, -0.2547, -0.4988, -0.1962, -0.0279,  0.1040, -0.0832,\n",
       "           0.0714, -0.4798,  0.0864],\n",
       "         [-0.3184, -0.2965, -0.4904, -0.2107,  0.0581,  0.0861, -0.1365,\n",
       "           0.1213, -0.4857,  0.1305],\n",
       "         [-0.2521, -0.3771, -0.5989, -0.2193,  0.0902,  0.1414, -0.2108,\n",
       "           0.1242, -0.4361,  0.2406],\n",
       "         [-0.2354, -0.3520, -0.5022, -0.2717,  0.0781,  0.0773, -0.0992,\n",
       "           0.2167, -0.4219,  0.1233],\n",
       "         [-0.4018, -0.1970, -0.5149, -0.1610, -0.0563,  0.0993, -0.1292,\n",
       "          -0.0429, -0.5624,  0.0882],\n",
       "         [-0.2207, -0.3602, -0.4255, -0.2920,  0.1614,  0.0403, -0.0940,\n",
       "           0.2872, -0.4233,  0.0764]],\n",
       "\n",
       "        [[-0.2484, -0.2738, -0.4540, -0.2784, -0.1254,  0.0653,  0.1976,\n",
       "           0.4940, -0.2432, -0.1620],\n",
       "         [-0.3227, -0.3443, -0.4928, -0.2012, -0.1005,  0.0827,  0.1708,\n",
       "           0.5103, -0.2941, -0.0890],\n",
       "         [-0.2864, -0.2610, -0.4608, -0.2802, -0.2189,  0.0168,  0.2297,\n",
       "           0.4516, -0.2707, -0.1247],\n",
       "         [-0.1886, -0.2523, -0.4765, -0.2494, -0.1194,  0.1237,  0.1712,\n",
       "           0.4919, -0.1752, -0.2651],\n",
       "         [-0.2991, -0.2990, -0.4446, -0.2442, -0.1479,  0.0394,  0.2060,\n",
       "           0.4640, -0.2834, -0.0943],\n",
       "         [-0.2294, -0.3221, -0.5222, -0.1630, -0.0406,  0.1909,  0.1159,\n",
       "           0.5112, -0.1988, -0.2208],\n",
       "         [-0.1908, -0.1512, -0.4195, -0.3952, -0.2891, -0.0317,  0.2747,\n",
       "           0.4703, -0.1983, -0.2817],\n",
       "         [-0.2696, -0.3890, -0.5323, -0.0959,  0.0575,  0.2188,  0.0555,\n",
       "           0.5556, -0.2316, -0.1864],\n",
       "         [-0.1755, -0.1894, -0.4296, -0.3386, -0.2245,  0.0464,  0.2546,\n",
       "           0.4364, -0.1779, -0.2431],\n",
       "         [-0.2083, -0.2324, -0.4544, -0.2792, -0.1742,  0.0781,  0.2110,\n",
       "           0.4599, -0.1965, -0.2261]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Example\n",
    "ex_multihead = MultiHeadAttention(10,2)\n",
    "ex_multihead(torch.randn(2,10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08568fc9-6e53-4fd7-9bfb-91317bdcd196",
   "metadata": {},
   "source": [
    "- **Normalization Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd44d054-eba7-4cf8-937b-60ac2ebab419",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape=parameters_shape\n",
    "        self.eps=eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
    "        mean = inputs.mean(dim=dims, keepdim=True)\n",
    "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        y = (inputs - mean) / std\n",
    "        out = self.gamma * y + self.beta\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e47d7b2d-58fb-4ac6-9aed-2d4039bbe250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9756, -1.4953,  0.8961, -0.1627],\n",
       "         [ 0.5595, -0.1139,  1.2191,  0.5873],\n",
       "         [ 1.6807,  0.1400, -1.5243, -0.8109]],\n",
       "\n",
       "        [[-1.9675, -0.3757,  0.2315,  0.0231],\n",
       "         [-0.4022,  1.2492,  0.9361, -0.5800],\n",
       "         [ 1.8513,  0.1362,  0.1366, -1.2385]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Example\n",
    "LayerNormalization(parameters_shape=(3, 4))(torch.randn(2, 3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39688d56-38fa-470d-8b34-d25d10ff341a",
   "metadata": {},
   "source": [
    "- **Feed Forward Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b395d9c6-3cbb-4431-b2a8-ea5e834bed0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TorchCuda",
   "language": "python",
   "name": "torchcuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
