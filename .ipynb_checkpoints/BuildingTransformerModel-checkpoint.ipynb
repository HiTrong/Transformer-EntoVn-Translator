{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce87a780-5d95-4d1f-b044-e916bee3cd14",
   "metadata": {},
   "source": [
    "# Building Transformer Model - Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f715195-33e0-4cd2-bea6-3877af2256dd",
   "metadata": {},
   "source": [
    "![Transformer Architecture](./Transformer_Understanding/img/transformerblock.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60ca720-9523-4748-a135-504f14310a63",
   "metadata": {},
   "source": [
    "Base on the upper picture, we will make our model functions with Transformer Architecture following the things we have been researching in the \"[Transformer Understanding](./Transformer_Understanding/TransformerNeuralNetworks.ipynb)\" part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8398fa25-9656-40a2-8a69-eb01e46cb1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import copy\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99343a2f-e0c9-4aa9-a6b1-2d101f8132cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d0c0e63-2dca-4b55-bbc3-3f51283811c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. PyTorch is using GPU.\n",
      "Number of GPUs available:  1\n",
      "GPU name:  NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. PyTorch is using GPU.\")\n",
    "    print(\"Number of GPUs available: \", torch.cuda.device_count())\n",
    "    print(\"GPU name: \", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch is using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44143e5c-22f3-4b4b-ba98-850d5512a972",
   "metadata": {},
   "source": [
    "## Start with these small blocks, functions, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de739e-d0e0-4776-ac43-aa0110974965",
   "metadata": {},
   "source": [
    "- **Get Using Device** (Torch requires all tensor  must be in the same device.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb05b66a-9351-4aa7-9276-f6bb32178767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cb8fed-58b8-4375-bc10-6134d4894127",
   "metadata": {},
   "source": [
    "- **Token Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ab64882-f3ce-4a20-8cb9-5f62b81df436",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        \"\"\"\n",
    "        Token Embedding is used for converting a word / token into a embedding numeric vector space.\n",
    "        \n",
    "        :param vocab_size: Number of words / token in vocabulary\n",
    "        :param d_model: The embedding dimension\n",
    "        \n",
    "        Example: With 1000 words in vocabulary and our embedding dimension is 512, the Token Embedding layer will be 1000x512\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: the word or sequence of words\n",
    "        :return: the numerical representation of the input\n",
    "        \n",
    "        Example:\n",
    "        Input: (Batch_size, Sequence of words) - (30x100)\n",
    "        Output: (Batch_size, Sequence of words, d_model) - (30x100x512)\n",
    "        \"\"\"\n",
    "        x = self.embedding_layer(x)\n",
    "        return x.to(get_device())\n",
    "\n",
    "# Or just Simple\n",
    "# token_embedding = nn.Embedding(vocab_size, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7837bba9-74c4-417f-8555-370e6923fa27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 100, 512])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Example\n",
    "vocab_size = 1000\n",
    "d_model = 512\n",
    "\n",
    "embedding_layer = TokenEmbedding(vocab_size, d_model)\n",
    "input_data = torch.randint(0, vocab_size, (30, 100))\n",
    "embedding_layer(input_data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e47d72-f888-491c-82ab-e3ea6a67f6c1",
   "metadata": {},
   "source": [
    "- **Positional Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b935a5a-5a1a-47fe-a6d9-b234e7f96055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, max_sequence_length, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Positional Encoding layer for adding positional information to token embeddings.\n",
    "        \n",
    "        :param d_model: The embedding dimension.\n",
    "        :param max_sequence_length: The maximum length of the input sequences.\n",
    "        :param dropout: Dropout rate.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding,self).__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self):\n",
    "        even_i = torch.arange(0, self.d_model, 2).float()\n",
    "        denominator = torch.pow(10000, even_i/self.d_model)\n",
    "        position = (torch.arange(self.max_sequence_length)\n",
    "                          .reshape(self.max_sequence_length, 1))\n",
    "        even_PE = torch.sin(position / denominator)\n",
    "        odd_PE = torch.cos(position / denominator)\n",
    "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
    "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "        PE = PE.unsqueeze(0)\n",
    "        return self.dropout(PE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0565979b-3e58-49a6-8fd4-4a375bd24df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 512])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PE = PositionalEncoding(512,100,0.1)\n",
    "PE().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00ab53f-2eb1-4237-931f-4bc4f65fac9f",
   "metadata": {},
   "source": [
    "- **Multi-Head Attention**\n",
    "\n",
    "2 options for: 'encoder' and 'decoder' (**Multi-Head Cross Attention**)\n",
    "\n",
    "options for mask: **None**, **Self-Attention Mask** (**Causal Mask** hoáº·c **Look-Ahead Mask**), **Padding Mask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e69746d2-092e-4b4b-a451-4f838cb03d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads=8, cross=False):\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        :param d_model: the embedding dimension\n",
    "        :param num_heads: the number of heads, default equals 8\n",
    "        :param cross: True for Multi-Head Cross Attention, False for Multi-Head Attention only\n",
    "        \n",
    "        # note: The embedding dimension must be divided by the number of heads\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.cross = cross\n",
    "\n",
    "        # query, key value layer\n",
    "        if self.cross: # Multi-Head Cross Attention\n",
    "            self.kv_layer = nn.Linear(d_model , 2 * d_model)\n",
    "            self.q_layer = nn.Linear(d_model , d_model)\n",
    "        else:\n",
    "            self.qkv_layer = nn.Linear(d_model , 3 * d_model)\n",
    "        \n",
    "        \n",
    "        # method 1: old, cost alot\n",
    "        # self.query = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        # self.key = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        # self.value = nn.Linear(self.head_dim, self.head_dim, bias=False) \n",
    "\n",
    "        # method 2: the fewer linear layers the better the cost\n",
    "        \n",
    "        \n",
    "        # Linear Layer in Multi-Head Attention\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product(self, q, k, v, mask=None):\n",
    "        d_k = q.size()[-1]\n",
    "        scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scaled = scaled.permute(1, 0, 2, 3) + mask\n",
    "            scaled = scaled.permute(1, 0, 2, 3)\n",
    "        attention = F.softmax(scaled, dim=-1)\n",
    "        values = torch.matmul(attention, v)\n",
    "        return values, attention\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Perform forward pass of the multi-head attention mechanism.\n",
    "\n",
    "        :param x: if cross is True then x is a dictionary including  'encoder_output' and 'w'.\n",
    "        :param mask: Optional mask tensor\n",
    "        \n",
    "        :return: Output tensor of shape (batch_size, length_seq, d_model)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # For MultiHead Cross Attention\n",
    "        if self.cross:\n",
    "            encoder_output = x['encoder_output']\n",
    "            w = x['w']\n",
    "            batch_size, length_seq, d_model = w.size()\n",
    "            kv = self.kv_layer(w)\n",
    "            q = self.q_layer(encoder_output)\n",
    "            kv = kv.reshape(batch_size, length_seq, self.num_heads, 2 * self.head_dim)\n",
    "            q = q.reshape(batch_size, length_seq, self.num_heads, self.head_dim)\n",
    "            kv = kv.permute(0, 2, 1, 3)\n",
    "            q = q.permute(0, 2, 1, 3)\n",
    "            k, v = kv.chunk(2, dim=-1)\n",
    "            values, attention = self.scaled_dot_product(q, k, v, mask) # mask is not required in Cross Attention\n",
    "            values = values.permute(0, 2, 1, 3).reshape(batch_size, length_seq, self.num_heads * self.head_dim)\n",
    "            out = self.linear_layer(values)\n",
    "            return out\n",
    "\n",
    "        # For MultiHead Attention\n",
    "        batch_size, length_seq, d_model = x.size()\n",
    "        qkv = self.qkv_layer(x)\n",
    "        qkv = qkv.reshape(batch_size, length_seq, self.num_heads, 3 * self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        values, attention = self.scaled_dot_product(q, k, v, mask)\n",
    "        values = values.permute(0, 2, 1, 3).reshape(batch_size, length_seq, self.num_heads * self.head_dim)\n",
    "        out = self.linear_layer(values)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8207b97a-4398-4057-91a1-0c02dd9e315d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0229,  0.3181, -0.0215,  ...,  0.0259,  0.1319, -0.0412],\n",
       "         [-0.0419,  0.2770,  0.0670,  ...,  0.0602,  0.1198, -0.0771],\n",
       "         [-0.0457,  0.2849,  0.0391,  ...,  0.0590,  0.0696, -0.0897],\n",
       "         ...,\n",
       "         [ 0.0452,  0.2738, -0.0414,  ...,  0.0301,  0.0869, -0.0696],\n",
       "         [ 0.0180,  0.2401, -0.0527,  ...,  0.0451,  0.0974, -0.0906],\n",
       "         [ 0.0140,  0.3105, -0.0263,  ...,  0.0064,  0.0948, -0.0760]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Example\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "\n",
    "\n",
    "# Attention\n",
    "mha_layer = MultiHeadAttention(d_model, num_heads)\n",
    "mha_layer(torch.randn(1,10,d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2f3315a-bff2-435c-9f9a-54b37630fc38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1622, -0.1015,  0.0889,  ..., -0.0870,  0.0573, -0.0422],\n",
       "         [-0.1445, -0.1074,  0.0502,  ..., -0.0585,  0.0648, -0.0502],\n",
       "         [-0.1760, -0.1166,  0.1122,  ..., -0.0488,  0.0623, -0.0487],\n",
       "         ...,\n",
       "         [-0.1120, -0.0916,  0.0743,  ..., -0.0733,  0.0644, -0.0471],\n",
       "         [-0.1116, -0.0606,  0.0635,  ..., -0.0384,  0.0473, -0.0808],\n",
       "         [-0.1102, -0.0878,  0.0236,  ..., -0.0136,  0.0609, -0.0803]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross Attention\n",
    "mha_layer = MultiHeadAttention(d_model, num_heads,cross=True)\n",
    "mha_layer({'encoder_output':torch.randn(1,10,d_model),'w':torch.randn(1,10,d_model)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52d1ed4-ff0d-4988-b5b0-82e964758ff5",
   "metadata": {},
   "source": [
    "- **Layer Normalization Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d640c067-39fa-4181-ad87-6a6bb6cadca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape=parameters_shape\n",
    "        self.eps=eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
    "        mean = inputs.mean(dim=dims, keepdim=True)\n",
    "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        y = (inputs - mean) / std\n",
    "        out = self.gamma * y  + self.beta\n",
    "        return out\n",
    "\n",
    "# Or using nn.LayerNorm(d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "276d988f-e30e-4436-8a06-3ef7c11f13f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.6767, -0.5315, -1.1357],\n",
       "         [ 0.7328,  0.2627, -1.0050]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Example\n",
    "ln = LayerNormalization((1,2,3))\n",
    "ln(torch.randn(1,2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bb68ec-eaf5-40e6-8ac2-21ab67b3e1a0",
   "metadata": {},
   "source": [
    "- **Positionwise Feed Forward Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79967c5c-330f-4ba9-ac6d-1cd1512843ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "# feed_forward = nn.Sequential(\n",
    "#     nn.Linear(d_model, expansion_factor * d_model),  # e.g: 512x(4*512) -> (512, 2048)\n",
    "#     nn.ReLU(),  # ReLU activation function\n",
    "#     nn.Linear(d_model * expansion_factor, d_model),  # e.g: 4*512)x512 -> (2048, 512)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9e15943-748f-45fa-a6de-41e33d52e6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 512])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Example\n",
    "ff = PositionwiseFeedForward(512, 300)\n",
    "ff(torch.randn(1,5,512)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2b3196-79bd-4808-bb96-ad2cab19acae",
   "metadata": {},
   "source": [
    "- **Copy Block Function**: we can use nn.Sequential but i think we don't need to do that because we don't have any changes in Module Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51af5dd9-2ae2-4f58-969a-5cd9e3e45c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replicate(block, N=6) -> nn.ModuleList:\n",
    "    \"\"\"\n",
    "    Method to replicate the existing block to N set of blocks\n",
    "    :param block: class inherited from nn.Module, mainly it is the encoder or decoder part of the architecture\n",
    "    :param N: the number of stack, in the original paper they used 6\n",
    "    :return: a set of N blocks\n",
    "    \"\"\"\n",
    "    block_stack = nn.ModuleList([copy.deepcopy(block) for _ in range(N)])\n",
    "    return block_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cd5d10-d943-4116-8d07-f32885077b2a",
   "metadata": {},
   "source": [
    "## With those small blocks and functions, let's build these important blocks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f430efe3-685e-485d-b44d-48af49b286d7",
   "metadata": {},
   "source": [
    "- **Preprocessing** for Input Pre-processing and Output Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f5b0acc-5acf-4a60-b2a7-76592cddd9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing(nn.Module):\n",
    "\n",
    "    def __init__(self, max_length_seq, d_model, language_to_index, start_token, end_token, pad_token, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = len(language_to_index)\n",
    "        self.language_to_index = language_to_index\n",
    "        self.max_length_seq = max_length_seq\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "        # Layer\n",
    "        self.token_embedding = TokenEmbedding(self.vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_length_seq, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def tokenize(self, sentence, start_token:bool, end_token:bool):\n",
    "        encode_char = [self.language_to_index[token] for token in list(sentence)]\n",
    "        if start_token:\n",
    "            encode_char.insert(0, self.language_to_index[self.start_token])\n",
    "        if end_token:\n",
    "            encode_char.append(self.language_to_index[self.end_token])\n",
    "        for _ in range(len(encode_char), self.max_length_seq):\n",
    "            encode_char.append(self.language_to_index[self.pad_token])\n",
    "        return torch.tensor(encode_char)\n",
    "    \n",
    "    def batch_tokens(self, batch, start_token:bool, end_token:bool):\n",
    "        tokens = []\n",
    "        for i in range(len(batch)):\n",
    "            tokens.append(self.tokenize(batch[i], start_token, end_token))\n",
    "        tokens = torch.stack(tokens)\n",
    "        return tokens.to(get_device())\n",
    "\n",
    "    def forward(self, x, start_token:bool, end_token:bool): \n",
    "        x = self.batch_tokens(x, start_token, end_token)\n",
    "        x = self.token_embedding(x)\n",
    "        pos = self.positional_encoding().to(get_device())\n",
    "        x = self.dropout(x + pos)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf2e50e-5dce-46d5-b9c5-f5145367baf5",
   "metadata": {},
   "source": [
    "- **Transformer Block** includes: **Multi-Head Attention**, **Add & Norm**, **Feed & Forward** and **Dropout**\n",
    "\n",
    "2 options: 'Encoder' and 'Decoder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "852e35db-7a0f-4804-b164-fb36427c7677",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model=512,\n",
    "                 num_heads=8,\n",
    "                 ff_hidden=300,\n",
    "                 dropout=0.1,\n",
    "                 options='encoder'\n",
    "                ):\n",
    "        \"\"\"\n",
    "        The Transformer Block used in the encoder and decoder as well\n",
    "\n",
    "        :param d_model: the embedding dimension\n",
    "        :param num_heads: the number of heads\n",
    "        :param ff_hidden: The output dimension of the feed forward layer\n",
    "        :param dropout: probability dropout (between 0 and 1)\n",
    "        :param options: The choice between 'encoder' and 'decoder'\n",
    "        \"\"\"\n",
    "        super(TransformerBlock, self).__init__()\n",
    "    \n",
    "        self.options = options\n",
    "        \n",
    "        # For both 2 options: encoder and decoder\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm_for_attention = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout_attention = nn.Dropout(dropout)\n",
    "\n",
    "        self.ff = PositionwiseFeedForward(d_model=d_model, hidden=ff_hidden, drop_prob=dropout)\n",
    "        self.norm_for_ff = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout_for_ff = nn.Dropout(dropout)\n",
    "        \n",
    "        # For decoder\n",
    "        if self.options=='decoder':\n",
    "            self.cross_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads, cross=True)\n",
    "            self.norm_for_cross_attention = LayerNormalization(parameters_shape=[d_model])\n",
    "            self.dropout2 = nn.Dropout(dropout)\n",
    "        elif self.options!='encoder':\n",
    "            raise Exception(f\"Unknown option {options}\")\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # For decoder\n",
    "        if self.options == 'decoder':\n",
    "            encoder_output = x['encoder_output']\n",
    "            w = x['w']\n",
    "            w_residual = w.clone()\n",
    "            w = self.attention(w,mask['self_attention_mask'])\n",
    "            w = self.dropout_attention(w)\n",
    "            w = self.norm_for_attention(w + w_residual)\n",
    "\n",
    "            w_residual = w.clone()\n",
    "            w = self.cross_attention({'encoder_output':encoder_output,'w':w},mask['cross_attention_mask'])\n",
    "            w = self.dropout2(w)\n",
    "            w = self.norm_for_cross_attention(w + w_residual)\n",
    "\n",
    "            w_residual = w.clone()\n",
    "            w = self.ff(w)\n",
    "            w = self.dropout_for_ff(w)\n",
    "            w = self.norm_for_ff(w + w_residual)\n",
    "            return w\n",
    "        else:\n",
    "        # For encoder\n",
    "            x_residual = x.clone()\n",
    "            x = self.attention(x, mask)\n",
    "            x = self.dropout_attention(x)\n",
    "            x = self.norm_for_attention(x + x_residual)\n",
    "\n",
    "            x_residual = x.clone()\n",
    "            x = self.ff(x)\n",
    "            x = self.dropout_for_ff(x)\n",
    "            x = self.norm_for_ff(x + x_residual)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "db60b8d3-ed32-44fa-83dd-637d7a202e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-6.7121e-01, -7.4082e-01, -7.5190e-01,  ...,  2.9882e-01,\n",
       "           5.3573e-01, -8.3785e-01],\n",
       "         [-1.4255e-01, -1.3426e+00,  4.7916e-01,  ...,  8.8798e-01,\n",
       "          -1.3014e+00,  2.9261e-03],\n",
       "         [-1.4443e+00, -9.3250e-01,  1.2280e+00,  ..., -1.1358e+00,\n",
       "          -2.5559e-01, -1.8362e+00],\n",
       "         ...,\n",
       "         [ 1.1481e+00,  8.5070e-02, -5.9660e-01,  ...,  1.5729e+00,\n",
       "          -7.7696e-02,  8.8113e-01],\n",
       "         [ 9.1666e-01,  5.7564e-01, -2.0591e+00,  ...,  1.4346e+00,\n",
       "           4.7820e-01, -8.5344e-01],\n",
       "         [-4.2563e-01,  3.1921e+00, -1.1351e+00,  ...,  8.1564e-01,\n",
       "          -1.2791e+00,  6.5261e-01]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "trans_block = TransformerBlock()\n",
    "trans_block(torch.randn(1,10,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8bc025b5-d22e-4a7a-91db-1cbdcb91895e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6512,  0.5878,  0.1309,  ...,  0.6954,  0.0358, -0.2004],\n",
       "         [ 1.1554, -1.0452, -0.0026,  ...,  0.1563, -0.9811, -0.5007],\n",
       "         [ 0.4417,  1.1563, -1.4752,  ..., -1.5348,  0.6136, -0.1436],\n",
       "         ...,\n",
       "         [-0.9515, -0.9044,  0.1165,  ..., -0.1490,  0.5223,  0.7547],\n",
       "         [-0.4491, -0.6069,  1.8417,  ...,  0.5312, -1.4932, -0.9565],\n",
       "         [ 0.4803,  1.2127, -0.4004,  ...,  0.2136,  0.6329, -0.1688]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_block = TransformerBlock(options='decoder')\n",
    "trans_block({'encoder_output':torch.randn(1,10,d_model),'w':torch.randn(1,10,d_model)},\n",
    "    {'self_attention_mask': None, 'cross_attention_mask': None}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0145ab0a-1473-4b97-8ce0-4f716f731674",
   "metadata": {},
   "source": [
    "- **Encoder** includes Input Pre-processing (**Token Embedding** & **Positional Encoding**) and N **Transformer Block** (Encode block in the picture on the top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d59c851-23c2-412a-b9b9-e9024bd74a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 ff_hidden,\n",
    "                 num_heads,\n",
    "                 dropout,\n",
    "                 num_blocks,\n",
    "                 max_length_seq,\n",
    "                 language_to_index,\n",
    "                 start_token, \n",
    "                 end_token, \n",
    "                 pad_token\n",
    "                ):\n",
    "        \"\"\"\n",
    "        The Encoder part of the Transformer architecture\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Layer\n",
    "        self.input_preprocessing = Preprocessing(max_length_seq, d_model, language_to_index, start_token, end_token, pad_token, dropout)\n",
    "        \n",
    "        # Transformer Blocks\n",
    "        self.transformer_blocks = replicate(TransformerBlock(d_model, num_heads, ff_hidden, dropout, options=\"encoder\"),num_blocks)\n",
    "\n",
    "    def forward(self, x, self_attention_mask, start_token:bool, end_token:bool):\n",
    "        # Input Pre-processing: Token Embedding + Positional Encoding\n",
    "        out = self.input_preprocessing(x, start_token, end_token)\n",
    "\n",
    "        # Go to Transformer Blocks (Encode)\n",
    "        for block in self.transformer_blocks:\n",
    "            out = block(out, self_attention_mask)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf4215e2-b0c5-486d-a4b1-1dc8e3555209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6574,  0.9927,  0.3610,  ...,  0.8072, -0.1423,  0.5075],\n",
       "         [-0.3589, -0.4645, -0.4618,  ..., -1.4731, -0.8579, -0.5986],\n",
       "         [-0.5117, -0.9965,  0.6796,  ..., -0.4416, -2.3177,  0.0577],\n",
       "         ...,\n",
       "         [ 0.3102, -2.5761,  0.1540,  ...,  0.3245,  0.1516,  1.6019],\n",
       "         [-0.6325, -2.8122,  0.6494,  ...,  0.9408,  0.3387, -0.1487],\n",
       "         [-0.5644, -1.7319,  1.4319,  ...,  1.2308,  0.4343,  0.3827]],\n",
       "\n",
       "        [[-0.9519,  0.0186, -0.8765,  ...,  0.9950, -0.4711, -0.3555],\n",
       "         [-0.1444, -0.9346,  0.5741,  ...,  0.5381, -0.7798, -0.9178],\n",
       "         [ 0.6244, -1.1338,  0.9847,  ...,  1.0216, -0.6965, -0.0603],\n",
       "         ...,\n",
       "         [ 0.0494, -2.3624, -0.3773,  ..., -0.0252,  0.5539,  1.3958],\n",
       "         [-0.9273, -2.5355,  0.8928,  ...,  0.7336,  0.5160,  1.4160],\n",
       "         [-0.9643, -1.7177,  1.7249,  ...,  0.7898,  0.3357,  0.3674]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "special_tokens = [\"<start>\", \"<end>\", \"<pad>\"]\n",
    "alphabet = list(\"abcdefghijklmnopqrstuvwxyz \") + special_tokens\n",
    "language_to_index = {word: idx for idx, word in enumerate(alphabet)}\n",
    "encoder = Encoder(d_model=100, ff_hidden=50, num_heads=2, dropout=0.1 ,num_blocks=1, max_length_seq=100, language_to_index=language_to_index, start_token=\"<start>\", end_token=\"<end>\", pad_token=\"<pad>\")\n",
    "encoder.to(get_device())\n",
    "batch = ['hello','goodbye']\n",
    "encoder(batch, None, False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58513193-9156-41c9-8861-6464a3ae3341",
   "metadata": {},
   "source": [
    "- **Decoder** includes **Output Pre-processing** (**Token Embedding** & **Positional Encoding**), N **Transformer Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64c335b7-26ce-447f-96d3-449efe7bf12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 ff_hidden,\n",
    "                 num_heads,\n",
    "                 dropout,\n",
    "                 num_blocks,\n",
    "                 max_length_seq,\n",
    "                 language_to_index,\n",
    "                 start_token, \n",
    "                 end_token, \n",
    "                 pad_token\n",
    "                ):\n",
    "        \"\"\"\n",
    "        The Decoder part of the Transformer architecture\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "         # Layer\n",
    "        self.output_preprocessing = Preprocessing(max_length_seq, d_model, language_to_index, start_token, end_token, pad_token, dropout)\n",
    "        \n",
    "        # Transformer Blocks\n",
    "        self.transformer_blocks = replicate(TransformerBlock(d_model, num_heads, ff_hidden, dropout, options=\"decoder\"),num_blocks)\n",
    "\n",
    "    def forward(self, x, y, self_attention_mask, cross_attention_mask, start_token:bool, end_token:bool): \n",
    "        # x is output, y is output from encoder\n",
    "        # Output Pre-processing: Token Embedding + Positional Encoding\n",
    "        x = self.output_preprocessing(x, start_token, end_token)\n",
    "\n",
    "        # Go to Transformer Blocks (Decode)\n",
    "        encode_decode = {'encoder_output': y,'w':x}\n",
    "        mask = {'self_attention_mask': self_attention_mask,'cross_attention_mask': cross_attention_mask}\n",
    "        for block in self.transformer_blocks:\n",
    "            encode_decode['w'] = x\n",
    "            x = block(encode_decode, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadb9ff7-e4f5-4a9c-b1c3-2c434216fe36",
   "metadata": {},
   "source": [
    "## Finally, The Transformer Architecture is complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "724efbec-53d8-4efd-9469-58df2f117555",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 ff_hidden,\n",
    "                 num_heads,\n",
    "                 dropout,\n",
    "                 num_blocks,\n",
    "                 max_length_seq,\n",
    "                 language_to_index,\n",
    "                 target_language_to_index,\n",
    "                 start_token, \n",
    "                 end_token, \n",
    "                 pad_token\n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Device\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            d_model=d_model,\n",
    "            ff_hidden=ff_hidden,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            num_blocks=num_blocks,\n",
    "            max_length_seq=max_length_seq,\n",
    "            language_to_index=language_to_index,\n",
    "            start_token=start_token,\n",
    "            end_token=end_token,\n",
    "            pad_token=pad_token\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(\n",
    "            d_model=d_model,\n",
    "            ff_hidden=ff_hidden,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            num_blocks=num_blocks,\n",
    "            max_length_seq=max_length_seq,\n",
    "            language_to_index=target_language_to_index,\n",
    "            start_token=start_token,\n",
    "            end_token=end_token,\n",
    "            pad_token=pad_token\n",
    "        )\n",
    "\n",
    "        # Linear Layer\n",
    "        self.linear = nn.linear(d_model, len(target_language_to_index))\n",
    "\n",
    "        # Softmax\n",
    "        \n",
    "\n",
    "    def forward(self,\n",
    "               x,\n",
    "               y,\n",
    "               encoder_self_attention_mask=None,\n",
    "               decoder_self_attention_mask=None, \n",
    "            decoder_cross_attention_mask=None,):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afa02306-57b4-481e-84de-0fe43bb2de9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 12]) torch.Size([2, 12])\n",
      "Transformer(\n",
      "  (encoder): Encoder(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (token_emb): TokenEmbedding(\n",
      "      (embedding_layer): Embedding(11, 512)\n",
      "    )\n",
      "    (pos_encode): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (transformer_blocks): ModuleList(\n",
      "      (0-5): 6 x TransformerBlock(\n",
      "        (multihead_attention): MultiHeadAttention(\n",
      "          (query): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (key): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (value): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (token_emb): TokenEmbedding(\n",
      "      (embedding_layer): Embedding(11, 512)\n",
      "    )\n",
      "    (pos_encode): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (decoder_blocks): ModuleList(\n",
      "      (0-5): 6 x DecoderBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (query): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (key): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (value): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (transformer_block): TransformerBlock(\n",
      "          (multihead_attention): MultiHeadAttention(\n",
      "            (query): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (key): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Sequential(\n",
      "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear_layer): Linear(in_features=512, out_features=11, bias=True)\n",
      ")\n",
      "Output Shape: torch.Size([2, 12, 11])\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "src_vocab_size = 11\n",
    "target_vocab_size = 11\n",
    "num_blocks = 6\n",
    "seq_len = 12\n",
    "\n",
    "# let 0 be sos token and 1 be eos token\n",
    "src = torch.tensor([[0, 2, 5, 6, 4, 3, 9, 5, 2, 9, 10, 1],\n",
    "                    [0, 2, 8, 7, 3, 4, 5, 6, 7, 2, 10, 1]])\n",
    "target = torch.tensor([[0, 1, 7, 4, 3, 5, 9, 2, 8, 10, 9, 1],\n",
    "                       [0, 1, 5, 6, 2, 4, 7, 6, 2, 8, 10, 1]])\n",
    "\n",
    "print(src.shape, target.shape)\n",
    "model = Transformer(d_model=512,\n",
    "                    vocab_size=src_vocab_size,\n",
    "                    target_vocab_size=target_vocab_size,\n",
    "                    max_length_seq=seq_len,\n",
    "                    num_blocks=num_blocks,\n",
    "                    expansion_factor=4,\n",
    "                    num_heads=8\n",
    "                   )\n",
    "\n",
    "print(model)\n",
    "out = model(src, target)\n",
    "print(f\"Output Shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0154f2-2def-44ae-8aef-b36759b77127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab8045-7a8b-4c2b-97c5-2fc5748556ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TorchCuda",
   "language": "python",
   "name": "torchcuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
