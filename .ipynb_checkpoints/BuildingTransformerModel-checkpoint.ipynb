{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce87a780-5d95-4d1f-b044-e916bee3cd14",
   "metadata": {},
   "source": [
    "# Building Transformer Model - Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f715195-33e0-4cd2-bea6-3877af2256dd",
   "metadata": {},
   "source": [
    "![Transformer Architecture](./Transformer_Understanding/img/transformerblock.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60ca720-9523-4748-a135-504f14310a63",
   "metadata": {},
   "source": [
    "Base on the upper picture, we will make our model functions with Transformer Architecture following the things we have been researching in the \"[Transformer Understanding](./Transformer_Understanding/TransformerNeuralNetworks.ipynb)\" part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8398fa25-9656-40a2-8a69-eb01e46cb1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import copy\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99343a2f-e0c9-4aa9-a6b1-2d101f8132cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d0c0e63-2dca-4b55-bbc3-3f51283811c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. PyTorch is using GPU.\n",
      "Number of GPUs available:  1\n",
      "GPU name:  NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. PyTorch is using GPU.\")\n",
    "    print(\"Number of GPUs available: \", torch.cuda.device_count())\n",
    "    print(\"GPU name: \", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch is using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44143e5c-22f3-4b4b-ba98-850d5512a972",
   "metadata": {},
   "source": [
    "## Start with these small blocks, functions, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cb8fed-58b8-4375-bc10-6134d4894127",
   "metadata": {},
   "source": [
    "- **Token Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ab64882-f3ce-4a20-8cb9-5f62b81df436",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        \"\"\"\n",
    "        Token Embedding is used for converting a word / token into a embedding numeric vector space.\n",
    "        \n",
    "        :param vocab_size: Number of words / token in vocabulary\n",
    "        :param d_model: The embedding dimension\n",
    "        \n",
    "        Example: With 1000 words in vocabulary and our embedding dimension is 512, the Token Embedding layer will be 1000x512\n",
    "        \"\"\"\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: the word or sequence of words\n",
    "        :return: the numerical representation of the input\n",
    "        \n",
    "        Example:\n",
    "        Input: (Batch_size, Sequence of words) - (30x100)\n",
    "        Output: (Batch_size, Sequence of words, d_model) - (30x100x512)\n",
    "        \"\"\"\n",
    "        output = self.embedding_layer(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7837bba9-74c4-417f-8555-370e6923fa27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 100, 512])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Example\n",
    "vocab_size = 1000\n",
    "d_model = 512\n",
    "\n",
    "embedding_layer = TokenEmbedding(vocab_size, d_model)\n",
    "input_data = torch.randint(0, vocab_size, (30, 100))\n",
    "embedding_layer(input_data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e47d72-f888-491c-82ab-e3ea6a67f6c1",
   "metadata": {},
   "source": [
    "- **Positional Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b935a5a-5a1a-47fe-a6d9-b234e7f96055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, max_sequence_length, dropout=0):\n",
    "        \"\"\"\n",
    "        Positional Encoding layer for adding positional information to token embeddings.\n",
    "        \n",
    "        :param d_model: The embedding dimension.\n",
    "        :param max_sequence_length: The maximum length of the input sequences.\n",
    "        :param dropout: Dropout rate.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding,self).__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self):\n",
    "        even_i = torch.arange(0, self.d_model, 2).float()\n",
    "        denominator = torch.pow(10000, even_i/self.d_model)\n",
    "        position = (torch.arange(self.max_sequence_length)\n",
    "                          .reshape(self.max_sequence_length, 1))\n",
    "        even_PE = torch.sin(position / denominator)\n",
    "        odd_PE = torch.cos(position / denominator)\n",
    "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
    "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "        PE = PE.unsqueeze(0)\n",
    "        return self.dropout(PE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0565979b-3e58-49a6-8fd4-4a375bd24df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PE = PositionalEncoding(512,100,0.1)\n",
    "PE().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00ab53f-2eb1-4237-931f-4bc4f65fac9f",
   "metadata": {},
   "source": [
    "- **Multi-Head Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e69746d2-092e-4b4b-a451-4f838cb03d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads=8):\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        :param d_model: the embedding dimension\n",
    "        :param num_heads: the number of heads, default equals 8\n",
    "        \n",
    "        # note: The embedding dimension must be divided by the number of heads\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # query, key value\n",
    "        self.query = nn.Linear(self.head_dim, self.head_dim, bias=False)  # the Query metrix\n",
    "        self.key = nn.Linear(self.head_dim, self.head_dim, bias=False)  # the Key metrix\n",
    "        self.value = nn.Linear(self.head_dim, self.head_dim, bias=False)  # the Value metrix\n",
    "        \n",
    "        \n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Perform forward pass of the multi-head attention mechanism.\n",
    "\n",
    "        :param query: Query tensor of shape (batch_size, q_len, d_model)\n",
    "        :param key: Key tensor of shape (batch_size, k_len, d_model)\n",
    "        :param value: Value tensor of shape (batch_size, v_len, d_model)\n",
    "        :param mask: Optional mask tensor of shape (batch_size, 1, 1, k_len)\n",
    "        \n",
    "        :return: Output tensor of shape (batch_size, q_len, d_model)\n",
    "\n",
    "        \"\"\"\n",
    "        # Input of size: batch_size x sequence length x embedding dims\n",
    "        batch_size = key.size(0)\n",
    "        k_len, q_len, v_len = key.size(1), query.size(1), value.size(1)\n",
    "\n",
    "        # reshape from (batch_size x seq_len x embed_size) -> (batch_size x seq_len x heads x head)\n",
    "        # example: from (30x10x512) -> (30x10x8x64)\n",
    "        key = key.reshape(batch_size, k_len, self.num_heads, self.head_dim)\n",
    "        query = query.reshape(batch_size, q_len, self.num_heads, self.head_dim)\n",
    "        value = value.reshape(batch_size, v_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        key = self.key(key)  # (30x10x8x64)\n",
    "        query = self.query(query)  # (30x10x8x64)\n",
    "        value = self.value(value)  # (30x10x8x64)\n",
    "\n",
    "        # query shape: batch_size x q_len, heads, head, e.g: (30x10x8x64)\n",
    "        # key shape: batch_size x v_len, heads, head, e.g: (30x10x8x64)\n",
    "        # product shape should be: batch_size, heads, q_len, v_len, e.g: (30x8x10x10)\n",
    "        product = torch.einsum(\"bqhd,bkhd->bhqk\", [query, key])\n",
    "\n",
    "        # if mask (in decoder)\n",
    "        if mask is not None:\n",
    "            product = product.masked_fill(mask == 0, float(\"-1e20\")) # -inf for softmax -> 0\n",
    "\n",
    "        product = product / math.sqrt(self.head_dim)\n",
    "\n",
    "        scores = F.softmax(product, dim=-1)\n",
    "\n",
    "        # scores shape: batch_size, heads, q_len, v_len, e.g: (30x8x10x10)\n",
    "        # value shape: batch_size, v_len, heads, head, e.g: (30x10x8x64)\n",
    "        # output: batch_size, heads, v_len, head, e.g: (30x10x512)\n",
    "        output = torch.einsum(\"nhql,nlhd->nqhd\", [scores, value]).reshape(\n",
    "            batch_size, q_len, self.num_heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        output = self.linear_layer(output)  # (30x10x512) -> (30x10x512)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8207b97a-4398-4057-91a1-0c02dd9e315d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 10, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Example\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "\n",
    "mha_layer = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "query = torch.rand(30, 10, d_model)\n",
    "key = torch.rand(30, 10, d_model)\n",
    "value = torch.rand(30, 10, d_model)\n",
    "\n",
    "mha_layer(query, key, value).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52d1ed4-ff0d-4988-b5b0-82e964758ff5",
   "metadata": {},
   "source": [
    "- **Layer Normalization Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d640c067-39fa-4181-ad87-6a6bb6cadca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape=parameters_shape\n",
    "        self.eps=eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
    "        mean = inputs.mean(dim=dims, keepdim=True)\n",
    "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        y = (inputs - mean) / std\n",
    "        out = self.gamma * y  + self.beta\n",
    "        return out\n",
    "\n",
    "# Or using nn.LayerNorm(d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "276d988f-e30e-4436-8a06-3ef7c11f13f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Example\n",
    "ln = LayerNormalization((1,2,3))\n",
    "ln(torch.randn(1,2,3)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bb68ec-eaf5-40e6-8ac2-21ab67b3e1a0",
   "metadata": {},
   "source": [
    "- **Positionwise Feed Forward Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79967c5c-330f-4ba9-ac6d-1cd1512843ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "# feed_forward = nn.Sequential(\n",
    "#     nn.Linear(d_model, expansion_factor * d_model),  # e.g: 512x(4*512) -> (512, 2048)\n",
    "#     nn.ReLU(),  # ReLU activation function\n",
    "#     nn.Linear(d_model * expansion_factor, d_model),  # e.g: 4*512)x512 -> (2048, 512)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9e15943-748f-45fa-a6de-41e33d52e6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Example\n",
    "ff = PositionwiseFeedForward(512, 300)\n",
    "ff(torch.randn(1,5,512)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2b3196-79bd-4808-bb96-ad2cab19acae",
   "metadata": {},
   "source": [
    "- **Copy Block Function**: we can use nn.Sequential but i think we don't need to do that because we don't have any changes in Module Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51af5dd9-2ae2-4f58-969a-5cd9e3e45c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replicate(block, N=6) -> nn.ModuleList:\n",
    "    \"\"\"\n",
    "    Method to replicate the existing block to N set of blocks\n",
    "    :param block: class inherited from nn.Module, mainly it is the encoder or decoder part of the architecture\n",
    "    :param N: the number of stack, in the original paper they used 6\n",
    "    :return: a set of N blocks\n",
    "    \"\"\"\n",
    "    block_stack = nn.ModuleList([copy.deepcopy(block) for _ in range(N)])\n",
    "    return block_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cd5d10-d943-4116-8d07-f32885077b2a",
   "metadata": {},
   "source": [
    "## With those small blocks and functions, let's build these important blocks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf2e50e-5dce-46d5-b9c5-f5145367baf5",
   "metadata": {},
   "source": [
    "- **Transformer Block** includes: **Multi-Head Attention**, **Add & Norm**, **Feed & Forward** and **Dropout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "852e35db-7a0f-4804-b164-fb36427c7677",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model=512,\n",
    "                 num_heads=8,\n",
    "                 expansion_factor=4,\n",
    "                 dropout=0.1\n",
    "                ):\n",
    "        \"\"\"\n",
    "        The Transformer Block used in the encoder and decoder as well\n",
    "\n",
    "        :param d_model: the embedding dimension\n",
    "        :param num_heads: the number of heads\n",
    "        :param expansion_factor: the factor that determines the output dimension of the feed forward layer\n",
    "        :param dropout: probability dropout (between 0 and 1)\n",
    "        \"\"\"\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.multihead_attention = MultiHeadAttention(d_model,num_heads)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, expansion_factor * d_model), # Ex: (512,1024)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(expansion_factor * d_model, d_model), # Ex: (1024,512)\n",
    "            # The output shape will be not different from input\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # First come to Multi-Head Attention\n",
    "        attention = self.multihead_attention(query,key,value,mask)\n",
    "\n",
    "        # Add & Norm\n",
    "        # Add\n",
    "        attention_added = attention + value;\n",
    "        # Norm \n",
    "        attention_norm = self.dropout(self.norm(attention_added))\n",
    "\n",
    "        # Feed & Forward\n",
    "        attention_ff = self.feed_forward(attention_norm)\n",
    "\n",
    "        # Add & Norm again!\n",
    "        # Add\n",
    "        attention_ff_added = attention_ff + attention_norm\n",
    "        # Norm\n",
    "        attention_ff_norm = self.dropout(self.norm(attention_ff_added))\n",
    "\n",
    "        return attention_ff_norm\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0145ab0a-1473-4b97-8ce0-4f716f731674",
   "metadata": {},
   "source": [
    "- **Encoder** includes Input Pre-processing (**Token Embedding** & **Positional Encoding**) and **Transformer Block** (Encode block in the picture on the top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d59c851-23c2-412a-b9b9-e9024bd74a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 max_length_seq,\n",
    "                 vocab_size,\n",
    "                 d_model=512,\n",
    "                 num_blocks=6,\n",
    "                 expansion_factor=4,\n",
    "                 num_heads=8,\n",
    "                 dropout=0.1\n",
    "                ):\n",
    "        \"\"\"\n",
    "        The Encoder part of the Transformer architecture\n",
    "\n",
    "        :param max_length_seq: the max length of the sequence\n",
    "        :param vocab_size: the total size of the vocabulary\n",
    "        :param d_model: the embedding dimension\n",
    "        :param num_blocks: the number of blocks (encoders), 6 by default\n",
    "        :param expansion_factor: the factor that determines the output dimension of the feed forward layer in each encoder\n",
    "        :param num_heads: the number of heads in each encoder\n",
    "        :param dropout: probability dropout (between 0 and 1)\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Token Embedding\n",
    "        self.token_emb = TokenEmbedding(vocab_size,d_model)\n",
    "        # Positional Encoding\n",
    "        self.pos_encode = PositionalEncoding(d_model,max_length_seq)\n",
    "\n",
    "        # Transformer Blocks\n",
    "        self.transformer_blocks = replicate(TransformerBlock(d_model,num_heads,expansion_factor,dropout),num_blocks)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Input Pre-processing: Token Embedding + Positional Encoding\n",
    "        output = self.dropout(self.pos_encode()[:, :x.size(1), :].requires_grad_(False) + self.token_emb(x))\n",
    "\n",
    "        # Go to Transformer Blocks (Encode)\n",
    "        for block in self.transformer_blocks:\n",
    "            output = block(output,output,output)\n",
    "\n",
    "        return output\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf4215e2-b0c5-486d-a4b1-1dc8e3555209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 50, 512])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "max_length_seq = 100\n",
    "vocab_size = 10000\n",
    "d_model = 512\n",
    "num_blocks = 6\n",
    "expansion_factor = 4\n",
    "num_heads = 8\n",
    "dropout = 0.1\n",
    "\n",
    "encoder = Encoder(max_length_seq, vocab_size, d_model, num_blocks, expansion_factor, num_heads, dropout)\n",
    "\n",
    "batch_size = 32\n",
    "sequence_length = 50\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "\n",
    "encoder_output = encoder(input_ids)\n",
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e016d08b-ddf3-41de-b55f-36cc4ee79ed1",
   "metadata": {},
   "source": [
    "- **Decoder Block** because the architecture of **Decoder Block** has a little difference from Transformer Block (Encoder Block in the picture), we need to build again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90a974cc-206b-4aff-a064-d04f4c8857fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model=512,\n",
    "                 num_heads=8,\n",
    "                 expansion_factor=4,\n",
    "                 dropout=0.1\n",
    "                ):\n",
    "        \"\"\"\n",
    "        The DecoderBlock which will consist of the TransformerBlock used in the encoder, plus a decoder multi-head attention\n",
    "        :param d_model: the embedding dimension\n",
    "        :param num_heads: the number of heads\n",
    "        :param expansion_factor: the factor that determines the output dimension of the feed forward layer\n",
    "        :param dropout: probability dropout (between 0 and 1)\n",
    "        \"\"\"\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        # Masked Multi-Head Attention\n",
    "        self.attention = MultiHeadAttention(d_model,num_heads)\n",
    "\n",
    "        # Normalization in Add & Norm\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Transformer Block\n",
    "        self.transformer_block = TransformerBlock(d_model,num_heads,expansion_factor,dropout)\n",
    "\n",
    "    def forward(self, query, key, x, mask): # Different from Encoder\n",
    "        # Masked Multi-Head Attention\n",
    "        decoder_attention = self.attention(x,x,x, mask)\n",
    "\n",
    "        # Add & Norm\n",
    "        # Add\n",
    "        decoder_attention_added = self.dropout(decoder_attention + x)\n",
    "        # Norm\n",
    "        decoder_attention_norm = self.dropout(self.norm(decoder_attention_added))\n",
    "\n",
    "        # Transformer Block\n",
    "        decoder_attention_output = self.transformer_block(query, key, decoder_attention_norm)\n",
    "\n",
    "        return decoder_attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58513193-9156-41c9-8861-6464a3ae3341",
   "metadata": {},
   "source": [
    "- **Decoder** includes **Output Pre-processing** (**Token Embedding** & **Positional Encoding**), **Decoder Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64c335b7-26ce-447f-96d3-449efe7bf12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 target_vocab_size,\n",
    "                 max_length_seq,\n",
    "                 d_model=512,\n",
    "                 num_blocks=6,\n",
    "                 expansion_factor=4,\n",
    "                 num_heads=8,\n",
    "                 dropout=0.1\n",
    "                ):\n",
    "        \"\"\"\n",
    "        The Decoder part of the Transformer architecture\n",
    "\n",
    "        :param target_vocab_size: the size of the target\n",
    "        :param max_length_seq: the length of the sequence, in other words, the length of the words\n",
    "        :param d_model: the embedding dimension\n",
    "        :param num_blocks: the number of blocks (encoders), 6 by default\n",
    "        :param expansion_factor: the factor that determines the output dimension of the feed forward layer in each decoder\n",
    "        :param num_heads: the number of heads in each decoder\n",
    "        :param dropout: probability dropout (between 0 and 1)\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "         # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Token Embedding\n",
    "        self.token_emb = TokenEmbedding(target_vocab_size,d_model)\n",
    "        # Positional Encoding\n",
    "        self.pos_encode = PositionalEncoding(d_model,max_length_seq)\n",
    "\n",
    "        # Decoder Blocks\n",
    "        self.decoder_blocks = replicate(DecoderBlock(d_model,num_heads,expansion_factor,dropout), num_blocks)\n",
    "\n",
    "    def forward(self, x, encoder_output, mask):\n",
    "        # Output Pre-processing: Token Embedding + Positional Encoding\n",
    "        output = self.dropout(self.pos_encode()[:, :x.size(1), :].requires_grad_(False) + self.token_emb(x))\n",
    "\n",
    "        # Go to Transformer Blocks (Encode)\n",
    "        for block in self.decoder_blocks:\n",
    "            output = block(encoder_output,encoder_output,output, mask)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc8193b3-c32b-49c6-8f94-9106c9c1367f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 50, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "def make_trg_mask(trg):\n",
    "    batch_size, trg_len = trg.shape\n",
    "    # returns the lower triangular part of matrix filled with ones\n",
    "    trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "        batch_size, 1, trg_len, trg_len\n",
    "    )\n",
    "    return trg_mask\n",
    "\n",
    "max_length_seq = 100\n",
    "vocab_size = 10000\n",
    "d_model = 512\n",
    "num_blocks = 6\n",
    "expansion_factor = 4\n",
    "num_heads = 8\n",
    "dropout = 0.1\n",
    "\n",
    "decoder = Decoder(vocab_size, max_length_seq, d_model, num_blocks, expansion_factor, num_heads, dropout)\n",
    "\n",
    "batch_size = 32\n",
    "sequence_length = 50\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "\n",
    "decoder(input_ids,encoder_output,make_trg_mask(input_ids)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadb9ff7-e4f5-4a9c-b1c3-2c434216fe36",
   "metadata": {},
   "source": [
    "## Finally, The Transformer Architecture is complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "724efbec-53d8-4efd-9469-58df2f117555",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 vocab_size,\n",
    "                 target_vocab_size,\n",
    "                 max_length_seq,\n",
    "                 num_blocks=6,\n",
    "                 expansion_factor=4,\n",
    "                 num_heads=8,\n",
    "                 dropout=0.1\n",
    "                ):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "\n",
    "        self.encoder = Encoder(max_length_seq=max_length_seq,\n",
    "                              vocab_size=vocab_size,\n",
    "                               d_model=d_model,\n",
    "                               num_blocks=num_blocks,\n",
    "                               expansion_factor=expansion_factor,\n",
    "                               num_heads=num_heads,\n",
    "                               dropout=dropout)\n",
    "\n",
    "        self.decoder = Decoder(target_vocab_size=target_vocab_size,\n",
    "                              max_length_seq=max_length_seq,\n",
    "                              d_model=d_model,\n",
    "                              num_blocks=num_blocks,\n",
    "                              expansion_factor=expansion_factor,\n",
    "                              num_heads=num_heads,\n",
    "                              dropout=dropout)\n",
    "\n",
    "        self.linear_layer = nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        batch_size, trg_len = trg.shape\n",
    "        # returns the lower triangular part of matrix filled with ones\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            batch_size, 1, trg_len, trg_len\n",
    "        )\n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        trg_mask = self.make_trg_mask(target)\n",
    "        enc_out = self.encoder(source)\n",
    "        outputs = self.decoder(target, enc_out, trg_mask)\n",
    "        output = F.softmax(self.linear_layer(outputs), dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afa02306-57b4-481e-84de-0fe43bb2de9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 12]) torch.Size([2, 12])\n",
      "Transformer(\n",
      "  (encoder): Encoder(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (token_emb): TokenEmbedding(\n",
      "      (embedding_layer): Embedding(11, 512)\n",
      "    )\n",
      "    (pos_encode): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (transformer_blocks): ModuleList(\n",
      "      (0-5): 6 x TransformerBlock(\n",
      "        (multihead_attention): MultiHeadAttention(\n",
      "          (query): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (key): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (value): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (token_emb): TokenEmbedding(\n",
      "      (embedding_layer): Embedding(11, 512)\n",
      "    )\n",
      "    (pos_encode): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (decoder_blocks): ModuleList(\n",
      "      (0-5): 6 x DecoderBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (query): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (key): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (value): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (transformer_block): TransformerBlock(\n",
      "          (multihead_attention): MultiHeadAttention(\n",
      "            (query): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (key): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Sequential(\n",
      "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear_layer): Linear(in_features=512, out_features=11, bias=True)\n",
      ")\n",
      "Output Shape: torch.Size([2, 12, 11])\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "src_vocab_size = 11\n",
    "target_vocab_size = 11\n",
    "num_blocks = 6\n",
    "seq_len = 12\n",
    "\n",
    "# let 0 be sos token and 1 be eos token\n",
    "src = torch.tensor([[0, 2, 5, 6, 4, 3, 9, 5, 2, 9, 10, 1],\n",
    "                    [0, 2, 8, 7, 3, 4, 5, 6, 7, 2, 10, 1]])\n",
    "target = torch.tensor([[0, 1, 7, 4, 3, 5, 9, 2, 8, 10, 9, 1],\n",
    "                       [0, 1, 5, 6, 2, 4, 7, 6, 2, 8, 10, 1]])\n",
    "\n",
    "print(src.shape, target.shape)\n",
    "model = Transformer(d_model=512,\n",
    "                    vocab_size=src_vocab_size,\n",
    "                    target_vocab_size=target_vocab_size,\n",
    "                    max_length_seq=seq_len,\n",
    "                    num_blocks=num_blocks,\n",
    "                    expansion_factor=4,\n",
    "                    num_heads=8\n",
    "                   )\n",
    "\n",
    "print(model)\n",
    "out = model(src, target)\n",
    "print(f\"Output Shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0154f2-2def-44ae-8aef-b36759b77127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab8045-7a8b-4c2b-97c5-2fc5748556ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TorchCuda",
   "language": "python",
   "name": "torchcuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
